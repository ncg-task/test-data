title
RetinaFace : Single - stage Dense Face Localisation in the Wild
abstract
Though tremendous strides have been made in uncontrolled face detection , accurate and efficient face localisation in the wild remains an open challenge .
This paper presents a robust single - stage face detector , named RetinaFace , which performs pixel - wise face localisation on various scales of faces by taking advantages of joint extra-supervised and self - supervised multi-task learning .
Specifically ,
We make contributions in the following five aspects :
( 1 ) We manually annotate five facial landmarks on the WIDER FACE dataset and observe significant improvement in hard face detection with the assistance of this extra supervision signal .
( 2 ) We further add a selfsupervised mesh decoder branch for predicting a pixel - wise 3D shape face information in parallel with the existing supervised branches .
( 3 ) On the WIDER FACE hard test set , RetinaFace outperforms the state of the art average precision ( AP ) by 1.1 % ( achieving AP equal to 91.4 % ) .
( 4 ) On the IJB - C test set , RetinaFace enables state of the art methods ( ArcFace ) to improve their results in face verification ( TAR = 89.59 % for FAR = 1 e - 6 ) .
( 5 ) By employing light - weight backbone networks , Retina Face can run real - time on a single CPU core fora VGA - resolution image .
Extra annotations and code have been made available at : https://github.com/deepinsight/insightface/tree/master/RetinaFace.
Introduction
Automatic face localisation is the prerequisite step of facial image analysis for many applications such as facial attribute ( e.g. expression and age ) and facial identity recognition .
A narrow definition of face localisation may refer to traditional face detection , which aims at estimating the face bounding boxes without any scale and position prior .
Nevertheless , in this paper .
The proposed single - stage pixel - wise face localisation method employs extra-supervised and self - supervised multi-task learning in parallel with the existing box classification and regression branches .
Each positive anchor outputs ( 1 ) a face score , ( 2 ) a face box , ( 3 ) five facial landmarks , and ( 4 ) dense 3 D face vertices projected on the image plane .
we refer to a broader definition of face localisation which includes face detection , face alignment , pixelwise face parsing and 3D dense correspondence regression .
That kind of dense face localisation provides accurate facial position information for all different scales .
Inspired by generic object detection methods , which embraced all the recent advances in deep learning , face detection has recently achieved remarkable progress .
Different from generic object detection , face detection features smaller ratio variations ( from 1:1 to 1:1.5 ) but much larger scale variations ( from several pixels to thousand pixels ) .
The most recent state - of - the - art methods focus on singlestage design which densely samples face locations and scales on feature pyramids , demonstrating promising performance and yielding faster speed compared to twostage methods .
Following this route , we improve the single - stage face detection framework and propose a state - of - the - art dense face localisation method by exploiting multi-task losses coming from strongly supervised and self - supervised signals .
Our idea is examplified in .
Typically , face detection training process contains both classification and box regression losses .
Chen et al. proposed to combine face detection and alignment in a joint cascade framework based on the observation that aligned face shapes provide better features for face classification .
Inspired by , MTCNN and STN simultaneously detected faces and five facial landmarks .
Due to training data limitation , JDA , MTCNN and STN have not verified whether tiny face detection can benefit from the extra supervision of five facial landmarks .
One of the questions we aim at answering in this paper is whether we can push forward the current best performance ( 90.3 % ) on the WIDER FACE hard test set by using extra supervision signal built of five facial landmarks .
In Mask R - CNN , the detection performance is significantly improved by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition and regression .
That confirms that dense pixel - wise annotations are also beneficial to improve detection .
Unfortunately , for the challenging faces of WIDER FACE it is not possible to conduct dense face annotation ( either in the form of more landmarks or semantic segments ) .
Since supervised signals can not be easily obtained , the question is whether we can apply unsupervised methods to further improve face detection .
In FAN , an anchor - level attention map is proposed to improve the occluded face detection .
Nevertheless , the proposed attention map is quite coarse and does not contain semantic information .
Recently , self - supervised 3D morphable models have achieved promising 3 D face modelling in - the - wild .
Especially , Mesh Decoder achieves over real - time speed by exploiting graph convolutions on joint shape and texture .
However , the main challenges of applying mesh decoder into the single - stage detector are : ( 1 ) camera parameters are hard to estimate accurately , and ( 2 ) the joint latent shape and texture representation is predicted from a single feature vector ( 1 1 Conv on feature pyramid ) instead of the RoI pooled feature , which indicates the risk of feature shift .
In this paper , we employ a mesh decoder branch through self - supervision learning for predicting a pixel - wise 3 D face shape in parallel with the existing supervised branches .
To summarise , our key contributions are :
Based on a single - stage design , we propose a novel pixel - wise face localisation method named Reti- naFace , which employs a multi-task learning strategy to simultaneously predict face score , face box , five facial landmarks , and 3D position and correspondence of each facial pixel .
On the WIDER FACE hard subset , RetinaFace outperforms the AP of the state of the art two - stage method ( ISRN ) by 1.1 % ( AP equal to 91.4 % ) .
On the IJB - C dataset , RetinaFace helps to improve Ar - c Face 's verification accuracy ( with TAR equal to 89.59 % when FAR = 1 e - 6 ) .
This indicates that better face localisation can significantly improve face recognition .
By employing light - weight backbone networks , Reti - na
Face can run real - time on a single CPU core fora VGA - resolution image . Extra annotations and code have been released to facilitate future research .
Related Work
Image pyramid
v.s. feature pyramid :
The slidingwindow paradigm , in which a classifier is applied on a dense image grid , can be traced back to past decades .
The milestone work of Viola - Jones explored cascade chain to reject false face regions from an image pyramid with real - time efficiency , leading to the widespread adoption of such scale - invariant face detection framework .
Even though the sliding - window on image pyramid was the leading detection paradigm , with the emergence of feature pyramid , sliding - anchor on multi-scale feature maps , quickly dominated face detection .
Two - stage v.s. single - stage : Current face detection methods have inherited some achievements from generic object detection approaches and can be divided into two categories : two - stage methods ( e.g. Faster R - CNN ) and single - stage methods ( e.g. SSD and Reti - naNet ) .
Two - stage methods employed a " proposal and refinement " mechanism featuring high localisation accuracy .
By contrast , single - stage methods densely sampled face locations and scales , which resulted in extremely unbalanced positive and negative samples during training .
To handle this imbalance , sampling and re-weighting methods were widely adopted .
Compared to two - stage methods , single - stage methods are more efficient and have higher recall rate but at the risk of achieving a higher false positive rate and compromising the localisation accuracy .
Context Modelling :
To enhance the model 's contextual reasoning power for capturing tiny faces , SSH and PyramidBox applied context modules on feature pyramids to enlarge the receptive field from Euclidean grids .
To enhance the non-rigid transformation modelling capacity of CNNs , deformable convolution network ( DCN ) employed a novel deformable layer to model geometric transformations .
The champion solution of the WIDER Face Challenge 2018 indicates that rigid ( expansion ) and non-rigid ( deformation ) context modelling are complementary and orthogonal to improve the performance of face detection .
Multi- task Learning :
Joint face detection and alignment is widely used as aligned face shapes provide better features for face classification .
In Mask R - CNN , the detection performance was significantly improved by adding a branch for predicting an object mask in parallel with the existing branches .
Densepose adopted the architecture of Mask - RCNN to obtain dense part labels and coordinates within each of the selected regions .
Neverthe - less , the dense regression branch in was trained by supervised learning .
In addition , the dense branch was a small FCN applied to each RoI to predict a pixel - to - pixel dense mapping .
RetinaFace
Multi- task Loss
For any training anchor i , we minimise the following multi -task loss :
( 1 ) Face classification loss L cls ( p i , p * i ) , where pi is the predicted probability of anchor i being a face and p * i is 1 for the positive anchor and 0 for the negative anchor .
The classification loss L cls is the softmax loss for binary classes ( face / not face ) .
represent the coordinates of the predicted box and ground - truth box associated with the positive anchor .
We follow to normalise the box regression targets ( i.e. centre location , width and height ) and use L box
where R is the robust loss function ( smooth - L 1 ) defined in .
. . , l * x 5 , l * y5 } i represent the predicted five facial landmarks and groundtruth associated with the positive anchor .
Similar to the box centre regression , the five facial landmark regression also employs the target normalisation based on the anchor centre .
( 4 ) Dense regression loss L pixel ( refer to Eq. 3 ) .
The loss - balancing parameters ?
1 -?
3 are set to 0.25 , 0.1 and 0.01 , which means that we increase the significance of better box and landmark locations from supervision signals .
Dense Regression Branch
Mesh Decoder .
We directly employ the mesh decoder ( mesh convolution and mesh up - sampling ) from , which is a graph convolution method based on fast localised spectral filtering .
In order to achieve further acceleration , we also use a joint shape and texture decoder similarly to the method in , contrary to which only decoded shape .
Below we will briefly explain the concept of graph convolutions and outline why they can be used for fast decoding .
As illustrated in , a 2D convolutional operation is a " kernel - weighted neighbour sum " within the Euclidean grid receptive field .
Similarly , graph convolution also employs the same concept as shown in .
However , the neighbour distance is calculated on the graph by counting the minimum number of edges connecting two vertices .
We follow to define a coloured face mesh G = ( V , E ) , where V ?
R n6 is a set of face vertices containing the joint shape and texture information , and E ?
{ 0 , 1 } nn is a sparse adjacency matrix encoding the connection status between vertices .
The graph Laplacian is defined as
Following , the graph convolution with kernel g ?
can be formulated as a recursive Chebyshev polynomial truncated at order K ,
where ? ?
R K is a vector of Chebyshev coefficients and T k ( L ) ?
R nn is the Chebyshev polynomial of order k evaluated at the scaled Laplacian L .
The whole filtering operation is extremely efficient including K sparse matrix - vector multiplications and one dense matrix - vector multiplication
Differentiable Renderer .
After we predict the shape and texture parameters PST ?
R 128 , we employ an efficient differentiable 3D mesh renderer to project a colouredmesh DP ST onto a 2D image plane with camera parame -
camera location , camera pose and focal length ) and illumination parameters
location of point light source , colour values and colour of ambient lighting ) .
Dense Regression Loss .
Once we get the rendered 2 D face R ( D PST , P cam , P ill ) , we compare the pixel - wise difference of the rendered and the original 2 D face using the following function :
Experiments
Dataset
The WIDER FACE dataset consists of 32 , 203 images and 393 , 703 face bounding boxes with a high degree of variability in scale , pose , expression , occlusion and illumination .
The WIDER FACE dataset is split into training ( 40 % ) , validation ( 10 % ) and testing ( 50 % ) subsets by randomly sampling from 61 scene categories .
Based on the detection rate of EdgeBox , three levels of difficulty ( i.e. Easy , Medium and Hard ) are defined by incrementally incorporating hard samples .
Extra Annotations .
As illustrated in and Tab. 1 , we define five levels of face image quality ( according to how .
An overview of the proposed single - stage dense face localisation approach .
RetinaFace is designed based on the feature pyramids with independent context modules .
Following the context modules , we calculate a multi - task loss for each anchor .
difficult it is to annotate landmarks on the face ) and annotate five facial landmarks ( i.e. eye centres , nose tip and mouth corners ) on faces that can be annotated from the WIDER FACE training and validation subsets .
In total , we have annotated 84.6 k faces on the training set and 18.5 k faces on the validation set ..
We add extra annotations of five facial landmarks on faces that can be annotated ( we call them " annotatable " ) from the WIDER FACE training and validation sets .
Implementation details
Feature Pyramid .
Retina
Face employs feature pyramid levels from P 2 to P 6 , where P 2 to P 5 are computed from the output of the corresponding ResNet residual stage ( C 2 through C 5 ) using top - down and lateral connections as in .
P 6 is calculated through a 33 convolution with stride = 2 on C 5 .
C 1 to C 5 is a pre-trained ResNet - 152 classification network on the ImageNet - 11 k dataset while P 6 are randomly initialised with the " Xavier " method .
Context Module .
Inspired by SSH and Pyramid - Box , we also apply independent context modules on five feature pyramid levels to increase the receptive field and enhance the rigid context modelling power .
Drawing lessons from the champion of the WIDER Face Challenge 2018 , we also replace all 3 3 convolution layers within the lateral connections and context modules by the deformable convolution network ( DCN ) , which further strengthens the non-rigid context modelling capacity .
Loss Head .
For negative anchors , only classification loss is applied .
For positive anchors , the proposed multi-task loss is calculated .
We employ a shared loss head ( 1 1 conv ) across different feature maps H n W n 256 , n ? { 2 , . . . , 6 }.
For the mesh decoder , we apply the pre-trained model , which is a small computational overhead that allows for efficient inference .
Anchor Settings .
As illustrated in Tab .
2 , we employ scalespecific anchors on the feature pyramid levels from P 2 to P 6 like .
Here , P 2 is designed to capture tiny faces by tiling small anchors at the cost of more computational time and at the risk of more false positives .
We set the scale step at 2 1 / 3 and the aspect ratio at 1 : During training , anchors are matched to a ground - truth box when IoU is larger than 0.5 , and to the background when IoU is less than 0.3 .
Unmatched anchors are ignored during training .
Since most of the anchors ( > 99 % ) are negative after the matching step , we employ standard OHEM to alleviate significant imbalance between the positive and negative training examples .
More specifically , we sort negative anchors by the loss values and select the top ones so that the ratio between the negative and positive samples is at least 3:1 .
Data Augmentation .
Since there are around 20 % tiny faces in the WIDER FACE training set , we follow and randomly crop square patches from the original images and resize these patches into 640 640 to generate larger training faces .
More specifically , square patches are cropped from the original image with a random size between [ 0.3 , 1 ] of the short edge of the original image .
For the faces on the crop boundary , we keep the overlapped part of the face box if its centre is within the crop patch .
Besides random crop , we also augment training data by random horizontal flip with the probability of 0.5 and photo-metric colour distortion .
Training Details .
We train the RetinaFace using SGD optimiser ( momentum at 0.9 , weight decay at 0.0005 , batch size of 8 4 ) on four NVIDIA Tesla P40 ( 24GB ) GPUs .
The learning rate starts from 10 ? 3 , rising to 10 ? 2 after 5 epochs , then divided by 10 at 55 and 68 epochs .
The training process terminates at 80 epochs .
Testing Details .
For testing on WIDER FACE , we follow the standard practices of and employ flip as well as multi-scale ( the short edge of image at [ 500 , 800 , 1100 , 1400 , 1700 ] ) strategies .
Box voting [ 15 ] is applied on the union set of predicted face boxes using an IoU threshold at 0.4 .
Ablation Study
To achieve a better understanding of the proposed Reti - naFace , we conduct extensive ablation experiments to examine how the annotated five facial landmarks and the pro-posed dense regression branch quantitatively affect the performance of face detection .
Besides the standard evaluation metric of average precision ( AP ) when IoU=0.5 on the Easy , Medium and Hard subsets , we also make use of the development server ( Hard validation subset ) of the WIDER Face Challenge 2018 , which employs a more strict evaluation metric of mean AP ( m AP ) for IoU=0.5:0.05:0.95 , rewarding more accurate face detectors .
As illustrated in Tab. 3 , we evaluate the performance of several different settings on the WIDER FACE validation set and focus on the observations of AP and mAP on the Hard subset .
By applying the practices of state - of - the - art techniques ( i.e. FPN , context module , and deformable convolution ) , we setup a strong baseline ( 91.286 % ) , which is slightly better than ISRN ( 90.9 % ) .
Adding the branch of five facial landmark regression significantly improves the face box AP ( 0.408 % ) and mAP ( 0.775 % ) on the Hard subset , suggesting that landmark localisation is crucial for improving the accuracy of face detection .
By contrast , adding the dense regression branch increases the face box AP on Easy and Medium subsets but slightly deteriorates the results on the Hard subset , indicating the difficulty of dense regression under challenging scenarios .
Nevertheless , learning landmark and dense regression jointly enables a further improvement compared to adding landmark regression only .
This demonstrates that landmark regression does help dense regression , which in turn boosts face detection performance even further .
Method
Easy . Ablation experiments of the proposed methods on the WIDER FACE validation subset .
Face box Accuracy
Following the stander evaluation protocol of the WIDER FACE dataset , we only train the model on the training set and test on both the validation and test sets .
To obtain the evaluation results on the test set , we submit the detection results to the organisers for evaluation .
As shown in , we compare the proposed RetinaFace with other 24 state - of - the - art face detection algorithms ( i.e. Multiscale Cascade CNN , Two - stage CNN , ACF - WIDER , Faceness - WIDER , Multitask Cascade CNN , CMS - RCNN , LDCF + , HR , Face R - CNN , ScaleFace , SSH , SFD , Face R - FCN , MSCNN , FAN , Zhu et al. , Pyramid - Box , FDNet , SRN , FANet , DSFD , DFS , VIM - FD , ISRN ) .
Our approach outper - forms these state - of - the - art methods in terms of AP .
More specifically , RetinaFace produces the best AP in all subsets of both validation and test sets , i.e. , 96.9 % ( Easy ) , 96.1 % ( Medium ) and 91.8 % ( Hard ) for validation set , and 96.3 % ( Easy ) , 95.6 % ( Medium ) and 91.4 % ( Hard ) for test set .
Compared to the recent best performed method , Reti - na Face sets up a new impressive record ( 91.4 % v.s. 90.3 % ) on the Hard subset which contains a large number of tiny faces .
In , we illustrate qualitative results on a selfie with dense faces .
RetinaFace successfully finds about 900 faces ( threshold at 0.5 ) out of the reported 1 , 151 faces .
Besides accurate bounding boxes , the five facial landmarks predicted by Retina Face are also very robust under the variations of pose , occlusion and resolution .
Even though there are some failure cases of dense face localisation under heavy occlusion , the dense regression results on some clear and large faces are good and even show expression variations .
Five Facial Landmark Accuracy
To evaluate the accuracy of five facial landmark localisation , we compare RetinaFace with MTCNN on the AFLW dataset ( 24,386 faces ) as well as the WIDER FACE validation set ( 18.5 k faces ) .
Here , we employ the face box size ( ?
W H ) as the normalisation distance .
As shown in , we give the mean error of each facial landmark on the AFLW dataset .
RetinaFace significantly decreases the normalised mean errors ( NME ) from 2.72 % to 2.21 % when compared to MTCNN .
In , we show the cumulative error distribution ( CED ) curves on the WIDER FACE validation set .
Compared to MTCNN , RetinaFace significantly decreases the failure rate from 26.31 % to 9.37 % ( the NME threshold at 10 % ) .
Dense Facial Landmark Accuracy
Besides box and five facial landmarks , RetinaFace also outputs dense face correspondence , but the dense regression branch is trained by self - supervised learning only .
Following , we evaluate the accuracy of dense facial landmark localisation on the AFLW2000 - 3D dataset considering ( 1 ) 68 landmarks with the 2D projection coordinates and ( 2 ) all landmarks with 3D coordinates .
Here , the mean error is still normalised by the bounding box size .
In , we give the CED curves of state - of - the - art methods as well as RetinaFace .
Even though the performance gap exists between supervised and self - supervised methods , the dense regression results of RetinaFace are comparable with these state - of - the - art methods .
More specifically , we observe that ( 1 ) five facial landmarks regression can alleviate the training difficulty of dense regression branch and significantly improve the dense regression results .
( 2 ) using single - stage features ( as in RetinaFace ) to predict dense correspondence parameters is much harder than employing ( Region of Interest ) RoI features ( as in Mesh Decoder ) .
As illustrated in , RetinaFace can easily handle faces with pose variations but has difficulty under complex scenarios .
This indicates that mis-aligned and over-compacted feature representation ( 1 1 256 in RetinaFace ) impedes the single - stage framework achieving high accurate dense regression outputs .
Nevertheless , the projected face regions in the dense regression branch still have the effect of attention which can help to improve face detection as confirmed in the section of ablation study .
Face Recognition Accuracy
Face detection plays a crucial role in robust face recognition but its effect is rarely explicitly measured .
In this paper , we demonstrate how our face detection method can boost the performance of a state - of - the - art publicly available face recognition method , i.e. ArcFace .
ArcFace studied how different aspects in the training process of a deep convolutional neural network ( i.e. , choice of the training set , the network and the loss function ) affect large scale face recognition performance .
However , ArcFace paper did not study the effect of face detection by applying only the MTCNN for detection and alignment .
In this paper , we replace MTCNN by RetinaFace to detect and align all of the training data ( i.e. MS1M ) and test data ( i.e. LFW , CFP - FP , AgeDB - 30 and IJBC ) , and keep the embedding network ( i.e. ResNet100 ) and the loss function ( i.e. additive angular margin ) exactly the same as Arc -Face .
In Tab. 4 , we show the influence of face detection and alignment on deep face recognition ( i.e. ArcFace ) by comparing the widely used MTCNN and the proposed Reti-naFace .
The results on CFP - FP , demonstrate that Reti - na Face can boost ArcFace 's verification accuracy from 98.37 % to 99.49 % .
This result shows that the performance of frontal - profile face verification is now approaching that of frontal - frontal face verification ( e.g. 99.86 % on LFW ) .
Methods
LFW CFP - FP AgeDB - 30 MTCNN + ArcFace In , we show the ROC curves on the IJB - C dataset as well as the TAR for FAR =
1 e ? 6 at the end of each legend .
We employ two tricks ( i.e. flip test and face detection score to weigh samples within templates ) to progressively improve the face verification accuracy .
Under fair comparison , TAR ( at FAR = 1 e ? 6 ) significantly improves from 88 . 29 % to 89.59 % simply by replacing MTCNN with RetinaFace .
This indicates that ( 1 ) face detection and align - In ( c ) , we compare the dense regression results from RetinaFace and Mesh Decoder .
Retina
Face can easily handle faces with pose variations but has difficulty to predict accurate dense correspondence under complex scenarios .
ment significantly affect face recognition performance and ( 2 ) RetinaFace is a much stronger baseline that MTCNN for face recognition applications .
Inference Efficiency
During testing , RetinaFace performs face localisation in a single stage , which is flexible and efficient .
Besides the above - explored heavy - weight model ( Res Net - 152 , size of 262 MB , and AP 91.8 % on the WIDER FACE hard set ) , we also resort to a light - weight model ( MobileNet - 0.25 , size of 1 MB , and AP 78.2 % on the WIDER FACE hard set ) to accelerate the inference .
For the light - weight model , we can quickly reduce the data size by using a 7 7 convolution with stride = 4 on the input image , tile dense anchors on P 3 , P 4 and P 5 as in , and remove deformable layers .
In addition , the first two convolutional layers initialised by the ImageNet pre-trained .
ROC curves of 1:1 verification protocol on the IJB - C dataset .
" + F " refers to flip test during feature embedding and " + S " denotes face detection score used to weigh samples within templates .
We also give TAR for FAR = 1 e ? 6 at the end of the each legend .
model are fixed to achieve higher accuracy .
Tab. 5 gives the inference time of two models with respect to different input sizes .
We omit the time cost on the dense regression branch , thus the time statistics are irrelevant to the face density of the input image .
We take advantage of TVM to accelerate the model inference and timing is performed on the NVIDIA Tesla P40 GPU , Intel i7-6700 K CPU and ARM - RK3399 , respectively .
RetinaFace - ResNet - 152 is designed for highly accurate face localisation , running at 13 FPS for VGA images ( 640 480 ) .
By contrast , RetinaFace - MobileNet - 0.25 is designed for highly efficient face localisation which demonstrates considerable real - time speed of 40 FPS at GPU for 4 K images ( 4096 2160 ) , 20 FPS at multi-thread CPU for HD images ( 1920 1080 ) , and 60 FPS at single - thread CPU for VGA images ( 640 480 ) .
Even more impressively , 16 FPS at ARM for VGA images ( 640 480 ) allows fora fast system on mobile devices ..
Inference time ( m s ) of RetinaFace with different backbones ( Res Net - 152 and MobileNet - 0.25 ) on different input sizes ( VGA@640x480 , HD@1920x1080 and 4K@4096x2160 ) . " CPU - 1 " and " CPU -m " denote single - thread and multi-thread test on the Intel i7-6700 K CPU , respectively .
" GPU " refers to the NVIDIA Tesla P40 GPU and " ARM " platform is RK3399 ( A72x 2 ) .
Conclusions
We studied the challenging problem of simultaneous dense localisation and alignment of faces of arbitrary scales in images and we proposed the first , to the best of our knowledge , one - stage solution ( RetinaFace ) .
Our solution outperforms state of the art methods in the current most challenging benchmarks for face detection .
Furthermore , when RetinaFace is combined with state - of - the - art practices for face recognition it obviously improves the accuracy .
The data and models have been provided publicly available to facilitate further research on the topic .
Acknowledgements
Jiankang
Deng acknowledges financial support from the Imperial President 's PhD Scholarship and GPU donations from NVIDIA .
Stefanos Zafeiriou acknowledges support from EPSRC Fellowship DEFORM ( EP / S010203/1 ) , FACER2VM ( EP / N007743/1 ) and a Google Faculty Fellowship .
