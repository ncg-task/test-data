160	0	3	For
160	8	28	discriminative model
160	34	38	used
160	39	56	hidden dimensions
160	57	59	of
160	60	63	128
160	68	83	2 - layer LSTMs
161	8	24	generative model
161	30	34	used
161	35	49	256 dimensions
161	54	67	2 layer LSTMs
162	4	15	both models
162	21	26	tuned
162	31	43	dropout rate
162	44	55	to maximize
162	56	81	validation set likelihood
162	84	93	obtaining
162	94	107	optimal rates
162	108	110	of
162	111	133	0.2 ( discriminative )
162	138	156	0.3 ( generative )
163	8	32	sequential LSTM baseline
163	33	36	for
163	41	55	language model
163	66	71	found
163	75	95	optimal dropout rate
163	96	98	of
163	99	102	0.3
164	4	12	training
164	16	20	used
164	21	48	stochastic gradient descent
164	49	53	with
164	56	69	learning rate
164	70	72	of
164	73	76	0.1
12	19	28	introduce
12	29	62	recurrent neural network grammars
12	81	115	new generative probabilistic model
12	116	118	of
12	119	128	sentences
12	134	151	explicitly models
12	152	187	nested , hierarchical relationships
12	188	193	among
12	194	211	words and phrases
13	0	5	RNNGs
13	6	17	operate via
13	20	47	recursive syntactic process
13	48	62	reminiscent of
13	63	110	probabilistic context - free grammar generation
24	4	24	discriminative model
24	38	41	use
24	42	59	ancestor sampling
24	60	69	to obtain
24	70	77	samples
24	78	80	of
24	81	92	parse trees
24	93	96	for
24	97	106	sentences
15	3	7	give
15	8	20	two variants
15	21	23	of
15	28	37	algorithm
15	44	47	for
15	48	55	parsing
15	128	138	generation
25	3	10	present
25	13	49	simple importance sampling algorithm
25	56	60	uses
25	61	68	samples
25	69	73	from
25	78	99	discriminative parser
25	100	108	to solve
25	109	127	inference problems
25	128	130	in
25	135	151	generative model
2	25	33	Grammars
