title
Parsing as Language Modeling
abstract
We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing - 93.8 F 1 on section 23 , using 2 - 21 as training , 24 as development , plus tri-training .
When trees are converted to Stanford dependencies , UAS and LAS are 95.9 % and 94.1 % .
Introduction
Recent work on deep learning syntactic parsing models has achieved notably good results , e.g. , with 92.4 F 1 on Penn Treebank constituency parsing and with 92.8 F 1 .
In this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results , 93.8 F 1 , with a comparatively simple architecture .
In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem .
Section 2 looks more closely at three of the most relevant previous papers .
We then describe our exact model ( Section 3 ) , followed by the experimental setup and results ( Sections 4 and 5 ) .
There is a one - to - one mapping between a tree and its sequential form .
( Part - of - speech tags are not used . )
Language Modeling
Formally , a language model ( LM ) is a probability distribution over strings of a language :
where x is a sentence and t indicates a word position .
The efforts in language modeling go into computing P ( x t |x 1 , , x t?1 ) , which as described next is useful for parsing as well .
Parsing as Language Modeling
A generative parsing model parses a sentence ( x ) into it s phrasal structure ( y ) according to
where Y ( x ) lists all possible structures of x .
If we think of a tree ( x , y) as a sequence ( z ) as illustrated in , we can define a probability distribution over ( x , y) as follows :
P ( x , y ) = P ( z ) = P ( z 1 , , z m ) = m t=1 P ( z t | z 1 , , z t?1 ) , which is equivalent to Equation ( 1 ) .
We have reduced parsing to language modeling and can use language modeling techniques of estimating P ( z t | z 1 , , z t?1 ) for parsing .
Previous Work
We look here at three neural net ( NN ) models closest to our research along various dimensions .
The first gives the basic language modeling architecture that we have adopted , while the other two are parsing models that have the current best results in NN parsing .
LSTM - LM
The LSTM - LM of turns ( x 1 , , x t?1 ) into ht , a hidden state of an LSTM , and uses ht to guess x t :
where
Wis a parameter matrix and [ i ] indexes ith element of a vector .
The simplicity of the model makes it easily extendable and scalable , which has inspired a character - based LSTM - LM that works well for many languages and an ensemble of large LSTM - LMs for English with astonishing perplexity of 23.7 .
In this paper , we build a parsing model based on the LSTM - LM of .
observe that a phrasal structure ( y ) can be expressed as a sequence and build a machine translation parser ( MTP ) , a sequence - tosequence model , which translates x into y using a conditional probability :
MTP
where the conditioning event ( x , y 1 , , y t?1 ) is modeled by an LSTM encoder and an LSTM decoder .
The encoder maps x into he , a set of vectors that represents x , and the decoder obtains a summary vector ( h t ) which is concatenation of the decoder 's hidden state ( h d t ) and weighted sum of word representations ( n i =1 ?
i he i ) with an alignment vector ( ? ) .
Finally the decoder predicts y t given ht .
Inspired by MTP , our model processes sequential trees .
RNNG
Recurrent Neural Network Grammars ( RNNG ) , a generative parsing model , defines a joint distribution over a tree in terms of actions the model takes to generate the tree :
where a is a sequence of actions whose output precisely matches the sequence of symbols in z , which implies Equation is the same as Equation .
RNNG and our model differ in how they compute the conditioning event ( z 1 , , z t?1 ) :
RNNG combines hidden states of three LSTMs that keep track of actions the model has taken , an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM 's hidden state as shown in the next section .
Model
Our model , the model of applied to sequential trees and we call LSTM - LM from now on , is a joint distribution over trees :
where ht is a hidden state of an LSTM .
Due to lack of an algorithm that searches through an exponentially large phrase - structure space , we use an n-best parser to reduce Y ( x ) to Y ( x ) , whose size is polynomial , and use LSTM - LM to find y that satisfies
( 4 )
Hyper-parameters
The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50 .
We initialize starting states with previous minibatch 's last hidden states .
The forget gate bias is initialized to be one and the rest of model parameters are sampled from U ( ? 0.05 , 0.05 ) .
Dropout is applied to non-recurrent connections and gradients are clipped when their norm is bigger than 20 .
The learning rate is 0.25 0.85 max where is an epoch number .
For simplicity , we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax or noise contrastive estimation ( Gutmann and Hyvrinen , 2012 ) .
Experiments
We describe datasets we use for evaluation , detail training and development processes .
1
Data
We performed better when trained on all of 24 million trees than when trained on resampled two million trees .
Given x , we produce Y ( x ) , 50 - best trees , with Charniak parser and find y with LSTM - LM as do with their discriminative and generative models .
3
Training and Development
Supervision
As shown in , with 92.6 F 1 LSTM - LM ( G ) outperforms an ensemble of five MTPs and RNNG , both of which are trained on the WSJ only .
Semi-supervision
We compare LSTM - LM ( GS ) to two very strong semi-supervised NN parsers : an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 ( HC ) ; and an ensemble of six one - to - many sequence models trained on the HC and 4.5 millions of English - German translation sentence pairs .
We also compare LSTM - LM ( GS ) to best performing non -NN parsers in the literature .
Parsers ' parsing performance along with their training data is reported in .
LSTM - LM ( GS ) outperforms all the other parsers with 93.1 F 1 .
Results
Improved Semi-supervision
Due to search errors - good trees are missing in 50 - best trees - in Charniak ( G ) , our supervised and semi-supervised models do not exhibit their full potentials when Charniak ( G ) provides Y ( x ) .
To mitigate the search problem , we tri-train Charniak ( GS ) on all of 24 million NYT trees in addition to the WSJ , to yield Y ( x ) .
As shown in , both LSTM - LM ( G ) and LSTM - LM ( GS ) are affected by the quality of Y ( x ) .
A single LSTM - LM ( GS ) together with Charniak ( GS ) reaches 93.6 and an ensemble of eight LSTM - LMs ( GS ) with Charniak ( GS ) achieves a new state of the art , 93.8 F 1 .
When trees are converted to Stanford dependencies , 5 UAS and LAS are 95.9 % and 94.1 % , 6 more than 1 % higher than those of the state of the art dependency parser .
Why an indirect method ( converting trees to dependencies ) is more accurate than a direct one ( dependency parsing ) remains unanswered .
Conclusion
The generative parsing model we presented in this paper is very powerful .
In fact , we see that a generative parsing model , LSTM - LM , is more effective than discriminative parsing models .
We suspect building large models with character embeddings would lead to further improvement as in language modeling .
We also wish to develop a complete parsing model using the LSTM - LM framework . :
Evaluation of models trained on the WSJ and additional resources .
Note that the numbers of and are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB - style trees .
E ( LSTM - LMs ( GS ) ) is an ensemble of eight LSTM - LMs ( GS ) .
X/Y in Silver column indicates the number of silver trees used to train Charniak parser and LSTM - LM .
For the ensemble model , we report the maximum number of trees used to train one of LSTM - LMs ( GS ) .
at Brown University for setting up GPU machines and David McClosky for helping us train Charniak parser on millions trees .
