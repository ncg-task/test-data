35	19	26	present
35	27	38	experiments
35	39	49	to isolate
35	54	60	degree
35	75	79	gain
35	87	90	for
35	91	158	each of two state - of - the - art generative neural parsing models
35	161	222	the Recurrent Neural Network Grammar generative parser ( RG )
35	236	283	LSTM language modeling generative parser ( LM )
36	37	66	beam - based search procedure
36	67	71	with
36	75	96	augmented state space
36	106	112	search
36	129	146	generative models
43	46	52	taking
43	55	71	weighted average
43	72	74	of
43	79	85	scores
43	86	88	of
43	89	100	both models
43	101	115	when selecting
43	118	123	parse
43	124	128	from
43	133	162	base parser 's candidate list
4	62	82	constituency parsing
10	15	42	neural constituency parsing
90	0	28	Augmenting the candidate set
98	0	2	RG
98	3	12	decreases
98	13	24	performance
98	25	29	from
98	30	38	93.45 F1
98	39	41	to
98	42	50	92.78 F1
107	0	17	Score combination
111	13	22	combining
111	27	33	scores
111	34	36	of
111	37	48	both models
111	49	57	improves
111	61	66	using
111	71	76	score
111	77	79	of
111	80	98	either model alone
119	0	31	Strengthening model combination
123	2	11	Combining
123	12	33	candidates and scores
123	34	38	from
123	39	55	all three models
123	71	77	obtain
123	78	86	93.94 F1
131	0	10	Ensembling
134	17	22	using
134	32	51	ensembled RD models
134	66	71	lower
134	72	76	than
134	77	86	rescoring
134	89	104	single RD model
135	0	2	In
135	7	18	PTB setting
135	21	31	ensembling
135	32	36	with
135	37	54	score combination
135	55	63	achieves
135	68	88	best over all result
135	89	91	of
135	92	97	94.25
