(Contribution||has||Hyperparameters)
(Hyperparameters||Training on||small dataset)
(small dataset||additionally used||2 dropout layers)
(2 dropout layers||between||LSTM 1 and LSTM 2)
(2 dropout layers||between||LSTM 2 and LSTM 3)
(Hyperparameters||pre-trained||skip - gram embeddings)
(skip - gram embeddings||using||word2vec)
(skip - gram embeddings||of||size 512)
(skip - gram embeddings||on||10B - word corpus)
(Hyperparameters||used||model)
(model||call||LSTM + A)
(model||with||3 LSTM layers)
(model||with||256 units)
(256 units||in||each layer)
(Hyperparameters||has||embedding layer)
(embedding layer||using||pre-trained word - vector embeddings)
(embedding layer||for||our 90K vocabulary)
(embedding layer||initialized||randomly)
