title
Apollo at SemEval-2018 Task 9 : Detecting Hypernymy Relations Using Syntactic Dependencies
abstract
This paper presents the participation of Apollo 's team in the SemEval - 2018 Task 9 " Hypernym Discovery " , Subtask 1 : " General - Purpose Hypernym Discovery " , which tries to produce a ranked list of hypernyms for a specific term .
We propose a novel approach for automatic extraction of hypernymy relations from a corpus by using dependency patterns .
The results show that the application of these patterns leads to a higher score than using the traditional lexical patterns .
Introduction
This paper presents the Apollo team 's system for hypernym discovery which participated in task 9 of Semeval 2018 based on unsupervised machine learning .
It is a rule - based system that exploits syntactic dependency paths that generalize Hearst - style lexical patterns .
The paper is structured in 4 sections : this section presents existing approaches for automatic extraction of hypernymy relations , Section 2 contains the current system architecture .
The next section presents the web interface of the project , and , finally , Section 4 briefly analyses the results and drafts some conclusions .
Since language is a " vital organ " , constantly evolving and changing overtime , there are many words which lose one of their meanings or attach a new meaning .
For instance , when searching the word " apple " in WordNet , it appears defined as " fruit with red or yellow or green skin and sweet to tart crisp whitish flesh " and " native Eurasian tree widely cultivated in many varieties for it s firm rounded edible fruits " but searching in British National Corpus 1 , we will remark that the term is used more frequently as a named entity ( referring to a " company " ) .
From this point of view , we consider that developing a system for hypernym discovery that uses linguistic features from a corpus could be more useful for this task than using a manuallycrafted taxonomy .
It is well known that in natural language processing ( NLP ) , one of the biggest challenges is to understand the meaning of words .
Also , detecting hypernymy relations is an important task in NLP , which has been pursued for over two decades , and it is addressed in the literature using two complementary approaches : rule - based and distributional methods .
Rule - based methods base the decision on the lexicosyntactic paths connecting the joint occurrences of two or more terms in a corpus .
In the case of supervised distributional methods , term - pair is represented using some combination of the terms ' embedding vectors .
This challenge has been shown to directly help in downstream applications such automatic hypernymy detection is useful for NLP tasks such as : taxonomy creation , recognizing textual entailment , text generation , Question Answering systems , semantic search , Natural Language Inference , Coreference Resolution and many others .
Traditional procedures to evaluate taxonomies have focused on measuring the quality of the edges , i.e. , assessing the quality of the is - a relations .
This process typically consists of extracting a random sample of edges and manually labeling them by human judges .
In addition to the manual effort required to perform this evaluation , this procedure is not easily replicable from taxonomy to taxonomy ( which would most likely include different sets of concepts ) , and do not reflect the over all quality of a taxonomy .
Moreover , some taxonomy learning approaches link their concepts to existing resources such as Wikipedia .
A new Approach to Detect Hypernymy Relation
The main purpose of this project was to identify the best ( set of ) candidate hypernyms for a certain term from the given corpus 2 .
In our system , we considered the rule - based approach and , in order to extract the corresponding patterns , we used syntactic dependencies relations ( Universal Dependencies Parser 3 ) .
Below , we present our method of extracting hypernyms from text :
2 For this subtask , we used the 3 - billion - word UMBC corpus , which consists of paragraphs extracted from the web as part of the Stanford WebBase Project .
This is a very large corpus containing information from different domains .
3 Universal Dependencies ( UD ) is a framework for crosslinguistically consistent grammatical annotation and an open community effort with over 200 contributors producing more than 100 treebanks in over 60 languages .
Tokenization : sentence boundaries are detected and punctuation signs are separated from words ;
Part - of - speech tagging : the process of assigning a part - of - speech or lexical class marker to each word in a corpus .
Words in natural languages usually encode many pieces of information , such as : what the word " means " in the real world , what categories , if any , the word belongs to , what is the function of the word in the sentence ?
Many language processing applications need to extract the information encoded in the words .
Parsers which analyze sentence structure need to know / check agreement between : subjects and verbs , adjectives and nouns , determiners and nouns , etc .
Information retrieval systems benefit from know what the stem of a word is .
Machine translation systems need to analyze words to their components and generate words with specific features in the target language .
Dependency parsing : the syntactic parsing of a sentence consists of finding the correct syntactic structure of that sentence in a given formalism / grammar .
Dependency parsing structure consists of lexical items , linked by binary asymmetric relations called dependencies .
It is interested in grammatical relations between individual words ( governing & dependent words ) , it does not propose a recursive structure , rather a network of relations .
These relations can also have labels and the phrasal nodes are missing in the dependency structure , when compared to constituency structure .
One of the boosts for this approach was to develop new dependency patterns for identifying hypernymy relations from text thatare based on dependency relations .
The increased popularity and the universal inventory of categories and guidelines ( which facilitate annotation across languages ) of Universal Dependencies determined us to use this resource in order to automatically extract the hypernyms from the corpus .
Figure1 : Project 's architecture
In this manner , we managed to compress a list of 44 lexico- syntactic patterns used for the hypernyms extraction 4 in only 8 dependencies patterns .
In the next lines , we present few examples of lexico - syntactic patterns that were replaced by dependencies patterns :
{X and other Y ; X or other Y ; X and any other Y ; X and some other Y ; Y other than X ; X like other Y ; Y other than X } replaced by X " amod " Y;
{ X is a Y ; X was a Y ; X area Y ; X are Y ; X will be a Y ; X is an adj Y ; X was a adj .
Y ; X area adj .
Y ; X was a adj .
Y;
which are similar to X ; Y which is similar to X } replaced by X " nmod " Y.
Because we used syntactic dependencies relations ( no lexical patterns were involved ) , our system is language independent .
Unfortunately , the limited hardware resources determined us to run 4 http://webdatacommons.org/is adb/lrec2016.pdf our system only in English but we are looking forward to running it in both Spanish and Italian .
The Web Interface
The interface 5 was implemented in the form of a website .
The site is backed by a Mongodb data base .
When a user types in a query and hits enter a post request is sent and the backend will do some processing on the query ( tokenizing , lemmatizing ) and then search in the data base .
The results are then sent back to the user where they are rendered .
Results
We consider that a qualitative way of analyzing our system is to look at which relations are more productive .
presents the percentages of the most representative syntactic relations which we have identified .
While some relations have not been very fruitful ( such as X " obj " Y , for insance ) , others , instead , have been very productive , generating tens of thousands relations .
The project 's results show that we have managed to accomplish the main objective of this project , to outperform the random strategy .
The lower scores have been obtained for multiword expressions , for which we plan to add dedicated modules .
An issue that we have noticed was that the given vocabulary was quite restrictive , for instance , it contains words like " above - water " , " artesian water " , " bath water " etc. , but it does n't contain the word " water " ( we had a case when our system identified the word " water " as a hypernym and it was a correct hypernym , but due to the fact that the vocabulary does n't contain the word " water " , it can not be evaluated ) and many other examples like this .
