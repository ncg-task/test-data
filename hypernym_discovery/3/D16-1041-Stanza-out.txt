title
Supervised Distributional Hypernym Discovery via Domain Adaptation
abstract
Lexical taxonomies are graph - like hierarchical structures that provide a formal representation of knowledge .
Most knowledge graphs to date rely on is -a ( hypernymic ) relations as the backbone of their semantic structure .
In this paper , we propose a supervised distributional framework for hypernym discovery which operates at the sense level , enabling large - scale automatic acquisition of dis ambiguated taxonomies .
By exploiting semantic regularities between hyponyms and hypernyms in embeddings spaces , and integrating a domain clustering algorithm , our model becomes sensitive to the target data .
We evaluate several configurations of our approach , training with information derived from a manually created knowledge base , along with hypernymic relations obtained from Open Information Extraction systems .
The integration of both sources of knowledge yields the best over all results according to both automatic and manual evaluation on ten different domains .
Introduction
Lexical taxonomies ( taxonomies henceforth ) are graph - like hierarchical structures where terms are nodes , and are typically organized over a predefined merging or splitting criterion .
By embedding cues about how we perceive concepts , and how these concepts generalize in a domain of knowledge , these resources bear a capacity for generalization that lies at the core of human cognition and have become key in Natural Language Processing ( NLP ) tasks where inference and reasoning have proved to be essential .
In fact , taxonomies have enabled a remarkable number of novel NLP techniques , e.g. the contribution of WordNet to lexical semantics as well as various tasks , from word sense dis ambiguation to information retrieval , question answering and textual entailment .
To date , the application of taxonomies in NLP has consisted mainly of , on one hand , formally representing a domain of knowledge ( e.g. Food ) , and , on the other hand , constituting the semantic backbone of large - scale knowledge repositories such as ontologies or Knowledge Bases ( KBs ) .
In domain knowledge formalization , prominent work has made use of the web ( Kozareva and Hovy , 2010 ) , lexico - syntactic patterns , syntactic evidence , graph - based algorithms or popularity of web sources .
As for enabling large - scale knowledge repositories , this task often tackles the additional problem of dis ambiguating word senses and entity mentions .
Notable approaches of this kind include Yago , WikiTaxonomy , and the Wikipedia Bitaxonomy .
In addition , while not being taxonomy learning systems per se , semi-supervised systems for Information Extraction such as NELL rely crucially on taxonomized concepts and their relations within their learning process .
Taxonomy learning is roughly based on a twostep process , namely is -a ( hypernymic ) relation de-tection , and graph induction .
The hypernym detection phase has gathered much interest not only for taxonomy learning but also for lexical semantics .
It has been addressed by means of pattern - based methods 1 , clustering ) and graph - based approaches .
Moreover , work stemming from distributional semantics introduced notions of linguistic regularities found in vector representations such as word embeddings .
In this area , supervised approaches , arguably the most popular nowadays , learn a feature vector between term - hypernym vector pairs and train classifiers to predict hypernymic relations .
These pairs maybe represented either as a concatenation of both vectors , difference , dot -product , or including additional linguistic information for LSTMbased learning .
In this paper we propose TAXOEMBED 2 , a hypernym detection algorithm based on sense embeddings , which can be easily applied to the construction of lexical taxonomies .
It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces and , unlike previous approaches , leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge .
Our best configuration ( ranking first in two thirds of the experiments conducted ) considers two training sources :
( 1 ) Manually curated pairs from Wikidata ; and ( 2 ) Hypernymy relations from a KB which integrates several Open Information Extraction ( OIE ) systems .
Since our method uses a very large semantic network as reference sense inventory , we are able to perform jointly hypernym extraction and dis ambiguation , from which 1 The terminology is not entirely unified in this respect .
In addition to pattern - based , other terms like path - based or rule - based are also used .
2 Data and source code available from the following link : www.taln.upf.edu/taxoembed . expanding existing ontologies becomes a trivial task .
Compared to word - level taxonomy learning , TAXO - EMBED results in more refined and unambiguous hypernymic relations at the sense level , with a direct application in tasks such as semantic search .
Evaluation ( both manual and automatic ) shows that we can effectively replicate the Wikidata is - a branch , and capture previously unseen relations in other reference taxonomies ( YAGO or WIBI ) .
Related Work
Pattern - based methods for hypernym identification exploit the joint co-ocurrence of term and hypernym in text corpora .
Building upon Hearst 's patterns , these approaches have focused on , for instance , exploiting templates for harvesting candidate instances which are ranked via mutual information , training a classifier with WordNet hypernymic relations combined with syntactic dependencies , or applying a doubly - anchored method , which queries the web with two semantically related terms for collecting domainspecific corpora .
Syntactic information is also used for supervised definition and hypernym extraction , or together with Wikipedia - specific heuristics .
One of the main drawbacks of these methods is that they require both term and hypernym to co-occur in text within a certain window , which strongly hinders their recall .
Higher recall can be achieved thanks to distributional methods , as they do not have co-occurrence requirements .
In addition , they can be tailored to cover any number of predefined semantic relations such as cohyponymy or meronymy , but also cause - effect or entity - origin ) .
However , they are often more imprecise and seem to perform best in discovering broader semantic relations .
One way to surmount the issue of generality was proposed by , who explored the possibility to learn a hypernymic transformation matrix over a word embeddings space .
As shown empirically in Fu et al. 's original work , the hypernymic relation that holds for the pair ( dragonfly , insect ) differs from the one of e.g. ( carpenter , man ) .
Prior to training , their system addresses this discrepancy via k-means clustering using a held - out development set for tuning .
The previously described methods for hypernym and taxonomy learning operate inherently at the surface level .
This is partly due to the way evaluation is conducted , which is often limited to very specific domains with no integrative potential ( e.g. taxonomies in food , science or equipment from ) , or restricted to lists of word pairs .
Hence , a drawback of surface - level taxonomy learning , apart from ambiguity issues , is that they require additional and error - prone steps to identify semantic clusters .
Alternatively , recent advances in OIE based on dis ambiguation and deeper semantic analysis have shown their potential to construct taxonomized dis ambiguated resources both at node and at relation level .
However , in addition to their inherently broader scope , OIE approaches are designed to achieve high coverage , and hence they tend to produce noisier data compared to taxonomy learning systems .
In our sense - based approach , instead , not only do we leverage an unambiguous vector representation for hypernym discovery , but we also take advantage of a domain - wise clustering strategy to directly obtain specific term - hypernym training pairs , thereby substantially refining this step .
Additionally , we exploit the complementary knowledge of OIE systems by incorporating high - confidence relation triples drawn from OIE - derived resources , yielding the best average configuration as evaluated on ten different domains of knowledge .
Preliminaries
TAXOEMBED leverages the vast amounts of training data available from structured and unstructured knowledge resources , along with the mapping among these resources and a state - of - the - art vector representation of word senses .
BabelNet 3 ( Navigli and Ponzetto , 2012 ) constitutes our sense inventory , as it is currently the largest single multilingual repository of named en - 3
http://babelnet.org tities and concepts , integrating various resources such as WordNet , Wikipedia or Wikidata .
As in WordNet , BabelNet is structured in synsets .
Each synset is composed of a set of words ( lexicalizations or senses ) representing the same meaning .
For instance , the synset referring to the members of a business organization is represented by the set of senses firm , house , business firm .
BabelNet contains around 14M synsets in total .
We exploit BabelNet 4 as ( 1 ) A repository for the manually - curated hypernymic relations included in Wikidata ; ( 2 ) A semantic pivot of the integration of several OIE systems into one single resource , namely KB - UNIFY ; and ( 3 ) A sense inventory for the SENSEMBED vector representations .
In the following we provide further details about each of these resources .
Training Data
Wikidata 5 ) is a document - oriented semantic data base operated by the Wikimedia Foundation with the goal of providing a common source of data that can be used by other Wikimedia projects .
Our initial training set W consists of the hypernym branch of Wikidata , specifically the version included in BabelNet .
Each term - hypernym ?
Wis in fact a pair of BabelNet synsets , e.g. the synset for Apple ( with the company sense ) , and the concept company .
KB - UNIFY 6 ( Delli Bovi et al. , 2015 a ) ( KB - U ) is a knowledge - based approach , based on BabelNet , for integrating the output of different OIE systems into a single unified and dis ambiguated knowledge repository .
The unification algorithm takes as input a set K of OIE - derived resources , each of which is modeled as a set of entity , relation , entity triples , and comprises two subsequent stages : in the first dis ambiguation stage , each KB in K is linked to the sense inventory of Babel Net by dis ambiguating its relation argument pairs ; in the following alignment stage , equivalent relations across different KB in K are merged together .
As a result , KB - U generates a KB of triples where arguments are linked to the corresponding BabelNet synsets , and relations are replaced by relation synsets of semantically similar OIE - derived relation patterns .
The original experimental setup of KB - UNIFY included NELL as one of its input resources : since NELL features its own manually - built taxonomic structure and relation type inventory ( hence it s own is - a relation type ) , we identified the relation synset containing NELL 's is - a 7 and then drew from the unified KB all the corresponding triples , which we denote as K .
These triples constitute , similarly as in the previous case , a set of term - hypernym pairs automatically extracted from OIE - derived resources , with a dis ambiguation confidence of above 0.9 according to the dis ambiguation strategy described in the original paper .
Initially , | W| = 5,301,867 and | K | = 1,358,949 .
Sense vectors
SENSEMBED 8 constitutes the sense embeddings space that we use for training our hypernym detection algorithm .
Vectors in the SENSEMBED space , denoted as S , are latent continuous representations of word senses based on the Word2Vec architecture , which was applied on a dis ambiguated Wikipedia corpus .
Each vector v ?
S represents a BabelNet sense , i.e. a synset along with one of its lexicalizations ( e.g. album_chart_bn:00002488 n ) .
This differs from unsupervised approaches that learn sense representations from text corpora only and are not mapped to any lexical resource , limiting their application in our task .
Methodology
Our approach can be summarized as follows .
First , we take advantage of a clustering algorithm for allocating each BabelNet synset of the training set into a domain cluster C ( Section 4.1 ) .
Then , we expand the training set by exploiting the different lexicalizations available for each BabelNet synset ( Section 4.2 ) .
Finally , we learn a cluster - wise linear projection ( a hypernym transformation matrix ) over all pairs ( term-hypernym ) of the expanded training set ( Section 4.3 ) .
induced semantic clusters via kmeans , where k was tuned on a development set .
Instead , we aim at learning a function sensitive to a predefined knowledge domain , under the assumption that vectors clustered with this criterion are likely to exhibit similar semantic properties ( e.g. similarity ) .
First , we allocate each synset into its most representative domain , which is achieved by exploiting the set of thirty four domains available in the Wikipedia featured articles page 9 .
Warfare , transport , or music are some of these domains .
In the Wikipedia featured articles page each domain is composed of 128 Wikipedia pages on average .
Then , in order to expand the set of concepts associated with each domain , we leverage NASARI 10 ( Camacho - Collados et al. , 2015 ) , a distributional approach that has been used to construct explicit vector representations of BabelNet synsets .
Domain Clustering
Our goal is to associate BabelNet synsets with domains .
To this end , we follow Camacho - Collados et al. and build a lexical vector for each Wikipedia domain by concatenating all Wikipedia pages representing the given domain into a single text .
Finally , given a BabelNet synset b , we calculate the similarity between its corresponding NASARI lexical vector and all the domain vectors , selecting the domain leading to the highest similarity score :
where
Dis the set of all thirty - three domains , d is the vector of the domain d ?
D , b is the vector of the BabelNet synset b , and WO refers to the Weighted Overlap comparison measure , which is defined as follows :
where rank w , vi is the rank of the word win the vector vi according to its weight , and O is the set of overlapping words between the two vectors .
In order to have a highly reliable set of domain labels , those synsets whose maximum similarity score is below a certain threshold are not annotated with any domain .
We fixed the threshold to 0.35 , which provided a fine balance between precision ( estimated in around 85 % ) and recall in our development set .
By following this approach almost 2 million synsets are labelled with a domain .
Training Data Expansion
Prior to training our model , we benefit from the fact that a given BabelNet synset maybe associated with a fixed number of lexicalizations or senses , i.e. different ways of referring to the same concept , to expand our set of training pairs .
For instance , the synset b associated with the concept mu-sic_album is represented by the set of lexicalizations L b = { album , music_album . . . album_project }.
We take advantage of this synset representation to expand each term - hypernym synset pair .
For each term - hypernym pair , both concepts are expanded to their given lexicalizations and thus , each synset pair term - hypernym in the training data is expanded to a set of | L t |.| L h | sense training pairs .
This expansion step results in much larger sets W * and K * , where | W * | = 18,291,330 and | K * | = 15,362,268 .
Specifically , they are 3 and 11 times bigger than the original training sets described in Section 3.1 .
These numbers are higher than those reported in recent approaches for hypernym detection , which exploited Chinese semantic thesauri along with manual validation of hypernym pairs ( obtaining a total of 1,391 instances ) , or pairs from knowledge resources such as Wikidata , Yago , WordNet and DBpedia , where the maximum reported split for training data ( 70 % ) amounted to 49,475 pairs .
Learning a Hypernym Detection Matrix
The gist of our approach lies on the property of current semantic vector space models to capture relations between vectors , in our case hypernymy .
This can be found even in disjoint spaces , where this property has been exploited for machine translation or language normalization .
For our purposes , however , instead of learning a global linear transformation function in two spaces over a broad relation like hypernymy , we learn a function sensitive to a given do - main of knowledge .
Thus , our training data becomes restricted to those term - hypernym BabelNet sense pairs
Under the intuition that there exists a matrix ?
so that yd = ? x d , we learn a transformation matrix for each domain cluster
Cd by minimizing :
Then , for any unseen term x d , we obtain a ranked list of the most likely hypernyms of its lexicalization vectors x j d , using as measure cosine similarity :
At this point , we have associated with each sense vector a ranked list of candidate hypernym vectors .
However , in the ( frequent ) cases in which one synset has more than one lexicalization , we need to condense the results into one single list of candidates , which we achieve with a simple ranking function ? ( ) , which we compute as ?( v ) = cos ( v , ? C x d ) rank ( v ) , where rank ( v ) is the rank of v according to its cosine similarity with ?
C x d .
The above operations allow us to cast the hypernym detection task as a ranking problem .
This is also particularly interesting to enable a flexible evaluation framework where we can combine highly demanding metrics for the quality of the candidate given at a certain rank , as well as other measures which consider the rank of the first valid retrieved candidate .
Evaluation
The performance of TAXOEMBED is evaluated by conducting several experiments , both automatic and manual .
Specifically , we assess its ability to return valid hypernyms for a given unseen term with a held - out evaluation dataset of 250 Wikidata termhypernym pairs ( Section 5.1 ) .
In addition , we assess the extent to which TAXOEMBED is able to correctly identify hypernyms outside of Wikidata ( Section 5.2 ) .
Experiment 1 : Automatic Evaluation
Experimental setting
For this experiment we use two configurations of TAXOEMBED : the first one includes 25 k domainwise expanded training pairs ( TaxE 25 k ) , whereas the second one adds 1 k pairs from KB - U ( TaxE 25 k + K d ) .
We randomly extract 200 test BabelNet synsets ( 20 per domain ) whose hypernyms are missing in Wikidata .
We compare against a number of taxonomy learning and Information Extraction systems , namely , WiBi and DefIE .
Yago and WiBi are used as upper bounds due to the nature of their hypernymic relations .
They include a great number of manually - encoded taxonomies ( e.g. exploiting WordNet and Wikipedia categories ) .
Yago derives its taxonomic relations from an automatic mapping between WordNet and Wikipedia categories .
WiBi , on the other hand , exploits , among a number of different Wikipedia - specific heuristics , categories and the syntactic structure of the introductory sentence of Wikipedia pages .
Finally , DefIE is an automaic OIE system relying on the syntactic structure of pre-dis ambiguated definitions 13 .
Three annotators manually evaluated the validity of the hypernyms extracted by each system ( one per test instance ) .
shows the results of TAXOEMBED and all comparison systems .
As expected , Yago and WiBi achieve the best over all results .
However , TAXOEM - BED , based solely on distributional information , performed competitively in detecting new hypernyms when compared to DefIE , improving its recall in most domains , and even surpassing Yago in technical areas like biology or health .
However , our model does not perform particularly well on media and physics .
In most domains our model is able to discover novel hypernym relations thatare not captured by any other system ( e.g. therapy for radiation treatment planning in the health domain or decoration for molding in the art domain ) .
Results and discussion
In fact , the overlap between our approach and the remaining systems is actually quite small ( on average less than 25 % with all of them on the Extra - Coverage experiment ) .
This is mainly due to the fact that TAXOEMBED only exploits distributional information and does not make use of predefined syntactic heuristics , suggesting that the information it provides and the rule - based comparison systems maybe complementary .
We foresee a potential avenue focused on combining a supervised distributional approach such as TAXOEMBED with syntacticallymotivated systems such as Wibi or Yago .
This combination of a distributional system and manual patterns was already introduced by Shwartz et al .
( 2016 ) on the hypernym detection task with highly encouraging results .
Experiment 2 : Extra-Coverage
In this experiment we evaluate the performance of TAXOEMBED on instances not included in Wikidata .
We describe the experimental setting in Section 5.2.1 and present the results in Section 5.2.2 .
Conclusion
We have presented TAXOEMBED , a supervised taxonomy learning framework exploiting the property that was observed in , namely that there exists , for a given domain - specific terminology , a shared linear projection among termhypernym pairs .
We showed how this can be used to learn a hypernym transformation matrix for discovering novel is - a relations , which are the backbone of lexical taxonomies .
First , we allocate almost 2M BabelNet synsets into a predefined domain of knowledge .
Then , we collect training data both from a manually constructed knowledge base ( Wiki - : Precision , recall and F - Measure between TAXOEMBED , two taxonomy learning systems ( Yago and WiBi ) , and a pattern - based approach that performs hypernym extraction ( DefIE ) .
data ) , and from OIE systems .
We substantially expand our initial training set by expanding both terms and hypernyms to all their available senses , and in a last step , to their corresponding dis ambiguated vector representations .
Evaluation shows that the general trend is that our hypernym matrix improves as we increase training data .
Our best domain - wise configuration combines 25 k training pairs from Wikidata and additional pairs from an OIE - derived KB , achieving promising results .
The domains in which the addition of the OIE - based information contributed the most are education , transport and media .
For instance , in the case of education , this maybe due to the over representation of the North American educational system in IE systems like NELL .
We accompany this quantitative evaluation with manual assessment of precision of false positives , and an analysis of the potential coverage comparing it with knowledge taxonomies like Yago or WiBi , and with DefIE , a quasi - OIE system .
Future Work
For future work we are planning to apply this strategy to learn large - scale semantic relations beyond hypernymy .
This may constitute a first step towards a global and fully automatic ontology learning system .
In the context of semantic web , we would like to include semantic parsers and distant supervision to our algorithm in order to capture n-ary relations between pairs of concepts to further create and improve existing KBs .
As mentioned in Section 5.2.2 , we are also planning to combine our distributional approach with rule - based heuristics , following the line of work introduced by .
Finally , we see potential in the domain clustering approach for improving graph - based taxonomy learning systems , as it can serve as a weighting measure as to how pertinent a given set of concepts in a taxonomy are for a specific domain .
