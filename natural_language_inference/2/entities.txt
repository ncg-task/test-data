80	4	14	spaCy tool
80	20	27	used to
80	28	36	tokenize
80	37	52	all the dataset
80	57	64	PyTorch
80	68	75	used to
80	76	85	implement
80	86	96	our models
84	3	21	lexicon embeddings
84	22	25	are
84	26	45	d =600 - dimensions
85	4	13	embedding
85	14	17	for
85	22	43	out - of - vocabulary
85	47	53	zeroed
86	4	15	hidden size
86	16	18	of
86	19	23	LSTM
86	24	26	in
86	31	82	contextual encoding layer , memory generation layer
86	86	92	set to
86	93	96	128
86	108	118	input size
86	119	121	of
86	122	134	output layer
86	138	158	1024 ( 128 * 2 * 4 )
87	4	19	projection size
87	20	22	in
87	27	42	attention layer
87	46	52	set to
87	53	56	256
89	4	16	dropout rate
89	20	23	0.2
89	34	46	dropout mask
89	50	63	fixed through
89	64	74	time steps
89	75	77	in
89	78	82	LSTM
90	4	21	mini - batch size
90	25	31	set to
90	32	34	32
91	0	13	Our optimizer
91	14	16	is
91	17	23	Adamax
91	32	45	learning rate
91	49	63	initialized as
91	64	69	0.002
91	74	86	decreased by
91	87	90	0.5
91	91	96	after
91	97	111	each 10 epochs
81	3	6	fix
81	7	21	word embedding
81	22	26	with
81	27	63	300 - dimensional GloVe word vectors
82	0	3	For
82	8	26	character encoding
82	32	35	use
82	38	51	concatenation
82	52	54	of
82	59	97	multi-filter Convolutional Neural Nets
82	98	102	with
82	103	110	windows
82	111	120	1 , 3 , 5
82	129	140	hidden size
82	141	155	50 , 100 , 150
88	3	10	speedup
88	11	19	training
88	25	28	use
88	29	49	weight normalization
17	101	108	explore
17	113	144	multi-step inference strategies
17	55	57	on
17	148	151	NLI
18	73	82	maintains
18	85	90	state
18	95	114	iteratively refines
18	119	130	predictions
2	31	57	Natural Language Inference
9	52	90	recognizing textual entailment ( RTE )
101	0	5	shows
101	11	31	our multi-step model
101	32	56	consistently outperforms
101	61	80	single - step model
101	81	83	on
101	88	95	dev set
101	96	98	of
101	99	116	all four datasets
101	117	128	in terms of
101	129	137	accuracy
102	14	16	on
102	17	32	SciTail dataset
102	35	38	SAN
102	39	50	outperforms
102	55	74	single - step model
131	23	92	most challenging " Long Sentence " and " Quantity / Time " categories
131	95	108	SAN 's result
131	112	132	substantially better
131	133	137	than
131	138	154	previous systems
105	0	2	On
105	3	18	SciTail dataset
105	21	24	SAN
105	30	41	outperforms
105	42	45	GPT
108	0	14	Comparing with
108	15	37	Single - step baseline
108	44	58	proposed model
108	59	66	obtains
108	67	84	+ 2.8 improvement
108	85	87	on
108	92	111	Sc - iTail test set
108	114	126	94.0 vs 91.2
108	133	150	+ 2.1 improvement
108	151	153	on
108	158	173	SciTail dev set
108	176	188	96.1 vs 93.9
130	15	28	Chen 's model
130	35	54	biggest improvement
130	55	57	of
130	58	61	SAN
130	94	96	on
130	154	174	" Antonym " category
128	0	9	Our model
128	10	21	outperforms
128	26	37	best system
128	38	40	in
128	41	53	RepEval 2017
128	69	78	except on
128	79	130	" Conditional " and " Tense Difference " categories
129	8	12	find
129	18	21	SAN
129	22	27	works
129	28	42	extremely well
129	43	45	on
129	46	96	" Active / Passive " and " Paraphrase " categories
