(Contribution||has||Experimental setup)
(Experimental setup||fix||word embedding)
(word embedding||with||300 - dimensional GloVe word vectors)
(Experimental setup||For||character encoding)
(character encoding||use||concatenation)
(concatenation||of||multi-filter Convolutional Neural Nets)
(multi-filter Convolutional Neural Nets||with||hidden size)
(hidden size||has||50 , 100 , 150)
(multi-filter Convolutional Neural Nets||with||windows)
(windows||has||1 , 3 , 5)
(Experimental setup||speedup||training)
(training||use||weight normalization)
(Experimental setup||has||PyTorch)
(PyTorch||used to||implement)
(implement||has||our models)
(Experimental setup||has||projection size)
(projection size||in||attention layer)
(projection size||set to||256)
(Experimental setup||has||dropout mask)
(dropout mask||fixed through||time steps)
(time steps||in||LSTM)
(Experimental setup||has||mini - batch size)
(mini - batch size||set to||32)
(Experimental setup||has||Our optimizer)
(Our optimizer||is||Adamax)
(Experimental setup||has||learning rate)
(learning rate||initialized as||0.002)
(learning rate||decreased by||0.5)
(0.5||after||each 10 epochs)
(Experimental setup||has||input size)
(input size||of||output layer)
(output layer||has||1024 ( 128 * 2 * 4 ))
(Experimental setup||has||spaCy tool)
(spaCy tool||used to||tokenize)
(tokenize||has||all the dataset)
(Experimental setup||has||embedding)
(embedding||for||out - of - vocabulary)
(out - of - vocabulary||has||zeroed)
(Experimental setup||has||hidden size)
(hidden size||in||contextual encoding layer , memory generation layer)
(contextual encoding layer , memory generation layer||set to||128)
(hidden size||of||LSTM)
(Experimental setup||has||dropout rate)
(dropout rate||has||0.2)
(Experimental setup||has||lexicon embeddings)
(lexicon embeddings||are||d =600 - dimensions)
