title
Stochastic Answer Networks for Natural Language Inference
abstract
We utilize a stochastic answer network ( SAN ) to explore multi-step inference strategies in Natural Language Inference .
Rather than directly predicting the results given the inputs , the model maintains a state and iteratively refines its predictions .
This can potentially model more complex inferences than the existing single - step inference methods .
Our experiments show that SAN achieves state - of - the - art results on four benchmarks : Stanford Natural Language Inference ( SNLI ) , MultiGenre Natural Language Inference ( MultiNLI ) , SciTail , and Quora Question Pairs datasets .
Motivation
The natural language inference task , also known as recognizing textual entailment ( RTE ) , is to infer the relation between a pair of sentences ( e.g. , premise and hypothesis ) .
This task is challenging , since it requires a model to fully understand the sentence meaning , ( i.e. , lexical and compositional semantics ) .
For instance , the following example from MultiNLI dataset illustrates the need fora form of multi-step synthesis of information between premise :
" If you need this book , it is probably too late unless you are about to take an SAT or GRE . " , and hypothesis :
" It 's never too late , unless you 're about to take a test . "
To predict the correct relation between these two sentences , the model needs to first infer that " SAT or GRE " is a " test " , and then pick the correct relation , e.g. , contradiction .
This kind of iterative process can be viewed as a form of multi-step inference .
To best of our knowledge , all of works on NLI use a single step inference .
Inspired by the recent success of multi-step inference on Machine Reading Comprehension ( MRC ) , we explore the multi-step inference strategies on NLI .
Rather than directly predicting the results given the inputs , the model maintains a state and iteratively refines its predictions .
We show that our model outperforms single - step inference and further achieves the state - of - the - art on SNLI , MultiNLI , SciTail , and Quora Question Pairs datasets .
Multi- step inference with SAN
The natural language inference task as defined here involves a premise P = {p 0 , p 1 , ... , p m?1 } of m words and a hypothesis H = {h 0 , h 1 , ... , h n?1 } of n words , and aims to find a logic relationship R between P and H , which is one of labels in a close set : entailment , neutral and contradiction .
The goal is to learn a model f ( P , H ) ?
R.
Ina single - step inference architecture , the model directly predicts R given P and H as input .
In our multi-step inference architecture , we additionally incorporate a recurrent state st ; the model processes multiple passes through P and H , iteratively refining the state st , before finally generating the output at step t = T , where T is an a priori chosen limit on the number of inference steps .
describes in detail the architecture of the stochastic answer network ( SAN ) used in this study ; this model is adapted from the MRC multistep inference literature .
Compared to the original SAN for MRC , in the SAN for NLI we simplify the bottom layers and Selfattention layers since the length of the premise and hypothesis is short ) .
We also modify the answer module from prediction a text span to an NLI classification label .
Overall , it contains four different layers :
1 ) the lexicon encoding layer computes word representations ; 2 ) the contextual encoding layer modifies these representations in context ; 3 ) the memory generation layer gathers all information from the premise and hypothesis and forms a
Information Gathering Layer s t - 1 s t s t +1 s t - 1 s t s t+1
Memory : Architecture of the Stochastic Answer Network ( SAN ) for Natural Language Inference .
" working memory " for the final answer module ; 4 ) the final answer module , a type of multi-step network , predicts the relation between the premise and hypothesis .
Lexicon Encoding Layer .
First we concatenate word embeddings and character embeddings to handle the out - of - vocabulary words
1 .
Following , we use two separate twolayer position - wise feedforward network to obtain the final lexicon embedings , E p ?
R dm and E h ?
R dn , for the tokens in P and H , respectively .
Here , dis the hidden size .
Contextual Encoding Layer .
Two stacked BiL - STM layers are used on the lexicon encoding layer to encode the context information for each word in both P and H . Due to the bidirectional layer , it doubles the hidden size .
We use a maxout layer ( Goodfellow et al. , 2013 ) on the BiLSTM to shrink its output into its original hidden size .
By a concatenation of the outputs of two BiLSTM layers , we obtain C p ?
R 2 dm and Ch ?
R 2 dn as representation of P and H , respectively .
We omit POS Tagging and Name Entity Features for simplicity Memory Layer .
We construct our working memory via an attention mechanism .
First , a dotproduct attention is adopted like in to measure the similarity between the tokens in P and H. Instead of using a scalar to normalize the scores as in , we use a layer projection to transform the contextual information of both C p and Ch :
where A is an attention matrix , and dropout is applied for smoothing .
Note that ?
p and ?
h is transformed from C p and Ch by one layer neural network ReLU ( W 3 x ) , respectively .
Next , we gather all the information on premise and hypothesis by : U p = C p ; Ch A ? R 4 dm and U h = Ch ; C p A ? R 4 dn .
The semicolon ; indicates vector / matrix concatenation ; A is the transpose of A. Last , the working memory of the premise and hypothesis is generated by using a BiLSTM based on all the information gathered :
Formally , our answer module will compute over T memory steps and output the relation label .
At the beginning , the initial state s 0 is the summary of the M h :
Here , x t is computed from the previous state s t?1 and memory M p :
.
Following , one layer classifier is used to determine the relation at each step t ? { 0 , 1 , . . . , T ? 1}.
At last , we utilize all of the T outputs by averaging the scores :
Each
Pr t is a probability distribution overall the relations , { 1 , . . . , | R |}.
During training , we apply stochastic prediction dropout before the above averaging operation .
During decoding , we average all outputs to improve robustness .
This stochastic prediction dropout is similar in motivation to the dropout introduced by ( Srivastava et al. , 2014 ) .
The difference is that theirs is dropout at the intermediate node - level , whereas ours is dropout at the final layer - level .
Dropout at the node - level prevents correlation between features .
Dropout at the final layer level , where randomness is introduced to the averaging of predictions , prevents our model from relying exclusively on a particular step to generate correct output .
Experiments
Dataset
Here , we evaluate our model in terms of accuracy on four benchmark datasets .
SNLI ( Bowman et al. , 2015 ) contains 570 k human annotated sentence pairs , in which the premises are drawn from the captions of the Flickr30 corpus , and hypothesis are manually annotated .
MultiNLI contains 433 k sentence pairs , which are collected similarly as SNLI .
However , the premises are collected from abroad range of genre of American English .
The test and development sets are further divided into in - domain ( matched ) and cross - domain ( mismatched ) sets .
The Quora Question Pairs dataset is proposed for paraphrase identification .
It contains Note that it only contains two types of labels , so is a binary task .
Implementation details
The spaCy tool 2 is used to tokenize all the dataset and PyTorch is used to implement our models .
We fix word embedding with 300 - dimensional GloVe word vectors .
For the character encoding , we use a concatenation of the multi-filter Convolutional Neural Nets with windows 1 , 3 , 5 and the hidden size 50 , 100 , 150 .
3
So lexicon embeddings are d =600 - dimensions .
The embedding for the out - of - vocabulary is zeroed .
The hidden size of LSTM in the contextual encoding layer , memory generation layer is set to 128 , thus the input size of output layer is 1024 ( 128 * 2 * 4 ) as Eq 2 .
The projection size in the attention layer is set to 256 .
To speedup training , we use weight normalization .
The dropout rate is 0.2 , and the dropout mask is fixed through time steps in LSTM .
The mini - batch size is set to 32 .
Our optimizer is Adamax and its learning rate is initialized as 0.002 and decreased by 0.5 after each 10 epochs .
Results
One main question which we would like to address is whether the multi-step inference help on NLI .
We fixed the lower layer and only compare different architectures for the output layer :
1 .
Single-step : Predict the relation using Eq 2 based on s 0 and x 0 .
Here ,
SAN :
The multi-step inference model .
We use 5 - steps with the prediction dropout rate 0.2 on the all experiments .
shows that our multi-step model consistently outperforms the single - step model on the dev set of all four datasets in terms of accuracy .
For example , on SciTail dataset , SAN outperforms the single - step model by .
We compare our results with the state - of - the - art in and BERT use a large amount of external knowledge or a large scale pretrained contextual embeddings .
However , SAN is still competitive these models .
On SciTail dataset , SAN even outperforms GPT .
Due to the space limitation , we only list two top models .
We further utilize BERT as a feature extractor 6 and use the SAN answer module on top of it .
Comparing with Single - step baseline , the proposed model obtains + 2.8 improvement on the Sc - iTail test set ( 94.0 vs 91.2 ) and + 2.1 improvement on the SciTail dev set ( 96.1 vs 93.9 ) .
This shows the generalization of the proposed model which can be easily adapted on other models
7 .
Analysis :
How many steps it needs ?
We search the number of steps t from 1 to 10 .
We observe that when t increases , our model obtains a better improvement ( e.g. , 86.7 ( t = 2 ) ) ; however when t = 5 or t = 6 , it achieves best results ( 89.4 ) on SciTail dev set and then begins to downgrade
4 For direct comparison , this has the same three lower layers as We run BERT ( the base model ) to extract embeddings of both premise and hypothesis and then feed it to answer models fora fair comparison .
7 Due to highly time consumption and space limitation , we omit the results using BERT on SNLI / MNLI / Quora Question dataset .
Model
MultiNLI Test Matched Mismatched DIIN 78.8 77.8 BERT 85.9 SAN 79.3 78.7 SNLI Dataset ( Accuracy % ) ESIM+ELM o 88.7 GPT 89.9 SAN 88.7
Quora Question Dataset ( Accuracy% ) 88.4 89.1 SAN 89.4 SciTail Dataset ( Accuracy% ) 77.3 GPT 88.3 SAN 88.4 the performance .
Thus , we sett = 5 in all our experiments .
We also looked internals of our answer module by dumping predictions of each step ( the max step is set to 5 ) .
Here is an example 8 from MutiNLI dev set .
Our model produces total 5 labels at each step and makes the final decision by voting neutral .
Surprising , we found that human annotators also gave different 5 labels : contradiction , neutral , neutral , neutral , neutral .
It shows robustness of our model which uses collective wise .
Finally , we analyze our model on the annotated subset 9 of development set of MultiNLI .
It contains 1,000 examples , each tagged by categories shown in .
Our model outperforms the best system in RepEval 2017 inmost cases , except on " Conditional " and " Tense Difference " categories .
We also find that SAN works extremely well on " Active / Passive " and " Paraphrase " categories .
Comparing with Chen 's model , the biggest improvement of SAN ( 50 % vs 77 % and 58 % vs 85 % on Matched and Mismatched settings respectively ) is on the " Antonym " category .
In particular , on the most challenging " Long Sentence " and " Quantity / Time " categories , SAN 's result is substantially better than previous systems .
This demonstrates the robustness of multi-step inference .
Conclusion
We explored the use of multi-step inference in natural language inference by proposing a stochastic answer network ( SAN ) .
Rather than directly predicting the results ( e.g. relation R such as entailment or not ) given the input premise P and hypothesis H , SAN maintains a state st , which it iteratively refines over multiple passes on P and H in order to make a prediction .
Our state - of - theart results on four benchmarks ( SNLI , MultiNLI , SciTail , Quora Question Pairs ) show the effectiveness of this multi-step inference architecture .
In future , we would like to incorporate the pertrained contextual embedding , e.g. , and GPT into our model and multi-task learning .
