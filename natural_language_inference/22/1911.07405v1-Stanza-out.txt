title
Multi- task Sentence Encoding Model for Semantic Retrieval in Question Answering Systems
abstract
Question Answering ( QA ) systems are used to provide proper responses to users ' questions automatically .
Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification ( PI ) problem .
Given a question , the aim of the task is to find the most similar question from a QA knowledge base .
In this paper , we propose a Multi - task Sentence Encoding Model ( MSEM ) for the PI problem , wherein a connected graph is employed to depict the relation between sentences , and a multi-task learning model is applied to address both the sentence matching and sentence intent classification problem .
In addition , we implement a general semantic retrieval framework that combines our proposed model and the Approximate Nearest Neighbor ( ANN ) technology , which enables us to find the most similar question from all available candidates very quickly during online serving .
The experiments show the superiority of our proposed method as compared with the existing sentence matching models .
I. INTRODUCTION
Question answering systems have been widely studied in both the academic and industrial community and are widely applied to various scenarios .
There are full - blown applications like Amazon 's Alexa , Apple 's Siri , Baidu 's DuerOS , Google 's Assistant and Microsoft 's Cortana .
Generally , there are two types of question answering systems : ( 1 ) information retrievalbased ( IR - based ) , and ( 2 ) generation - based .
In this work , we focus on building an IR - based QA system to answer the Frequently Asked Questions ( FAQ ) .
The critical part of IRbased QA system is to find the most similar question from a massive QA knowledge base , which could be further reformulated as a Paraphrase Identification ( PI ) problem , also known as sentence matching .
In recent years , neural network models have achieved great success in sentence matching .
Depends on whether to use crosssentence features or not , sentence - matching models can be classified roughly into two types : ( 1 ) encoding - based , and ( 2 ) interaction - based .
It is generally accepted that the interactionbased models could get better performance than the encodingbased models on certain datasets because they have abundant interaction features .
However , the leaderboards of published large datasets such as SNLI and MultiNLI encourage conducting research on the encoding - based models around the semantic representation , because the encoding - based models can learn vector representations of individual sentences , which can be further applied to other natural language processing tasks .
Models in practical QA systems have two main disadvantages .
Firstly , they consider the semantic sentence matching as a binary classification problem , assuming that samples are independent of one another as default .
However , the paraphrase relation between sentences could be transmitted .
For example , if question1 and question2 are paraphrases , and question2 and question3 are paraphrases , we can infer that question1 and question3 are also paraphrases .
Secondly , because of the hard time delay constraint in the online prediction procedure of a traditional IR - based QA system , as shown in , existing models often play the role of a re-rank module that depends on the results from the question analysis and recall module .
Thus they could only re-rank a few candidates from term - based index recall modules like Lucene , instead of retrieving the most similar question from all candidates .
In this paper , we aim to address these two challenges .
The main contributions of this work are summarized as follows :
We employ a connected graph to depict the paraphrase relation between sentences for the PI task , and propose a multi-task sentence - encoding model , which solves the paraphrase identification task and the sentence intent classification task simultaneously .
We propose a semantic retrieval framework that integrates the encoding - based sentence matching model with the approximate nearest neighbor search technology , which allows us to find the most similar question very quickly from all available questions , instead of within only a few candidates , in the QA knowledge base .
We evaluated our proposed method on various QA datasets and the experimental results show the effectiveness and superiority of our method .
First , it proves that we can achieve better performance with multi -task learning .
Besides , our method can achieve state - of - the - art performance compared with existing encoding - based models and interaction - based models .
II .
RELATED WORKS
A. Natural Language Sentence Matching
Natural language sentence matching ( NLSM ) has gone through substantial developments in recent years .
It plays a central role in a large number of natural language processing tasks .
For the paraphrase identification ( PI ) task , NLSM is utilized to determine whether two sentences are paraphrases or not .
The developments of deep neural networks and the emergence of large - scale annotated datasets have led great progress on NLSM tasks .
Depending on whether the crosssentence features or attention from one sentence to another were used , two types of deep neural network models have been proposed for NLSM .
The first type of models is encoding - based , where sentences are encoded into sentence vectors without any cross interaction , then the matching decision is made solely based on the two sentence vectors .
Typical representatives of such methods include Stack - augmented Parser - Interpreter Neural Network ( SPINN ) , Shortcut - Stacked Sentence Encoders ( SSE ) , or Gumbel Tree - LSTM .
The other type of methods , called interaction - based model , make use of cross interaction of small units ( such as words ) to express word - level or phrase - level alignments for performance improvements .
The main representatives are Enhanced Sequential Inference Model ( ESIM ) , Bilateral Multi - Perspective Matching Model ( BiMPM ) , and Densely Interactive Inference Network ( DIIN ) model .
Generally , the second type of methods captures more interactive features between the two input sentences , so it can achieve better performance .
On the other hand , the encoding - based model is much smaller and easier to train , and the vector representations can be further used for sentence clustering , semantic search , visualization and many other tasks .
The advantages of encoding - based models are much more significant to QA systems in the industry , so we focus on the research of encoding - based models .
B. Approximate Nearest Neighbor
Approximate nearest neighbor ( ANN ) search has been a hot topic over decades and plays an important role in machine learning , computer vision and information retrieval etc .
For dense real - valued vectors , such as vector representations of images or natural languages , many data structures and algorithms have been proposed to improve the retrieval efficiency of ANN search .
There are four types of mainstream methods , including tree structure based , hashing based , quantization based , and graph based .
In the task of image retrieval , ANN index technologies have been used to build efficient image retrieval systems .
In this paper , inspired by research on image retrieval systems , we propose a semantic retrieval framework for QA systems , which combines ANN search technology and sentence encoding technology .
III .
PROPOSED METHOD
A.
The Overall Architecture
Motivated by the fact that the questions in a large QA knowledge base are not independent , we try to utilize the paraphrase relationship among questions to facilitate modeling of question representation .
Each sentence is regarded as a vertex and an edge is added between a pair of vertices if they are paraphrases .
In this way we can build an undirected graph to represent the paraphrase relationship among sentences , wherein a connected sub - graph can be seen as a sentence cluster with similar intent , as shown in .
On this basis , we could train a multi-class classification model for sentence intent classification .
Two sentences can be considered as paraphrases if they are classified into the same class by the model .
As shows , we employ a multi-task learning method to simultaneously train a sentence matching model and a sentence intent classification model by sharing the sentence encoder between two tasks .
Take Quora Question
Pairs dataset as an instance : input data question1 and question 2 will be encoded as sentence representation , by the sentence encoder .
On the one hand , we use a softmax layer and cross entropy loss function to train the multi-class sentence intent classification model as follows :
is the class index of sentence intent , and M represents the number of classes .
On the other hand , for the convenience of integrating with approximate nearest neighbor ( ANN ) libraries , which only support cosine distance , Euclidean distance , Manhattan distance , Hamming distance , or Dot ( Inner )
Product distance , we use a simple cosine matching layer instead of a more complicated multi -layer perceptron as in .
A = sigmoid. * ( ( , ) ? ) ;
( 3 )
In , ? and are hyperparameters .
Equation is loss function of the matching layer .
The overall loss function is as follows :
wherein ( 5 ) is a hyperparameter for balancing the loss of each task in the multi - task learning and 0 ?
? 1 .
B. Sentence Encoder
The overall architecture of our sentence encoder is illustrated in .
The encoder transforms the input sentence into a fixed - length embedding .
The details of each component in the sentence encoder will be described in the subsections that follow .
C. Word Representation Layer
Word Representation
Layer is consists of word embedding and character representation .
For word embedding , we use pretrained Glo Ve word embeddings to represent each word as a d-dimensional vector .
For character representation , we infuse randomly initialized values to max - pooling convolution layer to compute the character representation of each word .
We concatenate the word embedding and character representation to get the final word representation .
D. Bidirectional Gated Recurrent Unit
We use Bidirectional Gated Recurrent Unit ( BiGRU ) to maintain the sequential information about the sentence being modeled .
BiGRU is consists of a forward directional GRU and a backward directional GRU .
Forward directional GRU process inputs sequence from left to right , backward directional GRU on the contrary .
Let us describe how the tth hidden unit is computed : ?
in denotes the element - wise product .
In GRU cell , context information of tth hidden unit is carried over by the last hidden unit state ?
LR > .
E. Attention Recurrent Unit
We propose an Attention Recurrent Unit ( ARU ) based on the attention mechanism .
As shown in , the term " Gate " stands for the element - wise forget gate .
We apply position - wise multi-head attention to compute the context representation ( self - attention computed here ) , and use it to compute the linear transformation ?
d with the input and the forget gate :
= sigmoid.^ > +^ X ;
Position - wise multi-head attention is a variant of multi-head attention as shown below :
Mpos ? ?
$ is a weight matrix where n is the number of words on sentence , which will be updated during training .
We use a dynamic average ( DA ) pooling to improve the sequence encoding capability further :
After that , we use a highway connection to connect input and output :
Compared with GRU , the computation of the tth hidden unit is no more depending on the last hidden unit state ? LR > , so that ( 10 ) , ( 11 ) , ( 12 ) could be computed in parallel on GPU effectively and ( 15 ) just needs a simple computation .
In this section , we also refer to the works of Quasi - RNN and SRU .
F. Feed - Forward Network
We use a feed - forward network same as the Transformer .
It uses a multi - layer perceptron with two layers and uses activation function ReLU , as follows :
FFN function is applied to each output state of ARU .
The term " Add " in represents the residual connection and the term " Norm " represents the layer normalization .
The output of FFN would be incorporated with the Add & Norm layer to simplify the network 's optimization .
G. Attentive Pooling
We perform an attentive pooling operation over the output of the FFN , which would convert them into a fixed - length vector .
It can be formulated as follows :
= softmax . x X tanh ( x>
7 ) ;
In , ? ?
$ is the output of FFN , where is the number of words in the sentence and is the number of hidden units of ARU .
x> ? ? $ s and x X ? ? s [ are two weight matrices where K and are hyperparameters that could beset manually .
After the attentive pooling layer , the output matrix ? ?
[ $ consists of sentence representations with udimensional vectors .
We concatenate the above vectors and feed it to a highway network with two layers to generate the final sentence representation vector .
shows the proposed semantic retrieval framework , where the encoding - based model plays a very important role and has a great impact on the overall performance .
In the offline system , all questions in the FAQ set are encoded to dense realvalued vectors .
Then we build an ANN vector index by using an ANN tool , such as Annoy , through which we can get the most similar vectors given a vector encoded from any new question .
IV .
SEMANTIC RETRIEVAL FRAMEWORK
A. Framework Overview
In the online system , we deploy the same module to encode the question input by the user .
By inputting the vector to the ANN module , we can get top similar questions with a semantic matching score .
Then the most similar question could be seen as synonymous to the user 's question , so they might share the same answer .
A more complicated rank module could be used following the ANN module to re-rank the top K candidates , such as interaction - based models , or ranking algorithms with handcrafted features .
However , the rank module is less important in our proposed framework than in the traditional IR - based QA frameworks .
B. Analysis
As compared with the traditional IR - based QA frameworks , our framework is less dependent on the general question analysis tools like keyword extraction .
Besides , our framework removes the traditional recall module based on text search engines , which is replaced by the new recall module based on the sentence encoding and ANN technology .
V .
EXPERIMENTS
A. Datasets
We conduct experiments on four sentence matching datasets , each comprising a large set of instances in the form of ? > , X , ? , where > and X are two questions , and is the label indicating whether they are paraphrases or not .
shows a brief description of these datasets .
The overlap rate is a ratio of the number of common words between two sentences in a sample to the average number of them .
Quora Question
Pair dataset is an open - domain English dataset derived from Quora.com .
We use the same split ratio as BiMPM .
LCQMC dataset is an open - domain Chinese dataset collected from Baidu Knows ( a popular Chinese community question answering website ) .
Bank Question ( BQ ) dataset is a specific - domain Chinese dataset for sentence semantic equivalence identification ( SSEI ) .
Telephone customer service ( TCS ) dataset is a specificdomain Chinese dataset from a real - world telephone customer service scenario , where voice speeches are converted into text using the automatic speech recognition technology .
The evaluation metric is accuracy for the Quora dataset , and F1 for other datasets .
B. Settings of Experiments
For Quora dataset , we use the Glove - 840B - 300D vector as the pre-trained word embedding .
The character embedding is randomly initialized with 150 D and the hidden size of BiGRU is set to 300 .
We set = 0.8 in the multi - task loss function .
For the sentence intent classification task , we only keep the sentence clusters with question number greater than 3 , and the remaining sentence clusters with question numberless than or equal to 3 are regarded as a special " other " cluster .
Dropout layer is also applied to the output of the attentive pooling layer , with a dropout rate of 0.1 .
An Adam optimizer is used to optimize all the trainable weights .
The learning rate is set to 4e - 4 and the batch size is set to 200 .
When the performance of the model is no longer improved , an SGD optimizer with a learning rate of 1e - 3 is used to find a better local optimum .
C. Comparing with other methods
We compared our model with the following models :
ESIM : Enhanced Sequential Inference Model is an interaction - based model for natural language inference .
It uses BiLSTM to encode sentence contexts and uses the attention mechanism to calculate the information between two sentences .
ESIM has shown excellent performance on the SNLI dataset .
BiMPM : Bilateral Multi- Perspective Matching model is an interaction - based sentence matching model with superior performance .
The model uses a BiLSTM layer to learn the sentence representation , four different types of multiperspective matching layers to match two sentences , an additional BiLSTM layer to aggregate the matching results , and a two - layer feed - forward network for prediction .
SSE : Shortcut - Stacked Sentence Encoder is an encodingbased sentence - matching model , which enhances multi - layer BiLSTM with short - cut connections .
SSE has been proved to be effective in improving the performance of sentence encoder , recording state - of - the - art performance of the sentence - encoding models on Quora dataset .
DIIN : Densely Interactive Inference Network is an interaction - based model for natural language inference ( NLI ) .
It hierarchically extracts semantic features from interaction space to achieve a high - level understanding of the sentence pair .
It achieves state - of - the - art performance on SNLI dataset and Quora dataset .
a.
The first five rows are copied from and the next two rows are copied from . a.
The first four rows are copied from and the next two rows are reproduced using the SMP_toolkit .
D. Results of Experiments
The results of experiments on four sentence matching datasets are summarized as follows :
Quora dataset : BiMPM and ESIM models without any sentence interaction information , and is very close to DIIN , the state - of - the - art interaction - based model , but we do n't any external knowledge in our method .
LCQMC dataset : Experimental results of LCQMC dataset compared with the existing models are shown in .
The experimental results show that our model outperforms state - of the - art models .
BQ dataset : BQ dataset is a specific - domain dataset with a low average overlap rate .
As shown in , our model outperforms state - of - the - art models by a large margin , reaching 83 . 62 % , recording the state - of - the - art performance .
TCS dataset :
As shown in show that our MSEM model achieves the best performance .
This indicates that our model is also very effective in the spoken question - answering scenario .
To sum up , experimental results show that our proposed model without multi-task learning outperforms SSE , the stateof - the - art encoding - based models , across all four datasets .
And the model with multi-task learning further improved performance ranging from 0.4 % to 1 % .
Compared with existing models , our model shows great advantages on datasets with low average overlap rate , which is known to be very common in realworld question answering scenarios .
E. Ablation Study
The above experiments show the effectiveness of our proposed multi-task training strategy .
In this section , we present the results of an ablation study on Quora dataset for evaluating the contribution of each component of the encoder , as shown in .
Note that we do the significant test for each ablation experiment using the t-test ( p < 0.05 ) .
We first study the contribution of the ARU component .
The accuracy decreases , the accuracy will drop to 88.25 % .
Next we compare the effect of attentive pooling vs max pooling .
It turns out that the attentive pooling is better than max pooling .
Then if we remove the highway network , the accuracy will drop to 88.36 % .
Finally when we remove the character - level embedding , the accuracy will drop to 88.26 % .
A possible reason might be that the character - level embedding can better handle the out - of - vocab ( OOV ) words .
F. Online System Evaluation
We perform an online evaluation with a telephone customer service system .
We randomly select 1138 questions from the system log and send them to a baseline system and the new system , respectively .
The baseline system is similar to what shown in , where the retrieval module is built on Elasticsearch and 30 candidate questions will be recalled and then ranked by the MSEM model .
The new system is similar to what shown in , where an ANN module based on Annoy and a sentence - encoding module based on MSEM are adopted .
We manually evaluate the returned results and measure the performance with the F1 score .
As shown in , the F 1 score of the new system is 14 . 26 % higher than the baseline system .
Obviously , the semantic competence derived from the MSEM module plays a key role in the new system .
G. Case Studies
We perform some case studies using the Quora test set to analyze the effectiveness of the multi -task strategy .
We randomly select 200 sentences with a predicted intent of nonother and manually annotate the correctness of the predicted intent .
We find that the accuracy can reach 96.5 % , indicating that our model can address the intent classification task pretty well .
shows some examples where the MSEM model works , while the MSEM without multi - task fails .
In the first example , although the text similarity between S1 and S2 is low , our model can correctly identify that they have the same intent .
In the second example , S1 and S2 have high text overlap , but the model can correctly identify that they have different intents , which helps our model can better distinguish their semantics .
In the third example , the model classifies S1 as " other " and S2 as " non - other " , which can also help the model distinguish their semantics .
VI .
CONCLUSION
In this paper , we first propose a Multi - task Sentence Encoding Model ( MSEM ) which addresses both the paraphrase identification task and the sentence intent classification task simultaneously , and then further propose a general semantic retrieval framework that combines the sentence encoding model and approximate nearest neighbor search technology , which can find the most similar question very quickly from all available questions in a massive QA knowledge base .
We evaluated our model on several benchmark datasets .
Experimental results show that our proposed method is superior to many recent sentence - matching models .
