{
  "has" : {
    "Experimental setup" : {
      "For" : {
        "Quora dataset" : {
          "use" : {
            "Glove - 840B - 300D vector" : {
              "as" : "pre-trained word embedding"
            }
          },
          "from sentence" : "For Quora dataset , we use the Glove - 840B - 300D vector as the pre-trained word embedding ."
        }
      },
      "has" : {
        "character embedding" : {
          "has" : {
            "randomly initialized" : {
              "with" : "150 D"
            },
            "hidden size" : {
              "of" : {
                "BiGRU" : {
                  "set to" : "300"
                }
              }
            }
          },
          "from sentence" : "The character embedding is randomly initialized with 150 D and the hidden size of BiGRU is set to 300 ."
        },
        "Dropout layer" : {
          "applied to" : {
            "output" : {
              "of" : "attentive pooling layer"
            }
          },
          "with" : {
            "dropout rate" : {
              "of" : "0.1"
            }
          },
          "from sentence" : "Dropout layer is also applied to the output of the attentive pooling layer , with a dropout rate of 0.1 ."
        },
        "Adam optimizer" : {
          "used to" : {
            "optimize" : {
              "has" : "all the trainable weights"
            }
          },
          "from sentence" : "An Adam optimizer is used to optimize all the trainable weights ."
        },
        "learning rate" : {
          "set to" : "4e - 4"
        },
        "batch size" : {
          "set to" : "200",
          "from sentence" : "The learning rate is set to 4e - 4 and the batch size is set to 200 ."
        },
        "SGD optimizer" : {
          "with" : {
            "learning rate" : {
              "of" : "1e - 3",
              "to find" : "better local optimum"
            }
          },
          "from sentence" : "When the performance of the model is no longer improved , an SGD optimizer with a learning rate of 1e - 3 is used to find a better local optimum ."
        }
      },
      "set" : {
        "0.8" : {
          "in" : "multi - task loss function",
          "from sentence" : "We set = 0.8 in the multi - task loss function ."
        }
      }
    }
  }
}