203	24	28	when
203	29	35	node U
203	36	38	is
203	39	45	shared
203	57	63	called
203	66	75	universal
203	83	89	learns
203	90	113	information interaction
203	114	121	between
203	126	146	question and passage
203	164	174	not shared
203	181	192	performance
203	193	210	slightly degraded
206	64	71	sharing
206	72	78	BiLSTM
206	85	101	effective method
206	105	112	improve
206	117	124	quality
206	125	127	of
206	132	139	encoder
207	0	14	After removing
207	19	43	plausible answer pointer
207	50	61	performance
207	67	74	dropped
208	19	34	answer verifier
208	41	52	performance
208	53	60	dropped
208	61	68	greatly
210	0	2	In
210	7	47	second block ( multi - level attention )
210	48	50	of
210	55	62	U - Net
210	75	80	split
210	85	91	output
210	92	94	of
210	99	119	encoded presentation
210	131	143	pass through
210	146	168	self - attention layer
180	3	6	use
180	7	12	Spacy
180	13	23	to process
180	24	49	each question and passage
180	50	59	to obtain
180	60	66	tokens
180	69	77	POS tags
180	80	88	NER tags
180	93	104	lemmas tags
181	7	20	12 dimensions
181	21	29	to embed
181	30	38	POS tags
181	43	46	for
181	47	55	NER tags
182	7	24	3 binary features
182	76	83	between
182	88	108	question and passage
182	27	38	exact match
182	41	59	lower - case match
182	64	75	lemma match
183	7	49	100 - dim Glove pretrained word embeddings
183	54	80	1024 - dim Elmo embeddings
187	7	21	Adam optimizer
187	22	26	with
187	29	42	learning rate
187	43	45	of
187	46	51	0.002
184	0	19	All the LSTM blocks
184	20	23	are
184	24	38	bi-directional
184	39	43	with
184	44	60	one single layer
185	3	6	set
185	11	33	hidden layer dimension
185	34	36	as
185	37	40	125
185	43	68	attention layer dimension
185	69	71	as
185	72	75	250
186	3	8	added
186	11	24	dropout layer
186	25	32	overall
186	37	52	modeling layers
186	55	64	including
186	69	84	embedding layer
186	87	89	at
186	92	104	dropout rate
186	105	107	of
186	108	111	0.3
37	81	98	three sub - tasks
37	101	115	answer pointer
37	118	137	no - answer pointer
37	144	159	answer verifier
42	4	18	universal node
42	19	26	acts on
42	27	52	both question and passage
42	53	61	to learn
42	74	82	question
42	83	85	is
42	86	96	answerable
39	3	10	propose
39	15	22	U - Net
39	23	37	to incorporate
39	44	61	three sub - tasks
39	62	66	into
39	69	82	unified model
40	7	21	answer pointer
40	22	32	to predict
40	35	59	can - didate answer span
40	60	63	for
40	66	74	question
40	83	101	no -answer pointer
40	102	110	to avoid
40	111	120	selecting
40	121	134	any text span
40	135	139	when
40	142	150	question
40	155	164	no answer
40	178	193	answer verifier
40	194	206	to determine
40	211	222	probability
40	223	225	of
40	232	247	unanswerability
40	250	252	of
40	255	263	question
40	264	268	with
40	269	297	candidate answer information
41	23	32	introduce
41	35	49	universal node
41	54	61	process
41	66	98	question and its context passage
41	99	101	as
41	104	140	single contiguous sequence of tokens
41	149	165	greatly improves
41	170	181	conciseness
41	182	184	of
41	185	192	U - Net
2	10	67	Machine Reading Comprehension with Unanswerable Questions
12	0	37	Machine reading comprehension ( MRC )
13	166	169	MRC
191	0	9	Our model
191	64	66	on
191	71	86	development set
191	22	31	F 1 score
191	32	34	of
191	35	39	74.0
191	47	55	EM score
191	56	58	of
191	59	63	70.3
191	141	149	Test set
191	96	105	F 1 score
191	106	108	of
191	109	113	72.6
191	121	129	EM score
191	130	132	of
191	133	137	69.2
192	10	21	outperforms
192	22	53	most of the previous approaches
193	0	12	Comparing to
193	17	42	best - performing systems
193	45	54	our model
193	61	80	simple architecture
193	91	111	end - to - end model
194	10	15	among
194	16	45	all the end - to - end models
194	51	58	achieve
194	63	77	best F1 scores
