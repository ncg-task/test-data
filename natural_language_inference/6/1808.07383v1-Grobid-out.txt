title
Dynamic Self-Attention: Computing Attention over Words Dynamically for Sentence Embedding
abstract
In this paper, we propose Dynamic Self-Attention (DSA), anew self-attention mechanism for sentence embedding. We design DSA by modifying dynamic routing in capsule network (Sabour et al., 2017) for natural language processing. DSA attends to informative words with a dynamic weight vector. We achieve new state-of-the-art results among sentence encoding methods in Stanford Natural Language Inference (SNLI) dataset with the least number of parameters, while showing comparative results in Stanford Sentiment Treebank (SST) dataset.
Introduction
In Natural Language Process (NLP), most neural network-based models contain a sentence encoder to map a sequence of words into a vector. The vector is then used for various downstream tasks, e.g., sentiment analysis, natural language inference, etc. The key part of a sentence encoder is a computation across a variable-length input sequence fora fixed size vector. One of the common approaches is the max-pooling in CNN or RNN.
Self-attention is another approach fora fixed size vector. Self-attention derived from the attention mechanism, originally designed for neural machine translation, is utilized in various tasks. Self-attention computes attention weights by the inner product between words and the learnable weight vector. The weight vector is important in that it detects informative words, yet it is static during inference. The significance of the role of the weight vector casts doubt on whether its being static is an optimal status. * Equal Contribution.
In parallel, recently proposed capsule network for image classification. In capsule network, dynamic routing iteratively computes weights over inputs by the inner product between inputs and a weighted sum of inputs. Varying with the inputs, the weighted sum detects informative inputs; therefore it can be interpreted as a dynamic weight vector from the perspective of self-attention. We expect the dynamic weight vector to give rise to flexibility in self-attention since it can adapt to given sentences even after training.
Motivated by dynamic routing), we propose anew self-attention mechanism for sentence embedding, namely Dynamic Self-Attention (DSA). To this end, we modify dynamic routing such that it functions as self-attention with the dynamic weight vector. DSA, which is stacked on CNN with Dense Connection, achieves new state-of-the-art results among the sentence encoding methods in Stanford Natural Language Inference (SNLI) dataset with the least number of parameters, while obtaining comparative results in Stanford Sentiment Treebank (SST) dataset. It also outperforms recent models in terms of time efficiency due to its simplicity and highly parallelized computations.
Our technical contributions are as follows:
• We design and implement Dynamic Self-Attention (DSA), anew self-attention mechanism for sentence embedding.
• We devise the dynamic weight vector with which DSA computes attention weights.
• We achieve new state-of-the-art results in SNLI dataset, while showing comparative results in SST dataset.

Preliminary
In self-attention, attention weights are computed as follows:
where X ? R dw×n is an input sequence, W ? R dv×dw is a projection matrix and v ? R dv is the learnable weight vector of self-attention. The weight vector v plays an important role, since attention weights are computed by the inner product between v and the projection of the input sequence X. The weight vector v is static with respect to the input sequence X during inference. Replacing the weight vector v with a weight matrix enables multiple attentions.

Our Approach
Our architecture, shown in, is built on CNN with Dense Connection. Dynamic Self-Attention (DSA), which is stacked on CNN with Dense Connection, computes attention weights over words.

CNN with Dense Connection
The goal of this module is to encode each word into a meaningful representation space while capturing local information. We do not add any positional encoding, as suggested by; deep convolution layers capture relative position information. We also enforce every output of layers to have the same number of columns by using appropriate zero padding. We denote a sequence of word embeddings as
is a composite function of the l th layer, composed of 1D Convolution, dropout, and leaky rectified linear unit (Leaky ReLU). We feed a sequence of word embeddings into h 1 (·) with kernel size 1:
where X 1 ? Rd 1 ×n . We add Dense Connection in every layer h l (·) with the same kernel size:
where X l ? Rd l ×n , and l ? [2, L]. We concatenate outputs of all h l (·), and denote it as a single function:
where kernel sizes of all h l (·) in ? k (·) are the same number k, except for h 1 (·). We then feed outputs of two different functions ? k 1 (·), ? k 2 (·), and a sequence of word embeddings X 0 into a compression layer:
where h c (·) is the composite function with kernel size 1. It compresses the first dimension of input (i.e., 2 L l=1 d l + d 0 ) into dc to represent a word compactly. Finally, L 2 norm of every column vector x c i in the X c is normalized, which is found to help our model to converge fast and stably.

Dynamic Self-Attention (DSA)
Dynamic Self-Attention (DSA) iteratively computes attention weights over words with the dynamic weight vector, which varies with inputs. DSA enables multiple attentions in parallel by multiplying different projection matrices to X c , the output from CNN with Dense Connection. For the j th attention, DSA projects the compact representation of every word x c i with LeakyReLU activation:x
where W j ? R do×dc is a projection matrix, b j ? R do is a bias term for the j th attention. Given the number of attentions m, i.e., j ? [1, m], attention weights of words are computed by following Algorithm 1:
Algorithm 1 Dynamic Self-Attention 1: procedure ATTENTION(x j|i , r) 2:
for all i th word, j th attention : q ij = 0 3:
for r iterations do for all i, j : q ij = q ij +x T j|i z j 8:
return all z jr is the number of iterations, and a ij is the attention weight for the i th word in the j th attention. z j is the output for the j th attention of DSA at the r th iteration, and also the dynamic weight vector for the j th attention of DSA before r th iteration. The final sentence embedding z is the concatenation of z 1 , ..., z m :
where z ? R mdo is used for downstream tasks. We modify dynamic routing) to make it function as self-attention with the dynamic weight vector. We remove capsulization layer in capsule network which transforms scalar neurons to capsules, multi-dimensional neurons. A single word is then not decomposed into multiple capsules, but represented as a single vector x c i in Eq. 6. Squashing function is a nonlinear function for capsules. We replace it with Tanh nonlinear function for scalar neurons in Line 6 of Algorithm 1. We also force all the words in the j th attention to share a projection matrix W j in Eq. 6, as an input is a variable-length sequence. By contrast, each capsule in capsule network has its own projection matrix W ij .

Dynamic Weight Vectors
The weight vector v of self-attention in Eq. 1 is static during inference. In DSA, however, the dynamic weight vector z j in Line 6 of Algorithm 1, varies with an input sequencex j|1 , ...,x j|n , even after training. In order to show how the dynamic weight vectors vary, we perform dimensionality reduction on them, z 1 at the (r ? 1) th iteration of Algorithm 1, by Principal Component Analysis (PCA). We randomly select 1,000 sentences from Stanford Natural Language Inference (SNLI) shows that dynamic weight vectors are scattered in all directions. Thus, DSA adapts the dynamic weight vector with respect to each sentence.

Experiments
We evaluate our sentence embedding method with two different tasks: natural language inference and sentiment analysis. We implement single DSA, multiple DSA and self-attention in Eq. 1 as a baseline. Both DSA and self-attention are stacked on CNN with Dense Connection for fair comparison.
For our implementations, we initialize word embeddings by 300D GloVe 840B pretrained vectors, and fix them during training. We use cross-entropy loss as an objective function for both tasks. We set do = 600, m = 1 for single DSA and do = 300, m = 8 for multiple DSA. In Appendix, we provide details for training our implementations, hyperparameter settings, and visualization of attention maps of DSA.

Natural Language Inference Results
Natural language inference is a task of classifying the semantic relationship between two sentences, i.e., a premise and a hypothesis. We conduct experiments on Stanford Natural Language Inference (SNLI) dataset, consisting of human-written 570k pairs of English sentences labeled with one of three classes: Entailment, Contradiction and Neutral. As the task considers the semantic relationship, SNLI is used as a benchmark for evaluating the performance of a sentence encoder.
We follow a conventional approach, called heuristic matching, to classify the relationship of two sentences. The sentences are encoded by our proposed model. Given encoded sentences sh , s p for hypothesis and premise respectively, an input of the classifier Model Train (%) Test (%) Parameters (m) T(s)/epoch 600D BiLSTM with self-attention 84.5 84.2 2.8 -300D Directional self-attention network 91.1 85.6 2.4 587 600D Gumbel TreeLSTM 93.1 86.0 10.0 -600D Residual stacked encoders 91.0 86.0 29.0 -300D Reinforced self-attention network 92.6 86.3 3.1 622 1200D Distance-based self-attention network 89 Model SST-2 SST-5 BiLSTM 87.5 49.5 CNN-non-static 87 is significantly faster than recent models because of its simple structure and highly parallelized computations. With tradeoffs in terms of parameters and learning time per epoch, multiple DSA outperforms other models by a large margin (+1.1%).
In comparison to the baseline, single DSA shows better performance than self-attention (+2.2%). This confirms that the dynamic weight vector is more effective for sentence embedding. Note that our implementation of the baseline, selfattention stacked on CNN with Dense Connection, shows better performance (+0.4%) than the one stacked on BiLSTM.

Sentiment Analysis Results
Sentiment analysis is a task of classifying sentiment in sentences. We use Stanford Sentiment Treebank (SST) dataset, consisting of 10k English sentences, to evaluate our model in singlesentence classification. We experiment SST-2 and SST-5 dataset labeled with binary sentiment labels and five fine-grained labels, respectively.
The SST results are summarized in. We compare single DSA with four baseline models: 1 https://nlp.stanford.edu/projects/snli/ BiLSTM, CNN and self-attention with BiLSTM or CNN with dense connection. Single DSA outperforms all the baseline models in SST-2 dataset, and achieves comparative results in SST-5, which again verifies the effectiveness of the dynamic weight vector. In contrast to the distinguished results in SNLI dataset (+2.2%), in SST dataset, only marginal differences in the performance between DSA and the previous self-attentive models are found. We conclude that DSA exhibits a more significant improvement for large and complex datasets.

Related Works
Our work differs from early self-attention for sentence embedding in that the dynamic weight vector is not static. Independently, there have recently been an approach to capsule network-based NLP. applied whole capsule network to text classification task. However, we only utilize an algorithm, dynamic routing from capsule network, and modify it into self-attention with the dynamic weight vector, without unnecessary concepts, e.g., capsule.

Conclusion
In this paper, we have proposed Dynamic Self-Attention (DSA), which computes attention weights over words with the dynamic weight vector. With the dynamic weight vector, the self attention mechanism can be furnished with flexibility. Our experiments show that DSA achieves new state-of-the-art results in SNLI dataset, while showing comparative results in SST dataset.