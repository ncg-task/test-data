title
Published as a conference paper at ICLR 2017 DYNAMIC COATTENTION NETWORKS FOR QUESTION ANSWERING
abstract
Several deep learning models have been proposed for question answering .
However , due to their single - pass nature , they have noway to recover from local maxima corresponding to incorrect answers .
To address this problem , we introduce the Dynamic Coattention Network ( DCN ) for question answering .
The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both .
Then a dynamic pointing decoder iterates over potential answer spans .
This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers .
On the Stanford question answering dataset , a single DCN model improves the previous state of the art from 71.0 % F1 to 75. 9 % , while a DCN ensemble obtains 80.4 % F1 .
INTRODUCTION
Question answering ( QA ) is a crucial task in natural language processing that requires both natural language understanding and world knowledge .
Previous QA datasets tend to be high in quality due to human annotation , but small in size .
Hence , they did not allow for training data-intensive , expressive models such as deep neural networks .
To address this problem , researchers have developed large - scale datasets through semi-automated techniques .
Compared to their smaller , hand - annotated counterparts , these QA datasets allow the training of more expressive models .
However , it has been shown that they differ from more natural , human annotated datasets in the types of reasoning required to answer the questions .
Recently , released the Stanford Question Answering dataset ( SQuAD ) , which is orders of magnitude larger than all previous hand - annotated datasets and has a variety of qualities that culminate in a natural QA task .
SQuAD has the desirable quality that answers are spans in a reference document .
This constrains answers to the space of all possible spans .
However , show that the dataset retains a diverse set of answers and requires different forms of logical reasoning , including multi-sentence reasoning .
We introduce the Dynamic Coattention Network ( DCN ) , illustrated in , an end - to - end neural network for question answering .
The model consists of a coattentive encoder that captures the interactions between the question and the document , as well as a dynamic pointing decoder that alternates between estimating the start and end of the answer span .
Our single model obtains an F1 of 75.9 % compared to the best published result of 71.0 % .
In addition , our ensemble model obtains an F1 of 80.4 % compared to the second best result of 78.1 % on the official SQuAD leaderboard .
1 illustrates an overview of the DCN .
We first describe the encoders for the document and the question , followed by the coattention mechanism and the dynamic decoder which produces the answer span .
Document encoder
Question encoder
What plants create most electric power ?
Coattention encoder
The weight of boilers and condensers generally makes the power - to - weight ...
However , most electric power is generated using steam turbine plants , so that indirectly the world 's industry is ...
Dynamic pointer decoder
DOCUMENT AND QUESTION ENCODER
Let ( x Q 1 , x Q 2 , . . . , x Q n ) denote the sequence of word vectors corresponding to words in the question and ( x D 1 , x D 2 , . . . , x D m ) denote the same for words in the document .
Using an LSTM , we encode the document as : d t = LSTM enc d t?1 , x D t .
We define the document encoding matrix as D = [ d 1 . . . d m d ? ]
?
R ( m + 1 ) .
We also add a sentinel vector d ? , which we later show allows the model to not attend to any particular word in the input .
The question embeddings are computed with the same LSTM to share representation power : qt = LSTM enc q t?1 , x Q t .
We define an intermediate question representation Q = [ q 1 . . . q n q ? ] ? R ( n+ 1 ) .
To allow for variation between the question encoding space and the document encoding space , we introduce a non-linear projection layer on top of the question encoding .
The final representation for the question becomes :
COATTENTION ENCODER
We propose a coattention mechanism that attends to the question and document simultaneously , similar to , and finally fuses both attention contexts .
provides an illustration of the coattention encoder .
We first compute the affinity matrix , which contains affinity scores corresponding to all pairs of document words and question words : L = D Q ?
R ( m + 1 ) ( n+ 1 ) .
The affinity matrix is normalized row - wise to produce the attention weights A Q across the document for each word in the question , and column - wise to produce the attention weights AD across the question for each word in the document :
Next , we compute the summaries , or attention contexts , of the document in light of each word of the question .
We similarly compute the summaries QA D of the question in light of each word of the document .
Similar to , we also compute the summaries C QA D of the previous attention contexts in light of each word of the document .
These two operations can be done in parallel , as is shown in Eq.
3 .
One possible interpretation for the operation C QA Dis the mapping of question encoding into space of document encodings .
We define CD , a co-dependent representation of the question and document , as the coattention context .
We use the notation [ a ; b ] for concatenating the vectors a and b horizontally .
The last step is the fusion of temporal information to the coattention context via a bidirectional LSTM :
We define U = [ u 1 , . . . , um ] ?
R 2 m , which provides a foundation for selecting which span maybe the best possible answer , as the coattention encoding .
DYNAMIC POINTING DECODER
Due to the nature of SQuAD , an intuitive method for producing the answer span is by predicting the start and end points of the span .
However , given a question - document pair , there may exist several intuitive answer spans within the document , each corresponding to a local maxima .
We propose an iterative technique to select an answer span by alternating between predicting the start point and predicting the endpoint .
This iterative procedure allows the model to recover from initial local maxima corresponding to incorrect answer spans .
provides an illustration of the Dynamic Decoder , which is similar to a state machine whose state is maintained by an LSTM - based sequential model .
During each iteration , the decoder updates its state taking into account the coattention encoding corresponding to current estimates of the start and end positions , and produces , via a multilayer neural network , new estimates of the start and end positions .
Let hi , s i , and e i denote the hidden state of the LSTM , the estimate of the position , and the estimate of the end position during iteration i .
The LSTM state update is then described by Eq. 5 .
where u si ?1 and u ei?1 are the representations corresponding to the previous estimate of the start and end positions in the coattention encoding U .
Given the current hidden state hi , previous start position u si ?1 , and previous end position u ei ? 1 , we estimate the current start position and end position via Eq. 6 and Eq.
7 .
where ?
t and ?
t represent the start score and end score corresponding to the tth word in the document .
We compute ? t and ?
t with separate neural networks .
These networks have the same architecture but do not share parameters .
Based on the strong empirical performance of Maxout Networks and Highway Networks , especially with regards to deep architectures , we propose a Highway Maxout Network ( HMN ) to compute ?
t as described by Eq .
8 .
The intuition behind using such model is that the QA task consists of multiple question types and document topics .
These variations may require different models to estimate the answer span .
Maxout provides a simple and effective way to pool across multiple model variations .
Here , u t is the coattention encoding corresponding to the tth word in the document .
HMN start is illustrated in .
The end score , ? t , is computed similarly to the start score ? t , but using a separate HMN end .
We now describe the HMN model : where r ?
R is a non-linear projection of the current state with parameters W ( D ) ?
R 5 , m
( 1 ) t is the output of the first maxout layer with parameters W ( 1 ) ?
R p 3 and b ( 1 ) ?
R p , and m
( 2 ) t is the output of the second maxout layer with pa -
( 2 ) tare fed into the final maxout layer , which has parameters W ( 3 ) ? R p12 , and b ( 3 ) ?
R p . p is the pooling size of each maxout layer .
The max operation computes the maximum value over the first dimension of a tensor .
We note that there is highway connection between the output of the first maxout layer and the last maxout layer .
To train the network , we minimize the cumulative softmax cross entropy of the start and end points across all iterations .
The iterative procedure halts when both the estimate of the start position and the estimate of the end position no longer change , or when a maximum number of iterations is reached .
Details can be found in Section 4.1
RELATED WORK
Statistical QA
Traditional approaches to question answering typically involve rule - based algorithms or linear classifiers over hand - engineered feature sets .
proposed two baselines , one that uses simple lexical features such as a sliding window to match bags of words , and another that uses word - distances between words in the question and in the document .
proposed an alternative approach in which one first learns a structured representation of the entities and relations in the document in the form of a knowledge base , then converts the question to a structured query with which to match the content of the knowledge base .
described a statistical model using frame semantic features as well as syntactic features such as part of speech tags and dependency parses .
proposed a competitive statistical baseline using a variety of carefully crafted lexical , syntactic , and word order features .
Neural QA
Neural attention models have been widely applied for machine comprehension or question - answering in NLP .
proposed an AttentiveReader model with the release of the CNN / Daily Mail cloze - style question answering dataset .
released another dataset steming from the children 's book and proposed a window - based memory network .
presented a pointer - style attention mechanism but performs only one attention step .
introduced an iterative neural attention model and applied it to cloze - style machine comprehension tasks .
Recently , released the SQuAD dataset .
Different from cloze - style queries , answers include non-entities and longer phrases , and questions are more realistic .
For SQuAD , proposed an end - to - end neural network model that consists of a Match - LSTM encoder , originally introduced in Wang & Jiang ( 2016 a ) , and a pointer network decoder ; introduced a dynamic chunk reader , a neural reading comprehension model that extracts a set of answer candidates of variable lengths from the document and ranks them to answer the question .
proposed a hierarchical co-attention model for visual question answering , which achieved state of the art result on the COCO - VQA dataset .
In , the co-attention mechanism computes a conditional representation of the image given the question , as well as a conditional representation of the question given the image .
Inspired by the above works , we propose a dynamic coattention model ( DCN ) that consists of a novel coattentive encoder and dynamic decoder .
In our model , instead of estimating the start and end positions of the answer span in a single pass , we iteratively update the start and end positions in a similar fashion to the Iterative Conditional Modes algorithm ) .
EXPERIMENTS
IMPLEMENTATION DETAILS
We train and evaluate our model on the SQuAD dataset .
To preprocess the corpus , we use the tokenizer from Stanford CoreNLP .
We use as Glo Ve word vectors pretrained on the 840B Common Crawl corpus .
We limit the vocabulary to words that are present in the Common Crawl corpus and set embeddings for out - of - vocabulary words to zero .
Empirically , we found that training the embeddings consistently led to overfitting and subpar performance , and hence only report results with fixed word embeddings .
We use a max sequence length of 600 during training and a hidden state size of 200 for all recurrent units , maxout layers , and linear layers .
All LSTMs have randomly initialized parameters and an initial state of zero .
Sentinel vectors are randomly initialized and optimized during training .
For the dynamic decoder , we set the maximum number of iterations to 4 and use a maxout pool size of 16 .
We use dropout to regularize our network during training , and optimize the model using ADAM .
All models are implemented and trained with Chainer .
RESULTS
Evaluation on the SQuAD dataset consists of two metrics .
The exact match score ( EM ) calculates the exact string match between the predicted answer and aground truth answer .
The F1 score calculates the overlap between words in the predicted answer and aground truth answer .
Because a document - question pair may have several ground truth answers , the EM and F1 for a documentquestion pair is taken to be the maximum value across all ground truth answers .
The overall metric is then computed by averaging overall document - question pairs .
The offical SQuAD evaluation is hosted on CodaLab 2 .
The training and development sets are publicly available while the test set is withheld .
The performance of the Dynamic Coattention Network on the SQuAD dataset , compared to other submitted models on the leaderboard 3 , is shown in The DCN has the capability to estimate the start and end points of the answer span multiple times , each time conditioned on its previous estimates .
By doing so , the model is able to explore local maxima corresponding to multiple plausible answers , as is shown in .
Question 3 : What kind of weapons did Tesla 's treatise concern ?
Answer : particle beam weapons Groundtruth : charged particle beam :
Examples of the start and end conditional distributions produced by the dynamic decoder .
Odd ( blue ) rows denote the start distributions and even ( red ) rows denote the end distributions .
i indicates the iteration number of the dynamic decoder .
Higher probability mass is indicated by darker regions .
The offset corresponding to the word with the highest probability mass is shown on the right hand side .
The predicted span is underlined in red , and aground truth answer span is underlined in green .
For example , Question 1 in demonstrates an instance where the model initially guesses an incorrect start point and a correct endpoint .
In subsequent iterations , the model adjusts the start point , ultimately arriving at the correct start point in iteration 3 .
Similarly , the model gradually shifts probability mass for the endpoint to the correct word .
Question 2 shows an example in which both the start and end estimates are initially incorrect .
The model then settles on the correct answer in the next iteration .
Average # Tokens in Answer :
Performance of the DCN for various lengths of documents , questions , and answers .
The blue dot indicates the mean F1 at given length .
The vertical bar represents the standard deviation of F1s at a given length .
While the dynamic nature of the decoder allows the model to escape initial local maxima corresponding to incorrect answers , Question 3 demonstrates a case where the model is unable to decide between multiple local maxima despite several iterations .
Namely , the model alternates between the answers " charged particle beam " and " particle beam weapons " indefinitely .
Empirically , we observe that the model , trained with a maximum iteration of 4 , takes 2.7 iterations to converge to an answer on average .
Model Ablation
The performance of our model and its ablations on the SQuAD development set is shown in Table 2 .
On the decoder side , we experiment with various pool sizes for the HMN maxout layers , using a 2 - layer MLP instead of a HMN , and forcing the HMN decoder to a single iteration .
Empirically , we achieve the best performance on the development set with an iterative HMN with pool size 16 , and find that the model consistently benefits from a deeper , iterative decoder network .
The performance improves as the number of maximum allowed iterations increases , with little improvement after 4 iterations .
On the encoder side , replacing the coattention mechanism with an attention mechanism similar to Wang & Jiang ( 2016 b ) by setting CD to QA D in equation 3 results in a 1.9 point F1 drop .
This suggests that , at an additional cost of a softmax computation and a dot product , the coattention mechanism provides a simple and effective means to better encode the document and question sequences .
Further studies , such as performance without attention and performance on questions requiring different types of reasoning can be found in the appendix .
Performance across length
One point of interest is how the performance of the DCN varies with respect to the length of document .
Intuitively , we expect the model performance to deteriorate with longer examples , as is the case with neural machine translation .
However , as in shown in , there is no notable performance degradation for longer documents and questions contrary to our expectations .
This suggests that the coattentive encoder is largely agnostic to long documents , and is able to focus on small sections of relevant text while ignoring the rest of the ( potentially very long ) document .
We do note a performance degradation with longer answers .
However , this is intuitive given the nature of the evaluation metric .
Namely , it becomes increasingly challenging to compute the correct word span as the number of words increases .
Performance across question type
Another natural way to analyze the performance of the model is to examine its performance across question types .
In , we note that the mean F1 of DCN exceeds those of previous systems across all question types .
The DCN , like other models , is adept at " when " questions and struggles with the more complex " why " questions .
Breakdown of F1 distribution
Finally , we note that the DCN performance is highly bimodal .
On the development set , the model perfectly predicts ( 100 % F1 ) an answer for 62.2 % of examples and predicts a completely wrong answer ( 0 % F1 ) for 16.3 % of examples .
That is , the model picks out partial answers only 21.5 % of the time .
Upon qualitative inspections of the 0 % F1 answers , some of which are shown in Appendix A.4 , we observe that when the model is wrong , it s mistakes tend to have the correct " answer type " ( eg. person for a " who " question , method for a " how " question ) and the answer boundaries encapsulate a well - defined phrase .
CONCLUSION
We proposed the Dynamic Coattention Network , an end - to - end neural network architecture for question answering .
The DCN consists of a coattention encoder which learns co-dependent representations of the question and of the document , and a dynamic decoder which iteratively estimates the answer span .
We showed that the iterative nature of the model allows it to recover from initial local maxima corresponding to incorrect predictions .
On the SQuAD dataset , the DCN achieves the state of the art results at 75.9 % F1 with a single model and 80.4 % F1 with an ensemble .
The DCN significantly outperforms all other models .
A APPENDIX
A.1 PERFORMANCE
In our experiments , we also investigate a model without any attention mechanism .
In this model , the encoder is a simple LSTM network that first ingests the question and then ingests the document .
The hidden states corresponding to words in the document is then passed to the decoder .
This model achieves 33.3 % exact match and 41.9 % F1 , significantly worse than models with attention .
A.2 SAMPLES REQUIRING DIFFERENT TYPES OF REASONING
We generate predictions for examples requiring different types of reasoning , given by .
Because this set of examples is very limited , they do not conclusively demonstrate the effectiveness of the model on different types of reasoning tasks .
Nevertheless , these examples show that the DCN is a promising architecture for challenging question answering tasks including those that involve reasoning over multiple sentences .
WHAT IS THE RANKINE CYCLE SOMETIMES CALLED ?
The Rankine cycle is sometimes referred to as a practical Carnot cycle because , when an efficient turbine is used , the TS diagram begins to resemble the Carnot cycle .
Type of reasoning Lexical variation ( synonymy )
Ground truth practical
Carnot cycle
ID 5730d473b7151e1900c0155 b
Elders are called by God , affirmed by the church , and ordained by a bishop to a ministry of Word , Sacrament , Order and Service within the church .
They maybe appointed to the local church , or to other valid extension ministries of the church .
Elders are given the authority to preach the Word of God , administer the sacraments of the church , to provide care and counseling , and to order the life of the church for ministry and mission .
Elders may also be assigned as District Superintendents , and they are eligible for election to the episcopacy .
Elders serve a term of 23 years as provisional Elders prior to their ordination .
Ground truth bishop , the local church
Prediction a bishop AN ALGORITHM FOR X WHICH REDUCES TO C WOULD ALLOW US TO DO WHAT ?
ID 56e1ce08e3433e14004231a6
This motivates the concept of a problem being hard for a complexity class .
A problem X is hard for a class of problems C if every problem in C can be reduced to X .
Thus no problem in C is harder than X , since an algorithm for X allows us to solve any problem in C .
Of course , the notion of hard problems depends on the type of reduction being used .
For complexity classes larger than P , polynomial - time reductions are commonly used .
In particular , the set of problems that are hard for NP is the set of NP - hard problems .
Ground truth solve any problem in C
ID 56e0d6cf231d4119001ac424
After leaving Edison 's company Tesla partnered with two businessmen in 1886 , Robert Lane and Benjamin Vail , who agreed to finance an electric lighting company in Tesla 's name , Tesla Electric Light & Manufacturing .
The company installed electrical arc light based illumination systems designed by Tesla and also had designs for dynamo electric machine commutators , the first patents issued to Tesla in the US .
Ground truth Tesla
Prediction Robert Lane and Benjamin Vail
Comment
The model produces an incorrect prediction that corresponds to people that funded Tesla , instead of Tesla who actually designed the illumination system .
Empirically , we find that most mistakes made by the model have the correct type ( eg. named entity type ) despite not including types as prior knowledge to the model .
In this case , the incorrect response has the correct type of person .
CYDIPPID ARE TYPICALLY WHAT SHAPE ?
ID 57265746dd62a815002e821 a
Cydippid ctenophores have bodies that are more or less rounded , sometimes nearly spherical and other times more cylindrical or egg - shaped ; the common coastal " sea gooseberry , " Pleurobrachia , sometimes has an egg - shaped body with the mouth at the narrow end , although some individuals are more uniformly round .
From opposite sides of the body extends a pair of long , slender tentacles , each housed in a sheath into which it can be withdrawn .
Some species of cydippids have bodies that are flattened to various extents , so that they are wider in the plane of the tentacles .
Ground truth more or less rounded , egg - shaped
Prediction spherical
Comment
Although the mistake is subtle , the prediction is incorrect .
The statement " are more or less rounded , sometimes nearly spherical " suggests that the entity is more often " rounded " than " spherical " or " cylindrical " or " egg - shaped " ( an answer given by an annotator ) .
This suggests that the model has trouble discerning among multiple intuitive answers due to alack of understanding of the relative severity of " more or less " versus " sometimes " and " other times " .
