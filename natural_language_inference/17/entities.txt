159	0	34	Our simplest baseline architecture
159	38	46	based on
159	47	78	sentence - to - vector encoders
157	48	89	https://github.com/nyu-mll/GLUE-baselines
157	126	172	https://github.com/jsalt18-sentence-repl/jiant
15	46	53	present
15	58	118	General Language Understanding Evaluation ( GLUE ) benchmark
15	123	133	collection
15	134	136	of
15	137	146	NLU tasks
15	147	156	including
15	157	175	question answering
15	178	196	sentiment analysis
15	203	221	textual entailment
16	0	4	GLUE
16	10	19	not place
16	20	35	any constraints
16	36	38	on
16	39	57	model architecture
16	58	64	beyond
16	69	76	ability
16	77	87	to process
16	88	132	single - sentence and sentence - pair inputs
16	137	144	to make
16	145	170	corresponding predictions
20	0	4	Four
20	5	7	of
20	12	20	datasets
20	21	28	feature
20	29	55	privately - held test data
20	77	86	to ensure
20	96	105	benchmark
20	72	76	used
20	114	120	fairly
17	0	3	For
17	4	19	some GLUE tasks
17	22	35	training data
17	39	48	plentiful
17	59	65	others
17	72	79	limited
17	83	97	fails to match
17	102	107	genre
17	108	110	of
17	115	123	test set
2	58	91	NATURAL LANGUAGE UNDERSTAND - ING
4	4	42	natural language understanding ( NLU )
9	97	100	NLU
183	3	12	find that
183	13	32	multi-task training
183	33	39	yields
183	40	61	better overall scores
183	62	66	over
183	67	89	single - task training
183	90	97	amongst
183	98	104	models
183	105	110	using
183	111	128	attention or ELMo
189	47	77	sentence representation models
189	78	104	substantially underperform
189	105	107	on
189	108	112	CoLA
189	113	124	compared to
189	129	135	models
189	145	155	trained on
189	23	27	task
185	3	6	see
185	9	31	consistent improvement
185	32	40	in using
185	41	56	ELMo embeddings
185	57	68	in place of
185	69	93	GloVe or CoVe embeddings
185	96	112	particularly for
185	113	136	single - sentence tasks
187	0	5	Among
187	10	52	pre-trained sentence representation models
187	58	65	observe
187	66	89	fairly consistent gains
187	90	101	moving from
187	102	148	CBoW to Skip - Thought to Infersent and GenSen
188	0	11	Relative to
188	16	22	models
188	23	42	trained directly on
188	47	57	GLUE tasks
188	60	69	InferSent
188	73	84	competitive
188	89	95	GenSen
188	96	107	outperforms
188	108	128	all but the two best
190	20	23	for
190	24	31	STS - B
190	34	40	models
190	41	60	trained directly on
190	65	69	task
190	70	73	lag
190	74	94	significantly behind
190	99	110	performance
190	111	113	of
190	118	152	best sentence representation model
192	0	2	On
192	3	7	WNLI
192	10	18	no model
192	19	26	exceeds
192	27	70	most - frequent - class guessing ( 65.1 % )
193	3	23	RTE and in aggregate
193	31	49	our best baselines
193	56	76	room for improvement
