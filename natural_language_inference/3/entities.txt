163	3	7	test
163	8	18	our models
163	19	21	on
163	22	67	Stanford Question Answering Dataset ( SQuAD )
164	4	17	SQuAD dataset
164	18	29	consists of
164	30	57	more than 100,000 questions
164	58	70	annotated by
164	71	92	crowdsourcing workers
164	93	95	on
164	98	110	selected set
164	111	113	of
164	114	132	Wikipedia articles
169	0	35	Out - of - vocabulary ( OOV ) words
169	40	51	initialized
169	52	60	randomly
169	61	65	with
169	66	82	Gaussian samples
170	0	21	CharCNN filter length
170	22	24	is
170	25	34	1 , 3 , 5
170	37	44	each is
170	45	58	50 dimensions
172	4	20	cluster number K
172	21	23	in
172	24	44	discriminative block
172	45	47	is
172	48	51	100
173	4	15	Adam method
173	19	27	used for
173	28	40	optimization
174	8	22	first momentum
174	26	35	set to be
174	36	39	0.9
174	48	54	second
174	55	60	0.999
175	4	25	initial learning rate
175	29	35	0.0004
175	44	54	batch size
175	58	60	32
178	0	41	All hidden states of GRUs , and TreeLSTMs
178	46	60	500 dimensions
178	69	95	word - level embedding d w
178	99	113	300 dimensions
180	0	39	Explicit question - type dimension d ET
180	43	45	50
168	3	6	use
168	7	45	pre-trained 300 - D Glove 840B vectors
168	46	59	to initialize
168	60	79	our word embeddings
179	3	6	set
179	7	17	max length
179	18	20	of
179	21	29	document
179	30	32	to
179	33	36	500
181	3	8	apply
181	9	16	dropout
181	17	19	to
181	24	59	Encoder layer and aggregation layer
181	60	64	with
181	67	79	dropout rate
181	80	82	of
181	83	86	0.5
13	19	23	take
13	26	37	closer look
13	38	40	at
13	41	59	modeling questions
13	60	62	in
13	71	110	end - to - end neural network framework
14	9	19	introduced
14	20	41	syntactic information
14	42	49	to help
14	50	56	encode
14	57	66	questions
15	8	27	viewed and modelled
15	28	43	different types
15	44	46	of
15	47	56	questions
15	65	76	information
15	77	83	shared
15	84	94	among them
15	95	100	as an
15	101	119	adaptation problem
192	0	9	Our model
192	10	18	achieves
192	21	28	68.73 %
192	29	37	EM score
192	42	49	77.39 %
192	50	58	F1 score
192	70	82	ranked among
192	87	117	state of the art single models
194	0	18	Our baseline model
194	19	24	using
194	25	35	no Q- code
194	36	44	achieved
194	47	66	68.00 % and 77.36 %
194	67	84	EM and F 1 scores
195	8	13	added
195	18	49	explicit question type T - code
195	50	54	into
195	59	73	baseline model
195	80	91	performance
195	96	113	improved slightly
195	114	116	to
195	117	150	68.16 % ( EM ) and 77.58 % ( F1 )
196	8	12	used
196	13	21	TreeLSTM
196	22	31	introduce
196	32	48	syntactic parses
196	49	52	for
196	53	94	question representation and understanding
196	184	189	shows
196	190	209	further improvement
204	39	47	observed
204	50	66	78.38 % F1 score
204	67	69	on
204	74	95	whole development set
2	51	94	Neural - Network - Based Question Answering
4	98	126	machine comprehension ( MC )
4	131	156	question answering ( QA )
