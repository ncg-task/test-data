226	45	56	shared LSTM
226	57	62	plays
226	66	80	important role
226	81	83	in
226	84	101	answer extraction
226	102	107	among
226	108	126	multiple documents
226	168	173	helps
226	177	186	normalize
226	191	216	content probability score
226	217	221	from
226	222	240	multiple documents
226	241	248	so that
226	253	260	answers
226	261	275	extracted from
226	276	295	different documents
226	303	320	directly compared
226	334	338	keep
226	343	356	ranking order
226	357	361	from
226	362	388	document ranking component
228	13	82	preliminary cascade ranking and multi-task answer extraction strategy
228	87	92	vital
228	93	96	for
228	101	118	final performance
228	127	132	serve
228	138	154	good trade - off
228	155	162	between
228	167	187	pure pipeline method
228	192	219	fully joint learning method
230	0	16	Jointly training
230	21	43	three extraction tasks
230	48	55	provide
230	56	70	great benefits
227	3	16	incorporating
227	21	36	manual features
227	43	54	performance
227	62	87	further improved slightly
208	3	9	choose
208	10	25	K = 4 and N = 2
208	26	29	for
208	34	50	good performance
208	51	55	when
208	56	66	evaluating
208	67	69	on
208	74	81	dev set
211	8	43	multi-task deep attention framework
211	49	54	adopt
211	59	73	Adam optimizer
211	74	77	for
211	78	86	training
211	89	93	with
211	96	111	mini-batch size
211	112	114	of
211	115	117	32
211	122	143	initial learning rate
211	144	146	of
211	147	153	0.0005
213	4	19	word embeddings
213	24	29	fixed
213	30	36	during
213	37	45	training
214	4	15	hidden size
214	16	18	of
214	19	23	LSTM
214	27	33	set as
214	34	37	150
214	38	41	for
214	42	50	TriviaQA
214	55	58	128
214	59	62	for
214	63	71	DuReader
215	4	38	task - specific hyper - parameters
215	66	72	set as
215	73	88	? 1 = ? 2 = 0.5
215	91	115	Regularization parameter
216	6	12	set as
216	15	26	small value
216	27	29	of
216	30	34	0.01
217	0	10	All models
217	15	25	trained on
217	26	46	Nvidia Tesla M40 GPU
217	47	51	with
217	52	67	Cudnn LSTM cell
217	68	70	in
217	71	85	Tensorflow 1.3
212	3	6	use
212	11	48	GloVe 300 dimensional word embeddings
212	49	51	in
212	52	60	TriviaQA
212	65	70	train
212	73	98	word2 vec word embeddings
212	99	103	with
212	108	129	whole DuReader corpus
212	130	133	for
212	134	142	DuReader
29	35	42	propose
29	45	63	deep cascade model
29	70	78	combines
29	83	93	advantages
29	94	96	of
29	97	109	both methods
29	110	112	in
29	115	140	coarse - to - fine manner
30	4	22	deep cascade model
30	26	37	designed to
30	47	51	keep
30	56	63	balance
30	64	71	between
30	76	104	effectiveness and efficiency
31	31	68	simple features and ranking functions
31	73	80	used to
31	81	87	select
31	90	103	candidate set
31	16	18	of
31	107	129	most relevant contents
31	132	145	filtering out
31	150	185	irrelevant documents and paragraphs
32	9	28	selected paragraphs
32	33	42	passed to
32	47	79	attention - based deep MRC model
32	80	83	for
32	84	94	extracting
32	99	117	actual answer span
32	118	120	at
32	121	131	word level
34	3	19	jointly optimize
34	20	39	all the three tasks
34	40	42	in
34	45	67	unified deep MRC model
34	76	82	shares
34	83	108	some common bottom layers
35	5	23	cascaded structure
35	24	31	enables
35	36	42	models
35	43	53	to perform
35	56	82	coarse - to - fine pruning
35	83	85	at
35	86	102	different stages
35	105	118	better models
35	126	132	learnt
35	133	160	effectively and efficiently
36	4	21	overall framework
36	25	34	our model
36	62	73	consists of
36	90	108	document retrieval
36	111	130	paragraph retrieval
36	135	152	answer extraction
37	4	16	first module
37	70	72	as
37	73	78	input
37	17	22	takes
37	27	69	question and a collection of raw documents
38	4	10	module
38	11	13	at
38	14	35	each subsequent stage
38	36	44	consumes
38	49	55	output
38	56	60	from
38	65	79	previous stage
38	86	93	further
38	94	100	prunes
38	105	144	documents , paragraphs and answer spans
38	145	150	given
38	155	163	question
40	4	20	ranking function
40	30	37	used as
40	40	58	preliminary filter
40	59	69	to discard
40	70	116	most of the irrelevant documents or paragraphs
41	4	23	extraction function
41	44	53	deal with
41	58	107	auxiliary document and paragraph extraction tasks
41	119	136	jointly optimized
41	137	141	with
41	146	176	final answer extraction module
41	177	180	for
41	181	210	better extraction performance
33	0	17	To better support
33	22	39	answer extraction
33	50	59	introduce
33	64	108	document extraction and paragraph extraction
33	109	111	as
33	112	131	two auxiliary tasks
33	140	148	helps to
33	157	168	narrow down
33	173	192	entire search space
2	25	63	Multi - Document Reading Comprehension
13	0	37	Machine reading comprehension ( MRC )
14	195	198	MRC
221	19	27	adopting
221	32	63	deep cascade learning framework
221	70	84	proposed model
221	85	96	outperforms
221	101	140	previous state - of - the - art methods
221	16	18	by
221	147	161	evident margin
