title
Convolutional Neural Network Architectures for Matching Natural Language Sentences
abstract
Semantic matching is of central importance to many natural language tasks [ 2,28 ] .
A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .
As a step toward this goal , we propose convolutional neural network models for matching two sentences , by adapting the convolutional strategy in vision and speech .
The proposed models not only nicely represent the hierarchical structures of sentences with their layerby - layer composition and pooling , but also capture the rich matching patterns at different levels .
Our models are rather generic , requiring no prior knowledge on language , and can hence be applied to matching tasks of different nature and in different languages .
The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models .
Introduction
Matching two potentially heterogenous language objects is central to many natural language applications .
It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between " linguistic objects " of different nature at different levels of abstractions .
Examples include top -k re-ranking in machine translation ( e.g. , comparing the meanings of a French sentence and an English sentence ) and dialogue ( e.g. , evaluating the appropriateness of a response to a given utterance ) .
Natural language sentences have complicated structures , both sequential and hierarchical , that are essential for understanding them .
A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .
Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .
To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .
Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .
This is part of our continuing effort 1 in understanding natural language objects and the matching between them .
Our main contributions can be summarized as follows .
First , we devise novel deep convolutional network architectures that can naturally combine 1 ) the hierarchical sentence modeling through layer - by - layer composition and pooling , and 2 ) the capturing of the rich matching patterns at different levels of abstraction ; Second , we perform extensive empirical study on tasks with different scales and characteristics , and demonstrate the superior power of the proposed architectures over competitor methods .
Roadmap
We start by introducing a convolution network in Section 2 as the basic architecture for sentence modeling , and how it is related to existing sentence models .
Based on that , in Section 3 , we propose two architectures for sentence matching , with a detailed discussion of their relation .
In Section 4 , we briefly discuss the learning of the proposed architectures .
Then in Section 5 , we report our empirical study , followed by a brief discussion of related work in Section 6 .
Convolutional Sentence Model
We start with proposing anew convolutional architecture for modeling sentences .
As illustrated in , it takes as input the embedding of words ( often trained beforehand with unsupervised methods ) in the sentence aligned sequentially , and summarize the meaning of a sentence through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .
As inmost convolutional models , we use convolution units with a local " receptive field " and shared weights , but we design a large feature map to adequately model the rich structures in the composition of words .
Convolution
As shown in , the convolution in Layer - 1 operates on sliding windows of words ( width k 1 ) , and the convolutions in deeper layers are defined in a similar way .
Generally , with sentence input x , the convolution unit for feature map of type -f ( among F of them ) on Layer - is
and it s matrix form is z
gives the output of feature map of type - f for location i in Layer - ;
w ( , f ) is the parameters for f on Layer - , with matrix form
? ( ) is the activation function ( e.g. , Sigmoid or Relu )
?
( ?1 ) i denotes the segment of Layer - ?
1 for the convolution at location i , whil
concatenates the vectors fork 1 ( width of sliding window ) words from sentence input x .
Max - Pooling
We take a max - pooling in every two - unit window for every f , after each convolution
The effects of pooling are two - fold :
1 ) it shrinks the size of the representation by half , thus quickly absorbs the differences in length for sentence representation , and 2 ) it filters out undesirable composition of words ( see Section 2.1 for some analysis ) .
Length Variability
The variable length of sentences in a fairly broad range can be readily handled with the convolution and pooling strategy .
More specifically , we put all - zero padding vectors after the last word of the sentence until the maximum length .
To eliminate the boundary effect caused by the great variability of sentence lengths , we add to the convolutional unit agate which sets the output vectors to all - zeros if the input is all zeros .
For any given sentence input x , the output of type - f filter for location i in the th layer is given by
( 2 ) where g ( v ) = 0 if all the elements in vector v equals 0 , otherwise g ( v ) = 1 . This gate , working with max - pooling and positive activation function ( e.g. , Sigmoid ) , keeps away the artifacts from padding in all layers .
Actually it creates a natural hierarchy of all - zero padding ( as illustrated in ) , consisting of nodes in the neural net that would not contribute in the forward process ( as in prediction ) and backward propagation ( as in learning ) .
The convolutional unit , when combined with max - pooling , can act as the compositional operator with local selection mechanism as in the recursive autoencoder .
gives an example on what could happen on the first two layers with input sentence " The cat sat on the mat " .
Just for illustration purpose , we present a dramatic choice of parameters ( by turning off some elements in W ) to make the convolution units focus on different segments within a 3 - word window .
For example , some feature maps ( group 2 ) give compositions for " the cat " and " cat sat " , each being a vector .
Different feature maps offer a variety of compositions , with confidence encoded in the values ( color coded in output of convolution layer in ) .
The pooling then chooses , for each composition type , between two adjacent sliding windows , e.g. , between " on the " and " the mat " for feature maps group 2 from the rightmost two sliding windows .
Some Analysis on the Convolutional Architecture
Relation to Recursive Models
Our convolutional model differs from Recurrent Neural Network ( RNN , ) and Recursive Auto - Encoder ( RAE , ) in several important ways .
First , unlike RAE , it does not take a single path of word / phrase composition determined either by a separate gating function , an external parser , or just natural sequential order .
Instead , it takes multiple choices of composition via a large feature map ( encoded in w ( , f ) for different f ) , and leaves the choices to the pooling afterwards to pick the more appropriate segments ( in every adjacent two ) for each composition .
With any window width k ? 3 , the type of composition would be much richer than that of RAE .
Second , our convolutional model can take supervised training and tune the parameters fora specific task , a property vital to our supervised learning - to - match framework .
However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .
For tasks like matching , this limitation can be largely compensated with a network afterwards that can take a " global " synthesis on the learned sentence representation .
Relation to " Shallow " Convolutional Models
The proposed convolutional sentence model takes simple architectures such as ( essentially the same convolutional architecture as SENNA ) , which consists of a convolution layer and a max - pooling over the entire sentence for each feature map .
This type of models , with local convolutions and a global pooling , essentially do a " soft " local template matching and is able to detect local features useful fora certain task .
Since the sentencelevel sequential order is inevitably lost in the global pooling , the model is incapable of modeling more complicated structures .
It is not hard to see that our convolutional model degenerates to the SENNA - type architecture if we limit the number of layers to be two and set the pooling window infinitely large .
Convolutional Matching Models
Based on the discussion in Section 2 , we propose two related convolutional architectures , namely ARC - I and ARC - II ) , for matching two sentences .
Architecture - I ( ARC - I )
Architecture - I ( ARC - I ) , as illustrated in , takes a conventional approach :
It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( MLP ) .
It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .
Although ARC - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .
In other words , in the forward phase ( prediction ) , the representation of each sentence is formed without knowledge of each other .
This can not be adequately circumvented in backward phase ( learning ) , when the convolutional model learns to extract structures informative for matching on a population level .
Architecture - II ( ARC - II )
In view of the drawback of Architecture - I , we propose Architecture - II ( ARC - II ) that is built directly on the interaction space between two sentences .
It has the desirable property of letting two sentences meet before their own high - level representations mature , while still retaining the space for the individual development of abstraction of each sentence .
Basically , in Layer - 1 , we take sliding windows on both sentences , and model all the possible combinations of them through " one-dimensional " ( 1D ) convolutions .
For segment ion S X and segment j on S Y , we have the feature map
where ?
i , j ? R 2 k1 De simply concatenates the vectors for sentence segments for S X and S Y : z
.
Clearly the 1D convolution preserves the location information about both segments .
After that in Layer - 2 , it performs a 2D max - pooling in non-overlapping 2 2 windows ( illustrated in )
( 4 ) In Layer - 3 , we perform a 2D convolution on k 3 k 3 windows of output from Layer - 2 :
( 5 )
This could goon for more layers of 2D convolution and 2D max - pooling , analogous to that of convolutional architecture for image input .
The 2D - Convolution
After the first convolution , we obtain a low level representation of the interaction between the two sentences , and from then we obtain a high level representation z ( ) i , j which encodes the information from both sentences .
The general two - dimensional convolution is formulated as z ( )
where ?
concatenates the corresponding vectors from its 2 D receptive field in Layer -?1 .
This pooling has different mechanism as in the 1D case , for it selects not only among compositions on different segments but also among different local matchings .
This pooling strategy resembles the dynamic pooling in in a similarity learning context , but with two distinctions :
1 ) it happens on a fixed architecture and 2 ) it has much richer structure than just similarity .
contains information about the words in S X before those in z ( ) i + 1 , j , although they maybe generated with slightly different segments in S Y , due to the 2D pooling ( illustrated in .
The orders is however retained in a " conditional " sense .
Our experiments show that when ARC - II is trained on the ( S X , S Y ,S Y ) triples whereS Y randomly shuffles the words in S Y , it consistently gains some ability of finding the correct S Y in the usual contrastive negative sampling setting , which however does not happen with ARC - I.
Model Generality
It is not hard to show that ARC - II actually subsumes ARC - I as a special case .
Indeed , in ARC - II if we choose ( by turning off some parameters in W ( , ) ) to keep the representations of the two sentences separated until the final MLP , ARC - II can actually act fully like ARC - I , as illustrated in .
More specifically , if we let the feature maps in the first convolution layer to be either devoted to S X or devoted to S Y ( instead of taking both as in general case ) , the output of each segment - pair is naturally divided into two corresponding groups .
As a result , the output for each filter f , denoted z
( 1 , f ) 1:n , 1:n ( n is the number of sliding windows ) , will be of rank - one , possessing essentially the same information as the result of the first convolution layer in ARC - I .
Clearly the 2D pooling that follows will reduce to 1 D pooling , with this separateness preserved .
If we further limit the parameters in the second convolution units ( more specifically w ( 2 , f ) ) to those for S X and S Y , we can ensure the individual development of different levels of abstraction on each side , and fully recover the functionality of ARC - I .
As suggested by the order - preserving property and the generality of ARC - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .
As a result , ARC - II can naturally blend two seemingly diverging processes :
1 ) the successive composition within each sentence , and 2 ) the extraction and fusion of matching patterns between them , hence is powerful for matching linguistic objects with rich structures .
This intuition is verified by the superior performance of ARC - II in experiments ( Section 5 ) on different matching tasks .
Training
We employ a discriminative training strategy with a large margin objective .
Suppose that we are given the following triples ( x , y + , y ? )
from the oracle , with x matched with y + better than with y ? .
We have the following ranking - based loss as objective :
where s ( x , y) is predicted matching score for ( x , y ) , and ?
includes the parameters for convolution layers and those for the MLP .
The optimization is relatively straightforward for both architectures with the standard back - propagation .
The gating function ( see Section 2 ) can be easily adopted into the gradient by discounting the contribution from convolution units that have been turned off by the gating function .
In other words , We use stochastic gradient descent for the optimization of models .
All the proposed models perform better with mini-batch ( 100 ? 200 in sizes ) which can be easily parallelized on single machine with multi-cores .
For regularization , we find that for both architectures , early stopping is enough for models with medium size and large training sets ( with over 500K instances ) .
For small datasets ( less than 10 k training instances ) however , we have to combine early stopping and dropout to deal with the serious overfitting problem .
We use 50 - dimensional word embedding trained with the Word2 Vec : the embedding for English words ( Section 5.2 & 5.4 ) is learnt on Wikipedia ( ?1B words ) , while that for Chinese words ( Section 5.3 ) is learnt on Weibo data (? 300 M words ) .
Our other experiments ( results omitted here ) suggest that fine - tuning the word embedding can further improve the performances of all models , at the cost of longer training .
We vary the maximum length of words for different tasks to cope with its longest sentence .
We use 3 - word window throughout all experiments 2 , but test various numbers of feature maps ( typically from 200 to 500 ) , for optimal performance .
ARC - II models for all tasks have eight layers ( three for convolution , three for pooling , and two for MLP ) , while ARC - I performs better with less layers ( two for convolution , two for pooling , and two for MLP ) and more hidden nodes .
We use ReLu as the activation function for all of models ( convolution and MLP ) , which yields comparable or better results to sigmoid - like functions , but converges faster .
Experiments
We report the performance of the proposed models on three matching tasks of different nature , and compare it with that of other competitor models .
Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .
Moreover , the three tasks involve two languages , different types of matching , and distinctive writing styles , proving the broad applicability of the proposed models .
Competitor Methods
WORDEMBED : We first represent each short - text as the sum of the embedding of the words it contains .
The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :
We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :
We use the SENNA - type sentence model for sentence representation ;
SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .
All the competitor models are trained on the same training set as the proposed models , and we report the best test performance over different choices of models ( e.g. , the number and size of hidden layers in MLP ) .
Experiment I : Sentence Completion
This is an artificial task designed to elucidate how different matching models can capture the correspondence between two clauses within a sentence .
Basically , we take a sentence from Reuterswith two " balanced " clauses ( with 8 ? 28 words ) divided by one comma , and use the first clause as S X and the second as S Y .
The task is then to recover the original second clause for any given first clause .
The matching here is considered heterogeneous since the relation between the two is nonsymmetrical on both lexical and semantic levels .
We deliberately make the task harder by using negative second clauses similar to the original ones 4 , both in training and testing .
One representative example is given as follows :
All models are trained on 3 million triples ( from 600K positive pairs ) , and tested on 50K positive pairs , each accompanied by four negatives , with results shown in .
The two proposed models get nearly half of the cases right 5 , with large margin over other sentence models and models without explicit sequence modeling .
ARC - II outperforms ARC - I significantly , showing the power of joint modeling of matching and sentence meaning .
As another convolutional model , SENNA + MLP performs fairly well on this task , although still running behind the proposed convolutional architectures since it is too shallow to adequately model the sentence .
It is a bit surprising that URAE comes last on this task , which might be caused by the facts that 1 ) the representation model ( including word - embedding ) is not trained on Reuters , and 2 ) the split - sentence setting hurts the parsing , which is vital to the quality of learned sentence representation ..
This task is slightly easier than Experiment I , with more training instances and purely random negatives .
It requires less about the grammatical rigor but more on detailed modeling of loose and local matching patterns ( e.g. , work - overtime ? rest ) .
Again ARC - II beats other models with large margins , while two convolutional sentence models ARC - I and SENNA + MLP come next .
Experiment II : Matching A Response to A Tweet
Experiment III : Paraphrase Identification
Paraphrase identification aims to determine whether two sentences have the same meaning , a problem considered a touchstone of natural language understanding .
is included to test our methods on matching homogenous objects .
Here we use the benchmark MSRP dataset , which contains 4,076 instances for training and 1,725 for test .
We use all the training instances and report the test performance from early stopping .
As stated earlier , our model is not specially tailored for modeling synonymy , and generally requires ? 100K instances to work favorably .
Nevertheless , our generic matching models still manage to perform reasonably well , achieving an accuracy and F1 score close to the best performer in 2008 based on hand - crafted features , but still significantly lower than the state - of - the - art ( 76.8%/83.6 % ) , achieved with unfolding - RAE and other features designed for this task .
Discussions
ARC - II outperforms others significantly when the training instances are relatively abundant ( as in Experiment I & II ) .
It s superiority over ARC - I , however , is less salient when the sentences have deep grammatical structures and the matching relies lesson the local matching patterns , as in Experiment - I .
This therefore raises the interesting question about how to balance the representation of matching and the representations of objects , and whether we can guide the learning process through something like curriculum learning .
As another important observation , convolutional models ( ARC - I & II , SENNA + MLP ) perform favorably over bag - of - words models , indicating the importance of utilizing sequential structures in understanding and matching sentences .
Quite interestingly , as shown by our other experiments , ARC - I and ARC - II trained purely with random negatives automatically gain some ability in telling whether the words in a given sentence are in right sequential order ( with around 60 % accuracy for both ) .
It is therefore a bit surprising that an auxiliary task on identifying the correctness of word order in the response does not enhance the ability of the model on the original matching tasks .
We noticed that simple sum of embedding learned via Word2 Vec yields reasonably good results on all three tasks .
We hypothesize that the Word2 Vec embedding is trained in such away that the vector summation can act as a simple composition , and hence retains a fair amount of meaning in the short text segment .
This is in contrast with other bag - of - words models like DEEPMATCH .
Related Work
Matching structured objects rarely goes beyond estimating the similarity of objects in the same domain , with few exceptions like .
When dealing with language objects , most methods still focus on seeking vectorial representations in a common latent space , and calculating the matching score with inner product .
Few work has been done on building a deep architecture on the interaction space for texts - pairs , but it is largely based on a bag - of - words representation of text .
Our models are related to the long thread of work on sentence representation .
Aside from the models with recursive nature ( as discussed in Section 2.1 ) , it is fairly common practice to use the sum of word - embedding to represent a short - text , mostly for classification .
There is very little work on convolutional modeling of language .
In addition to , there is a very recent model on sentence representation with dynamic convolutional neural network .
This work relies heavily on a carefully designed pooling strategy to handle the variable length of sentence with a relatively small feature map , tailored for classification problems with modest sizes .
Conclusion
We propose deep convolutional architectures for matching natural language sentences , which can nicely combine the hierarchical modeling of individual sentences and the patterns of their matching .
Empirical study shows our models can outperform competitors on a variety of matching tasks .
