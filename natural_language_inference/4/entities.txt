164	3	9	assume
164	19	32	concatenation
164	33	35	of
164	40	53	layer outputs
164	54	56	in
164	57	61	DEBS
164	62	67	helps
164	72	89	memory controller
164	90	95	store
164	96	122	contextual representations
166	20	25	using
166	26	30	DEBS
166	15	17	in
166	34	48	all the places
166	49	57	improves
166	62	73	performance
166	74	78	most
166	103	120	memory controller
166	121	125	with
166	126	130	DEBS
166	131	136	gives
166	141	167	largest performance margin
120	28	33	build
120	38	54	model and Sonnet
120	57	69	to implement
120	74	90	memory interface
121	0	4	NLTK
121	8	16	used for
121	17	27	tokenizing
121	28	33	words
123	4	29	hidden vector dimension l
123	33	39	set to
123	40	43	200
125	4	14	batch size
125	18	24	set to
125	25	27	20
125	28	31	for
125	32	40	TriviaQA
125	45	47	30
125	48	51	for
125	52	72	SQuAD and QUASAR - T
127	0	9	Our model
127	15	22	require
127	23	34	more memory
127	35	39	than
127	40	56	existing methods
127	65	75	single GPU
127	113	119	enough
127	123	128	train
127	129	134	model
127	135	141	within
127	144	169	reasonable amount of time
122	0	2	In
122	7	24	memory controller
122	30	33	use
122	34	68	four read heads and one write head
122	79	90	memory size
122	94	100	set to
122	101	107	100 36
122	119	133	initialized as
122	134	135	0
124	3	6	use
124	7	33	AdaDelta ( Zeiler , 2012 )
124	34	36	as
124	40	49	optimizer
124	50	54	with
124	57	70	learning rate
124	71	73	of
124	74	77	0.5
126	10	36	exponential moving average
126	37	39	of
126	40	47	weights
126	48	52	with
126	55	70	decaying factor
126	71	73	of
126	74	79	0.001
23	28	35	propose
23	36	56	two novel strategies
23	62	69	improve
23	74	102	memory - handling capability
23	109	119	mitigating
23	124	146	information distortion
24	3	9	extend
24	14	31	memory controller
24	32	36	with
24	39	58	residual connection
24	59	71	to alleviate
24	76	98	information distortion
25	8	14	expand
25	19	47	gated recurrent unit ( GRU )
25	48	52	with
25	55	71	dense connection
25	77	84	conveys
25	85	102	enriched features
25	103	105	to
25	110	120	next layer
25	121	131	containing
25	136	144	original
25	145	155	as well as
25	160	183	transformed information
2	13	48	Large - Scale Reading Comprehension
4	0	29	Machine reading comprehension
13	0	28	Reading comprehension ( RC )
6	107	109	RC
131	13	37	lengthy - document cases
131	38	45	such as
131	46	70	Trivi aQA and QUASAR - T
131	73	82	our model
131	83	94	outperforms
131	95	120	all the published results
131	164	185	short - document case
131	186	193	such as
131	194	199	SQuAD
131	212	219	achieve
131	224	236	best results
