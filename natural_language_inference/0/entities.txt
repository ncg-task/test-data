20	103	112	introduce
20	115	119	term
20	120	122	in
20	127	143	update equations
20	144	147	for
20	148	177	Gated Recurrent Units ( GRU )
20	15	20	given
20	24	38	input sequence
20	43	63	coreference clusters
20	64	78	extracted from
20	82	97	external system
21	9	22	hidden states
21	27	43	propagated along
21	89	100	in parallel
21	44	62	coreference chains
21	71	88	original sequence
22	3	10	compare
22	11	32	our Coref - GRU layer
22	33	37	with
22	42	59	regular GRU layer
22	63	82	incorporating it in
22	85	97	recent model
22	98	101	for
22	102	123	reading comprehension
2	18	68	Reasoning over Multiple Mentions using Coreference
12	13	42	coreference - based reasoning
94	0	13	BAbi AI tasks
100	16	19	see
100	20	38	clear improvements
100	39	47	of using
100	48	62	C - GRU layers
100	63	67	over
100	68	78	GRU layers
105	0	12	Comparing to
105	17	29	QRN baseline
105	35	45	found that
105	46	53	C - GRU
105	58	77	significantly worse
105	78	80	on
105	81	108	task 15 ( basic deduction )
107	20	27	C - GRU
107	32	52	significantly better
107	53	57	than
107	58	61	QRN
107	62	64	on
107	65	92	task 16 ( basic induction )
112	0	15	Wikihop dataset
120	3	6	see
120	7	25	higher performance
120	26	29	for
120	34	47	C - GRU model
120	48	50	in
120	55	70	low data regime
120	77	98	better generalization
120	99	109	throughout
120	114	128	training curve
120	129	132	for
120	133	151	all three settings
