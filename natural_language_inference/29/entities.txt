19	62	71	introduce
19	74	101	general framework PhaseCond
19	102	105	for
19	110	113	use
19	114	116	of
19	117	142	multiple attention layers
21	124	147	multi-hops architecture
21	151	158	used to
21	173	181	captures
21	182	222	question - aware passage representations
21	227	234	refines
21	239	246	results
21	247	255	by using
21	33	55	self - attention model
24	20	27	domains
24	28	35	such as
24	36	55	machine translation
24	62	89	jointly align and translate
24	90	133	words , question - passage attention models
24	134	137	for
24	138	159	machine comprehension
24	164	182	question answering
24	183	192	calculate
24	197	213	alignment matrix
24	214	230	corresponding to
24	231	266	all question and passage word pairs
2	53	74	MACHINE COMPREHENSION
13	163	181	question answering
126	4	13	EM result
126	14	16	of
126	21	47	baseline Iterative Aligner
126	51	56	lower
126	57	61	than
126	62	66	RNET
127	29	33	RNET
132	14	38	multiple stacking layers
132	43	49	needed
132	50	58	to allow
132	63	71	evidence
132	72	88	fully propagated
132	89	96	through
132	101	108	passage
128	0	5	shows
128	10	21	performance
128	22	26	with
128	27	53	different number of layers
128	54	57	for
128	63	97	question - passage attention phase
128	102	124	self - attention phase
130	0	3	For
130	8	42	question - passage attention phase
130	45	50	using
130	51	63	single layer
130	64	80	does n't degrade
130	85	96	performance
130	111	115	from
130	120	135	default setting
130	136	138	of
130	139	149	two layers
130	152	161	resulting
130	162	164	in
130	167	192	different conclusion from
