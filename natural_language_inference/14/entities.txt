153	0	5	MemNN
153	12	69	strongly supervised AM + NG + NL Memory Networks approach
157	0	10	MemNN- WSH
158	2	37	weakly supervised heuristic version
158	38	40	of
158	41	46	MemNN
161	0	4	LSTM
161	9	28	standard LSTM model
161	31	44	trained using
161	45	73	question / answer pairs only
133	0	3	For
133	4	26	each mini-batch update
133	33	39	2 norm
133	40	42	of
133	47	61	whole gradient
133	62	64	of
133	65	79	all parameters
133	83	91	measured
133	92	93	5
135	3	6	use
135	11	43	learning rate annealing schedule
135	60	62	if
135	67	82	validation cost
135	87	100	not decreased
135	101	106	after
135	107	116	one epoch
135	128	141	learning rate
135	145	156	scaled down
135	157	159	by
135	162	172	factor 1.5
137	0	7	Weights
137	12	29	initialized using
137	30	44	N ( 0 , 0.05 )
137	49	59	batch size
137	63	69	set to
137	70	73	128
138	0	2	On
138	7	24	Penn tree dataset
138	30	36	repeat
138	37	50	each training
138	51	59	10 times
138	60	64	with
138	65	97	different random initializations
138	102	106	pick
138	111	114	one
138	115	119	with
138	120	144	smallest validation cost
14	18	25	present
14	28	79	novel recurrent neural network ( RNN ) architecture
14	90	100	recurrence
14	101	111	reads from
14	114	144	possibly large external memory
14	145	159	multiple times
14	160	177	before outputting
14	180	186	symbol
15	17	27	considered
15	30	45	continuous form
15	46	48	of
15	53	67	Memory Network
17	4	27	continuity of the model
17	65	72	trained
17	73	87	end - to - end
17	88	92	from
17	93	113	input - output pairs
18	22	29	seen as
18	32	52	version of RNNsearch
18	59	63	with
18	64	92	multiple computational steps
18	120	123	per
18	124	137	output symbol
2	15	30	Memory Networks
