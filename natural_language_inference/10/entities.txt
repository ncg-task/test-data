84	0	6	During
84	7	15	training
84	21	25	save
84	30	35	model
84	36	57	every 3000 iterations
26	100	107	achieve
26	108	121	high accuracy
26	122	126	with
26	129	150	fully neural approach
26	151	160	involving
26	168	194	single feedforward network
26	199	236	pre-trained skip - thought embeddings
27	22	33	considering
27	34	56	only the last sentence
27	57	59	of
27	64	71	context
27	72	83	outperforms
27	84	90	models
27	96	104	consider
27	109	121	full context
29	9	21	our approach
29	97	102	using
29	103	128	skip - thought embeddings
29	129	132	for
29	133	142	sentences
29	143	145	in
29	150	155	story
29	156	158	in
29	161	190	feed - forward neural network
29	199	207	training
29	212	217	model
29	218	220	on
29	225	248	provided validation set
29	261	272	considering
29	277	288	two endings
29	289	293	with
29	294	316	only the last sentence
29	317	319	in
29	324	330	prompt
2	39	55	Story Cloze Test
89	4	43	3 - layer feed - forward neural network
89	44	54	trained on
89	59	73	validation set
89	77	84	summing
89	89	114	skip - thought embeddings
89	115	117	of
89	122	142	last sentence ( LS )
89	143	145	of
89	150	177	story prompt and the ending
89	178	183	gives
89	188	212	best accuracy ( 76.5 % )
95	17	22	model
95	23	41	trained using only
95	46	66	last sentence ( LS )
95	67	69	of
95	74	87	story context
95	92	107	higher accuracy
95	108	119	compared to
95	124	129	model
95	130	139	that uses
95	142	145	GRU
95	146	155	to encode
95	160	179	full context ( FC )
95	207	214	encodes
95	219	233	entire context
91	63	68	using
91	69	94	skip - thought embeddings
91	0	9	Comparing
91	12	26	val - LS- skip
91	29	31	to
91	34	51	val - LS - Glo Ve
91	95	98	for
91	99	134	sentences vs. GloVe word embeddings
91	142	149	confirm
91	159	166	success
91	167	169	of
91	170	183	this approach
91	184	191	lies in
91	196	209	sizable boost
91	210	212	to
91	213	221	accuracy
91	222	237	from the use of
91	238	274	pretrained skip - thought embeddings
83	3	6	use
83	7	25	cross-entropy loss
83	30	33	SGD
83	34	38	with
83	39	52	learning rate
83	53	55	of
83	56	60	0.01
