196	3	6	use
196	7	34	pre-trained word embeddings
196	35	39	with
196	40	54	300 dimensions
196	64	68	keep
196	74	79	fixed
196	80	86	during
196	91	107	training process
201	7	21	Adam optimizer
201	22	24	in
201	29	45	training process
201	46	50	with
201	51	56	0.001
201	57	78	initial learning rate
202	39	61	dropout regularization
202	66	69	set
202	70	79	drop rate
202	9	11	to
202	83	86	0.5
197	3	9	employ
197	10	26	256 hidden units
197	27	29	in
197	39	51	gloss module
197	60	74	context module
197	83	88	means
197	89	96	n = 256
198	0	25	Orthogonal initialization
198	29	37	used for
198	38	45	weights
198	46	48	in
198	49	53	LSTM
198	58	87	random uniform initialization
198	88	92	with
198	93	114	range [ - 0.1 , 0.1 ]
203	0	8	Training
203	9	17	runs for
203	24	34	100 epochs
203	35	39	with
203	40	54	early stopping
203	55	57	if
203	62	77	validation loss
203	78	94	does n't improve
203	95	101	within
203	111	120	10 epochs
199	3	9	assign
199	10	33	gloss expansion depth K
199	38	43	value
199	44	46	of
199	47	48	4
200	8	23	experiment with
200	28	52	number of passes | T M |
200	53	57	from
200	58	64	1 to 5
200	65	67	in
200	68	81	our framework
31	19	26	propose
31	29	44	novel model GAS
31	49	85	gloss - augmented WSD neural network
31	92	94	is
31	97	104	variant
31	105	107	of
31	112	126	memory network
32	0	3	GAS
32	4	19	jointly encodes
32	24	43	context and glosses
32	44	46	of
32	51	62	target word
32	67	73	models
32	78	99	semantic relationship
32	100	107	between
32	112	131	context and glosses
32	132	134	in
32	139	152	memory module
33	12	19	measure
33	24	42	inner relationship
33	43	50	between
33	51	70	glosses and context
33	71	86	more accurately
33	92	98	employ
33	99	124	multiple passes operation
33	125	131	within
33	136	142	memory
33	143	145	as
33	150	168	re-reading process
33	173	178	adopt
33	179	209	two memory updating mechanisms
228	0	27	English all - words results
233	0	15	GAS and GAS ext
233	16	24	achieves
233	29	60	state - of - theart performance
233	61	63	on
233	68	81	concatenation
233	82	84	of
233	85	102	all test datasets
240	10	24	our best model
240	34	38	beat
240	39	48	IMS + emb
240	49	51	on
240	56	85	SE3 , SE13 and SE15 test sets
237	7	15	performs
237	16	20	best
237	21	23	on
237	24	41	all the test sets
237	54	58	find
237	64	71	GAS ext
237	72	76	with
237	77	115	concatenation memory updating strategy
237	116	124	achieves
237	129	141	best results
237	142	146	70.6
237	147	149	on
237	154	167	concatenation
237	168	170	of
237	175	193	four test datasets
238	78	87	find that
238	88	102	our best model
238	103	114	outperforms
238	119	154	previous best neural network models
238	155	157	on
238	158	183	every individual test set
241	0	13	Incorporating
241	14	21	glosses
241	22	26	into
241	27	37	neural WSD
241	42	57	greatly improve
241	62	73	performance
241	78	87	extending
241	92	106	original gloss
241	111	124	further boost
241	129	136	results
242	0	13	Compared with
242	18	36	Bi - LSTM baseline
242	68	86	our proposed model
242	87	103	greatly improves
242	108	116	WSD task
242	117	119	by
242	120	136	2.2 % F1 - score
242	137	141	with
242	154	169	gloss knowledge
243	14	27	compared with
243	32	35	GAS
243	97	104	GAS ext
243	109	124	further improve
243	129	140	performance
243	141	157	with the help of
243	162	178	extended glosses
243	179	186	through
243	191	209	semantic relations
4	0	33	Word Sense Disambiguation ( WSD )
5	72	75	WSD
66	27	59	Neural Word Sense Disambiguation
