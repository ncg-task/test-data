title
Knowledge-based Word Sense Disambiguation using Topic Models
abstract
Disambiguation is an open problem in Natural Language Processing which is particularly challenging and useful in the unsupervised setting where all the words in any given text need to be disambiguated without using any labeled data. Typically WSD systems use the sentence or a small window of words around the target word as the context for disambiguation because their computational complexity scales exponentially with the size of the context. In this paper, we leverage the formalism of topic model to design a WSD system that scales linearly with the number of words in the context. As a result, our system is able to utilize the whole document as the context fora word to be disambiguated. The proposed method is a variant of Latent Dirichlet Allocation in which the topic proportions fora document are replaced by synset proportions. We further utilize the information in the WordNet by assigning a non-uniform prior to synset distribution over words and a logistic-normal prior for document distribution over synsets. We evaluate the proposed method on Senseval-2, Senseval-3, SemEval-2007, SemEval-2013 and SemEval-2015 English All-Word WSD datasets and show that it outperforms the state-of-the-art unsupervised knowledge-based WSD system by a significant margin.
Introduction
Word Sense Disambiguation (WSD) is the task of mapping an ambiguous word in a given context to its correct meaning. WSD is an important problem in natural language processing (NLP), both in its own right and as a steppingstone to more advanced tasks such as machine translation, information extraction and retrieval, and question answering. WSD, being AI-complete (Navigli 2009), is still an open problem after over two decades of research. Following Navigli (2009), we can roughly distinguish between supervised and knowledge-based (unsupervised) approaches. Supervised methods require senseannotated training data and are suitable for lexical sample WSD tasks where systems are required to disambiguate a restricted set of target words. However, the performance of supervised systems is limited in the all-word WSD tasks as labeled data for the full lexicon is sparse and difficult to obtain. As the all-word WSD task is more challenging and has, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. more practical applications, there has been significant interest in developing unsupervised knowledge-based systems. These systems only require an external knowledge source (such as WordNet) but no labeled training data.
In this paper, we propose a novel knowledge-based WSD algorithm for the all-word WSD task, which utilizes the whole document as the context fora word, rather than just the current sentence used by most WSD systems. In order to model the whole document for WSD, we leverage the formalism of topic models, especially Latent Dirichlet Allocation (LDA). Our method is a variant of LDA in which the topic proportions fora document are replaced by synset proportions fora document. We use a non-uniform prior for the synset distribution over words to model the frequency of words within a synset. Furthermore, we also model the relationships between synsets by using a logisticnormal prior for drawing the synset proportions of the document. This makes our model similar to the correlated topic model , with the difference that our priors are not learned but fixed. In particular, the values of these priors are determined using the knowledge from WordNet. We evaluate our system on a set of five benchmark datasets, show that the proposed model outperforms stateof-the-art knowledge-based WSD system.

Related Work
Lesk) is a classical knowledge-based WSD algorithm which disambiguates a word by selecting a sense whose definition overlaps the most with the words in its context. Many subsequent knowledge-based systems are based on the Lesk algorithm. extended Lesk by utilizing the definitions of words in the context and weighing the words by term frequency-inverse document frequency (tf-idf). further enhanced Lesk by using word embeddings to calculate the similarity between sense definitions and words in the context.
The above methods only use the words in the context for disambiguating the target word. However, Chaplot, show that sense of a word depends on not just the words in the context but also on their senses. Since the senses of the words in the context are also unknown, they need to be optimized jointly. In the past decade, many graph-based unsupervised WSD methods have been developed which typically leverage the underlying structure of Lexical Knowledge Base such as Word-Net and apply well-known graph-based techniques to efficiently select the best possible combination of senses in the context. and build a subgraph of the entire lexicon containing vertices useful for disambiguation and then use graph connectivity measures to determine the most appropriate senses. and construct a sentence-wise graph, where, for each word every possible sense forms a vertex. Then graph-based iterative ranking and centrality algorithms are applied to find most probable sense. More recently, presented an unsupervised WSD approach based on personalized page rank over the graphs generated using WordNet. The graph is created by adding content words to the Word-Net graph and connecting them to the synsets in which they appear in as strings. Then, the Personalized PageRank (PPR) algorithm is used to compute relative weights of the synsets according to their relative structural importance and consequently, for each content word, the synset with the highest PPR weight is chosen as the correct sense. Chaplot, Bhattacharyya, and Paranjape (2015) present a graph-based unsupervised WSD system which maximizes the total joint probability of all the senses in the context by modeling the WSD problem as a Markov Random Field constructed using the WordNet and a dependency parser and using a Maximum A Posteriori (MAP) Query for inference. Babelfy is another graph-based approach which unifies WSD and Entity Linking. It performs WSD by performing random walks with restart over BabelNet, which is a semantic network integrating WordNet with various knowledge resources.
The WSD systems which try to jointly optimize the sense of all words in the context have a common limitation that their computational complexity scales exponentially with the number of content words in the context due to pairwise comparisons between the content words. Consequently, practical implementations of these methods either use approximate or sub-optimal algorithms or reduce the size of context. Chaplot, Bhattacharyya, and Paranjape (2015) limit the context to a sentence and reduce the number of pairwise comparisons by using a dependency parser to extract important relations. Agirre, López de Lacalle, and Soroa (2014) limit the size of context to a window of 20 words around the target word. Moro, Raganato, and Navigli (2014) employ a densest subgraph heuristic for selecting high-coherence semantic interpretations of the input text. In contrast, the proposed approach scales linearly with the number of words in the context while optimizing sense of all the words in the context jointly. As a result, the whole document is utilized as the context for disambiguation.
Our work is also related to who were the first to apply LDA techniques to WSD. In their approach, words senses that share similar paths in the WordNet hierarchy are typically grouped in the same topic. However, they observe that WordNet is perhaps not the most optimal structure for WSD. Highly common, polysemous words such as man and time could potentially be associated with many different topics making decipherment of sense difficult. Even rare words that differ only subtly in their sense (e.g., quarterback -the position and quarterback -the player himself) could potentially only share the root node in WordNet and hence never have a chance of being on the same topic. also employ the idea of global context for the task of WSD using topic models, but rather than using topic models in an unsupervised fashion, they embed topic features in a supervised WSD model.

WordNet
Most WSD systems use a sense repository to obtain a set of possible senses for each word. WordNet is a comprehensive lexical database for the English language (Miller 1995), and is commonly used as the sense repository in WSD systems.
It provides a set of possible senses for each content word (nouns, verbs, adjectives and adverbs) in the language and classifies this set of senses by the POS tags. For example, the word "cricket" can have 2 possible noun senses: 'cricket#n#1: leaping insect' and 'cricket#n#2: a game played with a ball and bat', and a single possible verb sense, 'cricket#v#1: (play cricket)'. Furthermore, WordNet also groups words which share the same sense into an entity called synset (set of synonyms). Each synset also contains a gloss and example usage of the words present in it. For example, 'aim#n#2', 'object#n#2, 'objective#n#1', 'tar-get#n#5' share a common synset having gloss "the goal intended to be attained".
WordNet also contains information about different types of semantic relationships between synsets. These relations include hypernymy, meronymy, hyponymy, holonymy, etc. shows a graph of a subset of the WordNet where nodes denote the synset and edges denote different semantic relationship between synsets. For instance, 'plan of action#n#1' is a meronym of 'goal#n#1', and 'pur-pose#n#1', 'aim#n#1' and 'destination#n#1' are hyponyms of 'goal#n#1', as shown in the figure. These semantic relationships in the WordNet can be used to compute the similarity between different synsets using various standard relatedness measures (Pedersen, Patwardhan, and Michelizzi 2004). Note that although WordNet is the most widely used sense repository, the sense distinctions can be too fine-grained in many scenarios. This makes it difficult for expert annotators to agree on a correct sense, resulting in a very low interannotator agreement ( 72%) in standard WSD datasets. Nevertheless, we will use WordNet for our experiments fora fair comparison with previous work.

Methods

Problem Definition
First, we formally define the task of all-word Word Sense Disambiguation by illustrating an example. Consider a sentence, where we want to disambiguate all the content words (nouns, verbs, adjectives and adverbs):
They were troubled by insects while playing cricket. The sense xi of each content word (given its part-of-speech tag) w i can take k i possible values from the set y i = {y 1 i , y 2 i , . . . , y ki i } (see). In particular, the word w 5 = "cricket" can either meany 1 5 = "a leaping insect" or y 2 5 = "a game played with a ball and bat played by two teams of 11 players." In this example, the second sense is more appropriate. The problem of mapping each content word in any given text to its correct sense is called the all-word WSD task. The set of possible senses for each word is given by a sense repository like WordNet.

Semantics
In this subsection, we describe the semantic ideas underlying the proposed method and how they are incorporated in the proposed model:
 using the whole document as the context for WSD: modeled using Latent Dirichlet Allocation.
 some words in each synset are more frequent than others: modeled using non-uniform priors for the synset distribution over words.
 some synsets tend to co-occur more than others: modeled using logistic normal distribution for synset proportions in a document.
Wherever possible, we give examples to motivate the semantic ideas and illustrate their importance.

Document context
The sense of a word depends on other words in its context. In the WordNet, the context of a word is defined to be the discourse that surrounds a language unit and helps to determine its interpretation. It is very difficult to determine the context of any given word. Most WSD systems use the sentence in which the word occurs as its context and each sentence is considered independent of others. However, we know that a document or an article is about Here, the word 'chips' could refer to potato chips, micro chips or poker chips. It is not possible to disambiguate this word without looking at other words in the document. The presence of other words like 'casino', 'gambler', etc. in the document would indicate the sense of poker chips, while words like 'electronic' and 'silicon' indicate the sense of microchip. Gale, Church, and Yarowsky (1992) also observed that words strongly tend to exhibit only one sense in a given discourse or document. Thus, we hypothesize that the meaning of the word depends on words outside the sentence in which it occurs -as a result, we use the whole document containing the word as its context.
Topic Models In order to use the whole document as the context fora word, we would like to model the concepts involved in the document. Topic models are suitable for this purpose, which aim to uncover the latent thematic structure in collections of documents. The most basic example of a topic model is Latent Dirichlet Allocation (LDA). It is based on the key assumption that documents exhibit multiple topics (which are nothing but distributions over some fixed vocabulary).
LDA has an implicit notion of word senses as words with several distinct meanings can appear in distinct topics (e.g., cricket the game in a "sports" topic and cricket the insect in a "zoology" topic). However, since the sense notion is only implicit (rather than a set of explicit senses for each word in WSD), it is not possible to directly apply LDA to the WSD task. Therefore, we modify the basic LDA by representing documents by synset probabilities rather than topic probabilities and consequently, the words are generated by synsets rather than topics. We further modify this graphical model to incorporate the information in the WordNet as described in the following subsections.
Synset distribution over words Due to sparsity problems in large vocabulary size, the LDA model was extended to a "smoothed" LDA model by placing an exchangeable Dirichlet prior on topic distribution over words. In an exchangeable Dirichlet distribution, each component of the parameter vector is equal to the same scalar. However, such a uniform prior is not ideal for synset distribution over words since each synset contains only a fixed set of words. For example, the synset defined as "the place designated as the end (as of a race or journey)" contains only 'goal','destination' and 'finish'. Furthermore, some words in each synset are more frequent than others. For example, in the synset defined as "a person who participates in or is skilled at some game", the word 'player' is more frequent than word 'participant', while in the synset defined as "a theatrical performer", word 'actor' is more frequent than word 'player'. Thus, we decide to have non-uniform priors for synset distribution over words. Document distribution over synsets The LDA model uses a Dirichlet distribution for the topic proportions of a document. Under a Dirichlet, the components of the topic proportions vector are approximately independent; this leads to the strong and unrealistic modeling assumption that the presence of one topic is not correlated with the presence of other topics. Similarly, in our case, the presence of one synset is correlated with the presence of others. For example, the synset representing the 'sloping land' sense of the word 'bank' is more likely to cooccur with the synset of 'river' (a large natural stream of water) than the synset representing 'financial institution' sense of the word 'bank'. Hence, we model the correlations between synsets using a logistic normal distribution for synset proportions in a document.

Proposed Model
Following the ideas described in the previous subsection, we propose a probabilistic graphical model, which assumes that a corpus is generated according to the following process: where f (?) = exp(?) i exp(?i) is the softmax function. Note that the prior for drawing word proportions for each sense is not symmetric: ? sis a vector of length equal to word vocabulary size, having non-zero equal entries only for the words contained in synset sin WordNet. The graphical model corresponding to the generative process is shown in. illustrates a toy example of a possible word distribution in synsets and synset proportions in a document learned using the proposed model. Colors highlighting some of the words in the document denote the corresponding synsets they were sampled from.

Priors
We utilize the information in the WordNet for deciding the priors for drawing the word proportions for each synset and the synset proportions for each document. The prior for distribution of synset s over words is chosen as the frequency of the words in the synset s, i.e., ? sv = Frequency of word v in synset s. The logistic normal distribution for drawing synset proportions has two priors, µ and ?. The parameter µ s gives the probability of choosing a synset s. The frequency of the synset s would be the natural choice for µ s but since our method is unsupervised, we use a uniform µ for all synsets instead. The ? parameter is used to model the relationship between synsets. Since, the inverse of covariance matrix will be used in inference, we directly choose (i, j)th element of inverse of covariance matrix as follows: ? ?1 ij = Negative of similarity between synset i and synset j The similarity between any two synsets in the WordNet can be calculated using a variety of relatedness measures given in WordNet::Similarity library (Pedersen,. In this paper, we use the Lesk similarity measure as it is used in prior WSD systems. Lesk algorithm calculates the similarity between two synsets using the overlap between their definitions.

Inference
We use a Gibbs Sampler for sampling latent synsets z mn given the values of rest of the variables. Given a corpus of M documents, the posterior over latent variables, i.e. the synset assignments z, logistic normal parameter ?, is as follows:
The word distribution p(w mn |z mn , ?) is multinomial in ? zmn and the conjugate distribution p(? s |? s ) is Dirichlet in ? s . Thus, p(w|z, ?) p(?|?) can be collapsed to p(w|z, ?) by integrating out ? s for all senses s to obtain:  ? m follows a normal distribution which is not conjugate of the multinomial distribution:
Thus, p(z|?)p(?|µ, ?) can't be collapsed. In typical logistic-normal topic models, a block-wise Gibbs sampling algorithm is used for alternatively sampling topic assignments and logistic-normal parameters. However, since in our case the logistic-normal priors are fixed, we can sample synset assignments directly using the following equation:
Here, n SV sv , n SM sm and n S s correspond to standard counts:

Experiments & Results
For evaluating our system, we use the English all-word WSD task benchmarks of the SensEval-2,), SemEval-2013 and.) standardized all the above datasets into a unified format with gold standard keys in WordNet 3.0. We use the standardized version of all the datasets and use the same experimental setting as (Raganato, Camacho-Collados, and Navigli 2017) for fair comparison with prior methods.
In we compare our overall F1 scores with different unsupervised systems described in Section 2 which include Banerjee03, Basile14, Agirre14 and Moro14. In addition to knowledge-based systems which do not require any labeled training corpora, we also report F1 scores of the state-of-the-art supervised systems trained on SemCor and OMSTI (Taghipour and Ng 2015) for comparison. Zhong10 use a Support Vector Machine over a set of features which include surrounding words in the context, their PoS tags, and local collocations. Mel-maud16 learn context embeddings of a word and classify a test word instance with the sense of the training set word whose context embedding is the most similar to the context embedding of the test instance. We also provide the F1 scores of MFS baseline, i.e. labeling each word with its most frequent sense (MFS) in labeled datasets,) and OM-STI (Taghipour and Ng 2015).
The proposed method, denoted by WSD-TM in the tables referring to WSD using topic models, outperforms the state-of-the-art WSD system by a significant margin (pvalue <0.01) by achieving an overall F1-score of 66.9 as compared to Moro14's score of 65.5. We also observe that the performance of the proposed model is not much worse than the best supervised system, Melamud16 (69.4). In we report the F1 scores on different parts of speech.
The proposed system outperforms all previous knowledgebased systems overall parts of speech. This indicates that using document context helps in disambiguating words of all PoS tags.

Discussions
In this section, we illustrate the benefit of using the whole document as the context for disambiguation by illustrat-   ing an example. Consider an excerpt from the SensEval 2 dataset shown in. Highlighted words clearly indicate that the domain of the document is Biology. While most of these words are monosemous, let's consider disambiguating the word 'cell', which is highly polysemous, having 7 possible senses as shown in. As shown in, the correct sense of cell ('cell#2') has the highest similarity with senses of three monosemous words 'scientist', 'researcher' and 'protein'. The word 'cell' occurs 21 times in the document, and several times, the other words in the sentence are not adequate to disambiguate it. Since our method uses the whole document as the context, words such as 'scientists', 'researchers' and 'protein' help in disambiguating 'cell', which is not possible otherwise. The proposed model also overcomes several limitations of topic models based on Latent Dirichlet Allocation and its variants. Firstly, LDA requires the specification of the number of topics as a hyper-parameter which is difficult to tune. The proposed model doesn't require the total number of synsets to be specified as the total number of synsets are equal to the number of synsets in the sense repository which is fixed. Secondly, topics learned using LDA are often not meaningful as the words inside some topics are unrelated. However, synsets are always meaningful as they contain only synonymous words. This is ensured in the proposed by using a non-uniform prior for word distribution in synsets.

Conclusion
In this paper, we propose a novel knowledge-based WSD system based on a logistic normal topic model which incorporates semantic information about synsets as its priors. The proposed model scales linearly with the number of words in the context, which allows our system to use the whole document as the context for disambiguation and outperform state-of-the-art knowledge-based WSD system on a set of benchmark datasets.
One possible avenue for future research is to use this model for supervised WSD. This could be done by using sense tags from the SemCor corpus as training data in a supervised topic model similar to the one presented by. Another possibility would be to add another level to the hierarchy of the document generating process. This would allow us to bring back the notion of topics and then to define topic-specific sense distributions. The same model can also be extended to other problems such named-entity disambiguation.