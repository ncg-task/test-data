(Contribution||has||Experimental setup)
(Experimental setup||For||Transformer encoder layers)
(Transformer encoder layers||has||dropout)
(dropout||of||0.1)
(Transformer encoder layers||has||hidden size)
(hidden size||of||2048)
(Transformer encoder layers||has||6 layers)
(6 layers||with||8 attention heads)
(Experimental setup||For||BERT)
(BERT||used||model)
(model||named||bert - largecased)
(bert - largecased||consists of||vectors)
(vectors||of||dimension 1024)
(vectors||trained on||Book s Corpus and English Wikipedia)
(model||of||PyTorch implementation)
