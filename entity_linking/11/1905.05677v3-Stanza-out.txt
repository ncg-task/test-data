title
Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation
abstract
In this article , we tackle the issue of the limited quantity of manually sense annotated corpora for the task of word sense disambiguation , by exploiting the semantic relationships between senses such as synonymy , hypernymy and hyponymy , in order to compress the sense vocabulary of Princeton WordNet , and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database .
We propose two different methods that greatly reduce the size of neural WSD models , with the benefit of improving their coverage without additional training data , and without impacting their precision .
In addition to our methods , we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks .
Introduction
Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory .
Various approaches have been proposed to achieve WSD : Knowledge - based methods rely on dictionaries , lexical databases , thesauri or knowledge graphs as primary resources , and use algorithms such as lexical similarity measures or graph - based measures .
Supervised methods , on the other hand , exploit sense annotated corpora as training instances fora classifier such as SVM , or more recently by a neural network .
Finally , unsupervised methods automatically iden - tify the different senses of words from unannotated or parallel corpora ( e.g. ) .
Supervised methods are by far the most predominant as they generally offer the best results in evaluation campaigns ( for instance ) .
State of the art classifiers used to combine specific features such as the parts of speech and the lemmas of surrounding words , but they are now replaced by neural networks which learn their own representation of words .
One major bottleneck of supervised systems is the restricted quantity of manually sense annotated corpora :
In the annotated corpus SemCor , the largest manually sense annotated corpus available , words are annotated with 33 760 different sense keys , which corresponds to only approximately 16 % of the sense inventory of WordNet , the lexical database of reference widely used in WSD .
Many works try to leverage this problem by creating new sense annotated corpora , either automatically , semi-automatically , or through crowdsourcing .
In this work , the idea is to solve this issue by taking advantage of the semantic relationships between senses included in WordNet , such as the hypernymy , the hyponymy , the meronymy , the antonymy , etc .
Our method is based on the observation that a sense and its closest related senses ( it s hypernym or it s hyponyms for instance ) all share a common idea or concept , and so a word can sometimes be disambiguated using only related concepts .
Consequently , we do not need to know every sense of WordNet to disambiguate all words of WordNet .
For instance , let us consider the word " mouse " and two of its senses which are the computer mouse and the animal mouse .
We only need to know the notions of " animal " and " electronic de-vice " to distinguish them , and all notions that are more specialized such as " rodent " or " mammal " are therefore superfluous .
By grouping them , we can benefit from all other instances of electronic devices or animals in a training corpus , even if they do not mention the word " mouse " .
Contributions : In this paper , we hypothesize that only a subset of WordNet senses could be considered to disambiguate all words of the lexical database .
Therefore , we propose two different methods for building this subset and we call them sense vocabulary compression methods .
By using these techniques , we are able to greatly improve the coverage of supervised WSD systems , nearly eliminating the need fora backoff strategy that is currently used inmost systems when dealing with a word which has never been observed in the training data .
We evaluate our method on a state of the art WSD neural network , based on pretrained contextualized word vector representations , and we present results that significantly outperform the state of the art on every standard WSD evaluation task .
Finally , we provide a documented tool for training and evaluating neural WSD models , as well as our best pretrained model in a dedicated GitHub repository 1 .
Related Work
In WSD , several recent advances have been made in the creation of new neural architectures for supervised models and the integration of knowledge into these systems .
Multiple works also exploit the idea of grouping together related senses .
In this section , we give an overview of these works .
WSD Based on a Language Model
In this type of approach , that has been initiated by and reimplemented by , the central component is a neural language model able to predict a word with consideration for the words surrounding it , thanks to a recurrent neural network trained on a massive quantity of unannotated data .
Once the language model is trained , it is used to produce sense vectors that result from averaging the word vectors predicted by the language model at all positions of words annotated with the given sense .
At test time , the language model is used to predict a vector according to the surrounding context , 1 https://github.com/getalp/disambiguate and the sense closest to the predicted vector is assigned to each word .
These systems have the advantage of bypassing the problem of the lack of sense annotated data by concentrating the power of abstraction offered by recurrent neural networks on a good quality language model trained in an unsupervised manner .
However , sense annotated corpora are still indispensable to contruct the sense vectors .
WSD Based on a Softmax Classifier
In these systems , the main neural network directly classifies and attributes a sense to each input word through a probability distribution computed by a softmax function .
Sense annotations are simply seen as tags put on every word , like a POS - tagging task for instance .
We can distinguish two separate branches of these types of neural networks :
1 . Those in which we have several distinct and token - specific neural networks ( or classifiers ) for every different word in the dictionary ) , each of them being able to manage a particular word and its particular senses .
For instance , one of the classifiers is specialized in choosing between the four possible senses of the noun " mouse " .
This type of approach is particularly fitted for the lexical sample tasks , where a small and finite set of very ambiguous words have to be sense annotated in several contexts , but it can also be used in all - words word sense disambiguation tasks .
2 .
Those in which we have a larger and general neural network that is able to manage all different words and assign a sense in the set of all existing sense in the dictionary used .
The advantage of the first branch of approaches is that in order to disambiguate a word , limiting our choice to one of its possible senses is computationally much easier than searching through all the senses of all words .
To put things in perspective , the average number of senses of polysemous words in WordNet is approximately 3 , whereas the total number of senses considering all words is 206 941 .
The second approach , however , has an interesting property : all senses reside in the same vector space and hence share features in the hidden layers of the network .
This allows the model to predict an identical sense for two different words ( i.e. synonyms ) , but it also offers the possibility to predict a sense fora word not present in the dictionary ( e.g. neologism , spelling mistake ... ) .
Finally , in two recent articles , and have proposed an improvement of these type of architectures , by computing an attention between the context of a target word and the gloss of its different senses .
Thus , their work is one of the first to incorporate knowledge from WordNet into a WSD neural network .
Sense Clustering
Methods
Several works exploit the idea of grouping together mutiple Word Net sense tags in order to create a coarser sense inventory which can potentially be more useful in some NLP tasks .
In the works of , the authors propose a supervised system that learns and predicts " Supersense " tags , which belong to the set of the broad semantic categories of senses , organizing the sense inventory of WordNet .
This tagset consists , in their work , of 26 categories for nouns ( such as " food " , " person " or " object " ) , and 15 categories for verbs ( such as " emotion " or " weather " ) .
By predicting supersense tags instead of the usual fine - grained sense tags of WordNet , the output vocabulary of their system is shrinked to only 41 different classes , and this leads to a small and easy - to - train model able to perform partial WSD , which could be useful and sufficient for other NLP tasks where the fine - grained distinction is not necessary .
In , the authors propose several methods for creating " Basic Level Concepts " ( BLC ) , groups of related senses with a generally smaller size than supersenses , and which can be controlled by a threshold variable .
Their methods rely on the semantic relationships between senses of WordNet , and , in the same way as , they evaluated their clusters on a modified WSD task , where supersenses or BLC have to be predicted instead of the original sense tags from WordNet .
The main difference between our work and these works is that our end goal is to improve finegrained WSD systems .
Even though our methods generate clusters of related senses , we guarantee that two different senses of a lemma reside in two different clusters , so at the end , even if our supervised system produces a cluster tag fora target word , we are still able to find back the true sense tag , by simply keeping track of which sense key of its lemma belongs to the predicted group .
Sense Vocabulary
Compression
Current state of the art supervised WSD systems such as , , and are all confronted to the following issues :
1 . Due to the small number of manually sense annotated corpora available , a target word may never be observed during the training , and therefore the system is notable to annotate it .
2 . For the same reason , a word may have been observed , but not all of its senses .
In this case the system is able to annotate the word , but if the expected sense has never been observed , the output will be wrong , regardless of the architecture of the supervised system .
3 . Training a neural network to predict a tag which belongs to the set of all WordNet senses can become extremely slow and requires a lot of parameters with a large output vocabulary .
And this vocabulary goes up to 206 941 if we consider all word - senses of WordNet .
In order to overcome all these issues , we propose a method for grouping together multiple sense tags that refer in fact to the same concept .
In consequence , the output vocabulary decreases , the ability of the trained system to generalize improves , as well as its coverage .
From Senses to Synsets : A Vocabulary Compression Based on Synonymy
In the lexical database WordNet , senses are organized in sets of synonyms called synsets .
A synset is technically a group of one or more word - senses that have the same definition and consequently the same meaning .
For instance , the first senses of " eye " , " optic " and " oculus " all refer to a common synset which definition is " the organ of sight " .
Illustrated in , the word - sense to synset mapping is hence away of compressing the output vocabulary , and it is already applied in many works , while not being always explicitly stated .
This method clearly helps to improve the coverage of supervised systems however .
Indeed , if the verb " help " is observed in the annotated data in its first sense , the context surrounding the target word can be used to later annotate the verb " assist " or " aid " with the same valid synset tag . :
Word - sense to synset mapping ( compression through synonymy ) applied on the first two senses of the words " help " , " aid " and " assist " .
Going further , other information from WordNet can help the system to generalize .
Our first new method takes advantage of the hypernymy and hyponymy relationships to achieve the same idea .
Compression through Hypernymy and Hyponymy Relationships
According to , hypernymy and hyponymy are two semantic relationships which correspond to a particular case of sense inclusion : the hyponym of a term is a specialization of this term , whereas its hypernym is a generalization .
For instance , a " mouse " is a type of " rodent " which is in turn a type of " animal " .
In WordNet , these relationships bind nearly every noun together in a tree structure 2 that goes from the generic root , the node " entity " to the most specific leaves , for instance the node " whitefooted mouse " .
These relationships are also present on several verbs : for instance " add " is away of " compute " which is away of " reason " .
For the sake of WSD , just like grouping together the senses of the same synset helps to better generalize , we hypothesize that grouping together the synsets of the same hypernymy relationship also helps in the same way .
The general idea of our method is that the most specialized concepts in WordNet are often superfluous for WSD .
Indeed , considering a small subset of WordNet that only consists of the word " mouse " , its first sense ( the small rodent ) , its fourth sense ( the elec - We computed that 41 607 on the 44 449 polysemous nouns of WordNet ( 94 % ) are part of this hierarchy .
tronic device ) , and all of their hypernyms .
This is illustrated in .
We can see that every concept that is more specialized than the concepts " artifact " and " living _ thing " could be removed .
We could map every tag of " mouse# 1 " to the tag of " living _ thing# 1 " and we could still be able to disambiguate this word , but with a benefit : all other " living things " and animals in the sense annotated data could be tagged with the same sense .
They would give examples of what is an animal and then show how to differentiate the small rodent from the hand - operated electronic device .
Therefore , the goal of our method is to map every sense of WordNet to its highest ancestor in the hypernymy hierarchy , but with the following constraints :
First , this ancestor must discriminate all the different senses of the target word .
Second , we need to preserve the hypernyms that are indispensable to discriminate the senses of the other words in the dictionary .
For instance , we can not map " mouse# 1 " to " living _ thing# 1 " , because the more specific tag " animal# 1 " is essential to distinguish the two senses of the word " prey " ( one sense describes a person , the other describes an animal ) .
Our method thus works in two steps :
1 . We mark as " necessary " the children of the first common ancestor of every pair of senses of every word of Word Net .
2 . We map every sense to its first ancestor in the hypernymy hierarchy that has been previously marked as " necessary " .
As a result , the most specific synsets of the tree that are not indispensable for discriminating any word of the lexical inventory are automatically removed from the vocabulary .
In other words , the set of synsets that is left in the vocabulary is the smallest subset of all synsets that are necessary to distinguish every sense of every word of WordNet , following the hypernym and hyponym links .
Compression through all semantic relationships
In addition to hypernymy and hyponymy , Word - Net contains several other relationships between synsets , such as the instance relationship ( e.g. " Albert Einstein " is an instance of " physicist ' ) , the meronymy ( X is part of Y , or X is a member of Y ) and it s counterpart the holonymy , the antonymy ( X is the opposite of Y ) , etc .
We hence propose a second method for sense vocabulary compression , that considers all the semantic relationships offered by WordNet , in order to form clusters of related synsets .
For instance , using all semantic relationships , we could form a cluster containing " physicist " , " physics " ( domain category ) , " Albert Einstein " ( instance of ) , " astronomer " ( hyponym ) , but also further related senses such as " photon " , because it is a meronym of " radiation " , which is a hyponym of " energy " , which belongs to the same domain category of " physics " .
Our method works by constructing these clusters iteratively :
First , we initialize the set of clusters C with one synset in each cluster .
C ={c 0 , c 1 , ... , c n } S = {s 0 , s 1 , ... , s n } C ={{s 0 } , {s 1 } , ... , {s n }}
Then at each step , we sort C by sizes of clusters , and we peek the smallest one c x and the smallest related cluster to c x , c y .
We define a cluster being related to another if they contain at least one synset that have a semantic link together .
We merge c x and c y together , and we verify that the operation still allows to discriminate the different senses of all words in the lexical database .
If it is not the case , we cancel the merge and we try another semantic link .
If no link is possible , we try to create one with the next smallest cluster , and if no further link can be created , the algorithm stops .
In , we show a possible set of clusters that could result from our method , focusing on two senses of the word " Weber " and only on a few relationships . :
Example of clusters of sense that could result from our method , if we limit our view to two senses of the word " Weber " and only some relationship links .
This method produces clusters significantly larger than the method based on hypernyms .
On average , a cluster has 5 senses with the hypernym method , whereas it has 17 senses with this method .
This method , unlike the previous one , is also stochastic , because the formation of clusters depends on the underlying order of iteration when multiple clusters are the same size .
However , because we always sort clusters by size before creating a link , we observed that the final vocabulary size ( i.e. number of clusters ) is always between 11 000 and 13 000 .
In the following , we consider a resulting mapping where the algorithm stopped after 105 774 steps .
Method
Vocabulary size : Effects of the sense vocabulary compression on the vocabulary size and on the coverage of the SemCor .
In , we show the effect of the common compression through synonyms , our first proposed compression through hypernyms , and our second method of compression through all semantic relationships , on the size of the vocabulary of Word - Net sense tags , and on the coverage of the SemCor corpus .
As we can see , the sense vocabulary size is drastically decreased , and the coverage of the same corpus really improved .
In order to evaluate our sense vocabulary compression methods , we applied them on a neural WSD system based on a softmax classifier capable of classifying a word in all possible synsets of Word - Net ( see subsection 2.2 ) .
We implemented a system similar to 's BiLSTM but with some key differences .
In particular , we used BERT contextualized word vectors in input of our network , Transformer encoder layers instead of LSTM layers as hidden units , our output vocabulary only consists of sense tags seen during training ( mapped according to the compression method used ) , and we ignore the network 's predictions on words that are not annotated .
Implementation details
For BERT , we used the model named " bert - largecased " of the PyTorch implementation 3 , which consists of vectors of dimension 1024 , trained on Book s Corpus and English Wikipedia .
Due to the fact that BERT 's internal tokenizer sometimes split words in multiples tokens ( i.e. [ " rodent " ] becomes [ " rode " , " # # nt " ] ) , we trained our system to predict a sense tag on the first token only of a splitted annotated word .
For the Transformer encoder layers , we used the same parameters as the " base " model of , that is 6 layers with 8 attention heads , a hidden size of 2048 , and a dropout of 0.1 .
Finally , because BERT already encodes the position of the words inside their vectors , we did not add any positional encoding .
Training
We compared our sense vocabulary compression methods on two training sets : The SemCor , and the concatenation of the SemCor and the Princeton WordNet Gloss Corpus ( WNGC ) .
The latter is a corpus distributed as part of WordNet since its version 3.0 , and it consists of the definitions ( glosses ) of every synset of WordNet , with words manually or semi-automatically sense annotated .
We used the version of these corpora given as part of the UFSAC 2.1 resource 4 .
We performed every training for 20 epochs .
At the beginning of each epoch , we shuffled the training set .
We evaluated our model at the end of every epoch on a development set , and we kept only the one which obtained the best F1 WSD score .
The development set was composed of 4 000 random sentences taken from the Princeton WordNet Gloss Corpus for the models trained on the Sem - Cor , and 4 000 random sentences extracted from the whole training set for the other models .
For each training set , we trained three systems :
1 . A " baseline " system that predicts a tag belonging to all the synset tags seen during training , thus using the common vocabulary compression through synonyms method .
2 . A " hypernyms " system which applies our vocabulary compression through hypernyms algorithm on the training corpus .
3 . A " all relations " system which applies our second vocabulary compression through all relations on the training corpus .
We trained with mini-batches of 100 sentences , truncated to 80 words , and we used Adam with a learning rate of 0.0001 as the optimization method .
All models have been trained on one Nvidia 's Titan X GPU .
The number of parameters of individual models are displayed in .
As we can see , our compression methods drastically reduce the number of parameters , by a factor of 1.2 to 2 .
Evaluation
We evaluated our models on all evaluation corpora commonly used in WSD , that is the English all - words WSD tasks of the evaluation campaigns SensEval / Sem Eval .
We used the fine - grained evaluation corpora from the evaluation framework of , which consists of Sen - s Eval 2 , SensEval 3 , SemEval 2007 task 17 , SemEval 2013 task 12 and SemEval 2015 task 13 , as well as the " ALL " corpus consisting of the concatenation of all pre- : F1 scores ( % ) on the English WSD tasks of the evaluation campaigns SensEval / Sem Eval .
The task " ALL " is the concatenation of SE2 , SE3 , SE07 17 , SE13 and SE15 .
The first sense is assigned on words for which none of its sense has been observed during the training .
Results in bold are to our knowledge the best results obtained on the task .
Scores prefixed by a dagger ( ) are not provided by the authors but are deduced from their other scores .
vious ones .
We also compared our result on the coarse - grained task 7 of SemEval 2007 which is not present in this framework .
For each evaluation , we trained 8 independent models , and we give the score obtained by an ensemble system that averages their predictions through a geometric mean . :
Coverage of our systems on the task " ALL " .
" Backoff on Monosemics " means that monosemic words are considered annotated .
In the results in , we first observe that our systems that use the sense vocabulary compression through hypernyms or through all relations obtain scores that are overall equivalent to the systems that do not use it .
Our methods greatly improves their coverage on the evaluation tasks however .
As we can see in , on the total of 7 253 words to annotate for the corpus " ALL " , the baseline system trained on the SemCor is notable to annotate 491 of them , while the vocabulary compression through hypernyms reduces this number to 91 and 24 for the compression through all relations .
When adding the Princeton WordNet Gloss Corpus to the training set , only one word ( the monosemic adjective " cytotoxic " ) can not be annotated with the system that uses the compression through all relations because it s sense has not been observed during training .
If we exclude the monosemic words , the system based on our compression method through all relations miss only one word ( the adverb " eloquently " ) when trained on the SemCor , and has a coverage to 100 % when the WNGC is addded .
In comparison to the other works , thanks to the Princeton WordNet Gloss Corpus added to the training data and the use of BERT as input embeddings , we outperform systematically the state of the art on every task .
Ablation Study
In order to give a better understanding of the origin of our scores , we provide a study of the impact of our main parameters on the results .
In addition to the training corpus and the vocabulary compression method , we chose two parameters that differentiate us from the state of the art : the pretrained word embeddings model and the ensembling method , and we have made them vary .
For the word embeddings model , we experimented with BERT as in our main results , with ELMo , and with GloVe , the same pre-trained word embeddings used by .
For ELMo , we used the model trained on : Ablation study on the task " ALL " ( i.e. the concatenation of all SensEval / SemEval tasks ) .
For systems that do not use ensemble , we display the mean score ( x ) of eight individually trained models along with its standard deviation ( ? ) .
Wikipedia and the monolingual news crawl data from For GloVe , we used the model trained on Wikipedia 2014 and Gigaword 5 .
6 Due to the fact that Glo Ve embeddings do not encode the position of the words ( a word has the same vector representation in any context ) , we used bidirectional LSTM cells of size 1 000 for each direction , instead of Transformer encoders for this set of experiments .
In addition , because the vocabulary of GloVe is finite and all words are lowercased , we lowercased the inputs , and we assigned a vector filled with zeros to outof - vocabulary words .
For the ensembling method , we either perform ensembling as in our main results , by averaging the prediction of 8 models trained separately or we give the mean and the standard deviation of the scores of the 8 models evaluated separately .
As we can see in , the additional training corpus ( WNGC ) and even more the use of BERT as input embeddings both have a major impact on our results and lead to scores above the state of the art .
Using BERT instead of ELMo or Glo Ve improves respectively the score by approximately 3 and 5 points in every experiment , and adding the WNGC to the training data improves it by approximately 2 points .
Finally , using ensembles adds roughly another 1 point to the final F1 score .
5 https://allennlp.org/elmo
6 https://nlp.stanford.edu/projects/glove/
Finally , through the scores obtained by invidual models ( without ensemble ) , we can observe on the standard deviations that the vocabulary compression method through hypernyms never impact significantly the final score .
However , the compression method through all relations seems to negatively impact the results in some cases ( when using ELMo or GloVe especially ) .
Conclusion
In this paper , we presented two new methods that improve the coverage and the capacity of generalization of supervised WSD systems , by narrowing down the number of different sense in WordNet in order to keep only the senses that are essential for differentiating the meaning of all words of the lexical database .
On the scale of the whole lexical database , we showed that these methods can shrink the total number of different sense tags in WordNet to only 6 % of the original size , and that the coverage of an identical training corpus has more than doubled .
We implemented a state of the art WSD neural network and we showed that these methods compress the size of the underlying models by a factor of 1.2 to 2 , and greatly improve their coverage on the evaluation tasks .
As a result , we reach a coverage of 99. 99 % of the evaluation tasks ( 1 word missing on 7 253 ) when training a system on the SemCor only , and 100 % when adding the WNGC to the training data , on the pol-ysemic words .
Therefore , the need fora backoff strategy is nearly eliminated .
Finally , our method combined with the recent advances in contextualized word embeddings and with a training corpus composed of sense annotated glosses , our system achieves scores that considerably outperform the state of the art on all WSD evaluation tasks .
