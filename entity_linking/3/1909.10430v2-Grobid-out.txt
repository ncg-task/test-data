title
Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings
abstract
Contextualized word embeddings (CWE) such as provided by ELMo (Peters et al.,  2018), Flair NLP (Akbik et al., 2018), or  BERT (Devlin et al., 2019 area major recent innovation in NLP. CWEs provide semantic vector representations of words depending on their respective context. Their advantage over static word embeddings has been shown fora number of tasks, such as text classification, sequence tagging, or machine translation. Since vectors of the same word type can vary depending on the respective context, they implicitly provide a model for word sense disambiguation (WSD). We introduce a simple but effective approach to WSD using a nearest neighbor classification on CWEs. We compare the performance of different CWE models for the task and can report improvements above the current state of the art for two standard WSD benchmark datasets. We further show that the pre-trained BERT model is able to place polysemic words into distinct 'sense' regions of the embedding space, while ELMo and Flair NLP do not seem to possess this ability.
Synonymy and Polysemy of Word Representations
Lexical semantics is characterized by a high degree of polysemy, i.e. the meaning of a word changes depending on the context in which it is currently used. Word Sense Disambiguation (WSD) is the task to identify the correct sense of the usage of a word from a (usually) fixed inventory of sense identifiers. For the English language, WordNet is the most commonly used sense inventory providing more than 200K word-sense pairs.
To train and evaluate WSD systems, a number of shared task datasets have been published in the SemEval workshop series. In the lexical sample task, a training set and a test set is provided. The relatively large data contains one sense-annotated word per training/test instance. The all-words task only provides a small number of documents as test data where each ambiguous word is annotated with its sense. To facilitate the comparison of WSD systems, some efforts have been made to provide a comprehensive evaluation framework, and to unify all publicly available datasets for the English language.
WSD systems can be distinguished into three types -knowledge-based, supervised, and semisupervised approaches. Knowledge-based systems utilize language resources such as dictionaries, thesauri and knowledge graphs to infer senses. Supervised approaches train a machine classifier to predict a sense given the target word and its context based on an annotated training data set. Semisupervised approaches extend manually created training sets by large corpora of unlabeled data to improve WSD performance. All approaches rely on someway of context representation to predict the correct sense. Context is typically modeled via dictionary resources linked with senses, or as some feature vector obtained from a machine learning model.
A fundamental assumption in structuralist linguistics is the distinction between signifier and signified as introduced by Ferdinand de Saussure in the early 20 th century. Computational linguistic approaches, when using character strings as the only representatives for word meaning, implicitly assume identity between signifier and signified. Different word senses are simply collapsed into the same string representation. In this respect, word counting and dictionary-based approaches to analyze natural language texts have been criticized as pre-Saussurean. In contrast, the distributional hypothesis not only states that meaning is dependent on context. It also states that words occurring in the same contexts tend to have a similar meaning. Hence, a more elegant way of representing meaning has been introduced by using the contexts of a word as an intermediate semantic representation that mediates between signifier and signified. For this, explicit vector representations, such as TF-IDF, or latent vector representations, with reduced dimensionality, have been widely used. Latent vector representations of words are commonly called word embeddings. They are fixed length vector representations, which are supposed to encode semantic properties. The seminal neural word embedding model, for instance, can be trained efficiently on billions of sentence contexts to obtain semantic vectors, one for each word type in the vocabulary. It allows synonymous terms to have similar vector representations that can be used for modeling virtually any downstream NLP task. Still, a polysemic term is represented by one single vector only, which represents all of its different senses in a collapsed fashion.
To capture polysemy as well, the idea of word embeddings has been extended to encode word sense embeddings. first introduced a neural model to learn multiple embeddings for one word depending on different senses. The number of senses can be defined by a given parameter, or derived automatically in a non-paramentric version of the model. However, employing sense embeddings in any downstream NLP task requires a reliable WSD system in an earlier stage to decide how to choose the appropriate embedding from the sense inventory.
Recent efforts to capture polysemy for word embeddings give upon the idea of a fixed word sense inventory. Contextualized word embeddings (CWE) do not only create one vector representation for each type in the vocabulary, they also they produce distinct vectors for each token in a given context. The contextualized vector representation is supposed to represent word meaning and context information. This enables downstream tasks to actually distinguish the two levels of the signifier and the signified allowing for more realistic modeling of natural language. The advantage of such contextually embedded token representations compared to static word embeddings has been shown fora number of tasks such as text classification and sequence tagging.

Contribution:
We show that CWEs can be utilized directly to approach the WSD task due to their nature of providing distinct vector representations for the same token depending on its context. To learn the semantic capabilities of CWEs, we employ a simple, yet interpretable approach to WSD using a k-nearest neighbor classification (kNN) approach. We compare the performance of three different CWE models on four standard benchmark datasets. Our evaluation yields that not all contextualization approaches are equally effective in dealing with polysemy, and that the simple kNN approach suffers severely from sparsity in training datasets. Yet, by using kNN, we include provenance into our model, which allows to investigate the training sentences that lead to the classifier's decision. Thus, we are able to study to what extent polysemy is captured by a specific contextualization approach. For two datasets, we are able to report new state-of-the-art (SOTA) results.

Related Work

Neural Word Sense Disambiguation
Several efforts have been made to induce different vectors for the multiplicity of senses a word can express.,, or induce so-called sense embeddings in a pre-training fashion. While induce sense embeddings in an unsupervised way and only fix the maximum number of senses per word, require a pre-labeled sense inventory such as WordNet. Then, the sense embeddings are mapped to their corresponding synsets. Other approaches include the re-use of pre-trained word embeddings in order to induce new sense embeddings. then also use induced sense embeddings for the downstream task of WSD. Camacho-Collados and Pilehvar (2018) provide an extensive overview of different word sense modeling approaches.
For WSD, (semi-)supervised approaches with recurrent neural network architectures represent the current state of the art. Two major approaches were followed. First, and, for instance, compute sentence context vectors for ambiguous target words. In the prediction phase, they select nearest neighbors of context vectors to determine the target word sense. also use unlabeled sentences in a semisupervised label propagation approach to overcome the sparse training data problem of the WSD task. Second, Kågebäck and Salomonsson (2016) employ a recurrent neural network to classify sense labels for an ambiguous target word given its surrounding sentence context. In contrast to earlier approaches, which relied on feature engineering, their architecture only uses pretrained GloVe word embeddings to achieve SOTA results on two English lexical sample datasets. For the all-words WSD task, also employ a recurrent neural network. But instead of single target words, they sequentially classify sense labels for all tokens in a sentence. They also introduce an approach to collapse the sense vocabulary from WordNet to unambiguous hypersenses, which increases the label to sample ratio for each label, i.e. sense identifier. By training their network on the large sense annotated datasets SemCor and the Princeton Annotated Gloss Corpus based on WordNet synset definitions, they achieve the highest performance so far on most all-words WSD benchmarks. A similar architecture with an enhanced sense vocabulary compression was applied in, but instead of GloVe embeddings, BERT wordpiece embeddings are used as input for training. Especially the BERT embeddings further improved the performance yielding new state-of-the-art results.

Contextualized Word Embeddings
The idea of modeling sentence or context-level semantics together with word-level semantics proved to be a powerful innovation. For most downstream NLP tasks, CWEs drastically improved the performance of neural architectures compared to static word embeddings. However, the contextualization methodologies differ widely. We, thus, hypothesize that they are also very different in their ability to capture polysemy.
Like static word embeddings, CWEs are trained on large amounts of unlabeled data by some vari-ant of language modeling. In our study, we investigate three most prominent and widely applied approaches: Flair, ELMo (Peters et al., 2018), and BERT.
Flair: For the contextualization provided in the Flair NLP framework, take a static pre-trained word embedding vector, e.g. the GloVe word embeddings, and concatenate two context vectors based on the left and right sentence context of the word to it. Context vectors are computed by two recurrent neural models, one character language model trained from left to right, one another from right to left. Their approach has been applied successfully especially for sequence tagging tasks such as named entity recognition and part-of-speech tagging.
ELMo: Embeddings from language models (ELMo) approaches contextualization similar to Flair, but instead of two character language models, two stacked recurrent models for words are trained, again one left to right, and another right to left. For CWEs, outputs from the embedding layer, and the two bidirectional recurrent layers are not concatenated, but collapsed into one layer by a weighted, element-wise summation.
BERT: In contrast to the previous two approaches, Bidirectional Encoder Representations from Transformers (BERT) does not rely on the merging of two uni-directional recurrent language models with a (static) word embedding, but provides contextualized token embeddings in an end-to-end language model architecture. For this, a self-attention based transformer architecture is used, which, in combination with a masked language modeling target, allows to train the model seeing all left and right contexts of a target word at the same time. Self-attention and non-directionality of the language modeling task result in extraordinary performance gains compared to previous approaches.
According to the distributional hypothesis, if the same word regularly occurs in different, distinct contexts, we may assume polysemy of its meaning). Contextualized embeddings should be able to capture this property. In the following experiments, we investigate this hypothesis on the example of the introduced models.
SE-2 (Tr) SE-2 (Te) SE-3 (Tr) SE-3 (Te) S7-T7 (coarse) S7-T17

Nearest Neighbor Classification for WSD
We employ a rather simple approach to WSD using non-parametric nearest neighbor classification (kNN) to investigate the semantic capabilities of contextualized word embeddings. Compared to parametric classification approaches such as support vector machines or neural models, kNN has the advantage that we can directly investigate the training examples that lead to a certain classifier decision.
The kNN classification algorithm) assigns a plurality vote of a sample's nearest labeled neighbors in its vicinity. In the most simple case, one-nearest neighbor, it predicts the label from the nearest training instance by some defined distance metric. Although complex weighting schemes for kNN exist, we stick to the simple non-parametric version of the algorithm to be able to better investigate the semantic properties of different contextualized embedding approaches.
As distance measure for kNN, we rely on cosine distance of the CWE vectors. Our approach considers only senses fora target word that have been observed during training. We call this approach localized nearest neighbor word sense disambiguation. We use spaCy 1 for pre-processing and the lemma of a word as the target word representation, e.g. 'danced', 'dances' and 'dancing' are mapped to the same lemma 'dance'. Since BERT uses wordpieces, i.e. subword units of words instead of entire words or lemmas, we re-tokenize the lemmatized sentence and average all wordpiece CWEs that belong to the target word. Moreover, for the experiments with BERT embeddings 2 , we follow the heuristic by and concatenate the averaged wordpiece vectors of the last four layers. 1 https://spacy.io/ 2 We use the bert-large-uncased model.
We test different values for our single hyperparameter k ? {1, . . . , 10, 50, 100, 500, 1000}. Like words in natural language, word senses follow a power-law distribution. Due to this, simple baseline approaches for WSD like the most frequent sense (MFS) baseline are rather high and hard to beat. Another effect of the skewed distribution are imbalanced training sets. Many senses described in WordNet only have one or two example sentences in the training sets, or are not present at all. This is severely problematic for larger k and the default implementation of kNN because of the majority class dominating the classification result. To deal with sense distribution imbalance, we modify the majority voting of kNN to k = min(k, |V s |) where V sis the set of CWEs with the least frequent training examples fora given word sense s.

Datasets
We conduct our experiments with the help of four standard WSD evaluation sets, two lexical sample tasks and two all-words tasks. As lexical sample tasks, SensEval-2 and provide a training data set and test set each. The all-words tasks of SemEval 2007 Task 7 and Task 17 solely comprise test data, both with a substantial overlap of their documents. The two sets differ in granularity: While ambiguous terms in Task 17 are annotated with one WordNet sense only, in Task 7 annotations are coarser clusters of highly similar WordNet senses. For training of the allwords tasks, we use a) the SemCor dataset, and b) the Princeton WordNet gloss corpus (WNGT) separately to investigate the influence of different training sets on our approach. For all experiments, we utilize the suggested datasets as provided by the UFSAC Model SE-2 SE-3 S7-T7 (coarse) S7-T17 (fine), i.e. the respective training data. A concise overview of the data can be found in. From this, we can observe that the SE-2 and SE-3 training data sets, which were published along with the respective test sets, provide many more examples per word and sense than SemCor or WNGT.

Experimental Results
We conduct two experiments to determine whether contextualized word embeddings can solve the WSD task. In our first experiment, we compare different pre-trained embeddings with k = 1. In our second experiment, we test multiple values of k and the BERT pre-trained embeddings 4 in order to estimate an optimal k. Further, we qualitatively examine the results to analyze, which cases can be typically solved by our approach and where it fails.

Contextualized Embeddings
To compare different CWE approaches, we use k = 1 nearest neighbor classification. shows a high variance in performance. Simple kNN with ELMo as well as BERT embeddings beats the state of the art of the lexical sample task SE-2 (cp. Table 3). BERT also outperforms all others on the SE-3 task. However, we observe a major performance drop of our approach for the two all-words WSD tasks in which no training data is provided along with the test set. For S7-T7 and S7-T17, the content and structure of the out-of-domain SemCor and WNGT training datasets differ drastically from those in the test data, which prevents yielding near stateof-the-art results. In fact, similarity of contextualized embeddings largely relies on semantically and structurally similar sentence contexts of polysemic target words. Hence, the more example sentences can be used fora sense, the higher are the chances 3 Unification of Sense Annotated Corpora and Tools. We are using Version 2.1: https://github.com/getalp/ UFSAC 4 BERT performed best in experiment one.  that a nearest neighbor expresses the same sense.
As can be seen in, the SE-2 and SE-3 training datasets provide more CWEs for each word and sense, and our approach performs better with a growing number of CWEs, even with a higher average number of senses per word as is the casein SE-3. Thus, we conclude that the nearest neighbor approach suffers specifically from data sparseness. The chances increase that aspects of similarity other than the sense of the target word in two compared sentence contexts drive the kNN decision. Moreover, CWEs actually do not organize well in spherically shaped form in the embedding space. Although senses might be actually separable, the nonparametric kNN classification is unable to learn a complex decision boundary focusing only on the most informative aspects of the CWE.

Nearest Neighbors

K-Optimization:
To attenuate for noise in the training data, we optimize fork to obtain a more robust nearest neighbor classification. shows our best results using the BERT embeddings along with results from related works. For SensEval-2 and SensEval-3, we achieve anew state-of-the-art result. We observe convergence with higher k values since our k normalization heuristic is activated. For the S7-T*, we also achieve minor improvements with a higher k, but still drastically lack behind the state of the art.
Senses in CWE space: We investigate how well different CWE models encode information such as distinguishable senses in their vector space. shows T-SNE plots (van der Maaten and Hinton, 2008) of six different senses of the word "bank" in the SE-3 training dataset encoded by the three different CWE methods. For visualization purposes, we exclude senses with a frequency of less than two. The Flair embeddings hardly allow to distinguish any clusters as most senses are scattered across the entire plot. In the ELMo embedding space, the major senses are slightly more separated in different regions of the point cloud. Only in the BERT embedding space, some senses form clearly separable clusters. Also within the larger clusters, single senses are spread mostly in separated regions of the cluster. Hence, we conclude that BERT embeddings actually seem to encode some form of sense knowledge, which also explains why kNN can be successfully applied to them. Moreover, we can see why a more powerful parametric classification approach such as employed by is able to learn clear decision boundaries. Such clear decision boundaries seem to successfully solve the data sparseness issue of kNN.
Error analysis: From a qualitative inspection of true positive and false positive predictions, we are able to infer some semantic properties of the BERT embedding space and the used training corpora. shows selected examples of polysemic words in different test sets, including their nearest neighbor from the respective training set.
Not only vocabulary overlap in the context as in 'along the bank of the river' and 'along the bank of the river Greta' (2) allows for correct predictions, but also semantic overlap as in 'little earthy bank' and 'huge bank [of snow]' (3). On the other hand, vocabulary overlap, as well as semantic relatedness as in 'land bank' (5) can lead to false predictions. Another interesting example for the latter is the confusion between 'grass bank' and 'river bank' (6) where the nearest neighbor sentence in the training set shares some military context with the target sentence. The correct sense (bank%1:17:01::Sloping Land) and the predicted sense (bank%1:17:00::A Long Ridge or Pile [of earth]) share high semantic similarity, too. In this example, they might even be used interchangeably. Apparently this context yields higher similarity than any of the other training sentences containing 'grass bank' explicitly.
In Example (10), the targeted sense is an action, i.e. a verb sense, while the predicted sense is a noun, i.e. a different word class. In general, this could be easily fixed by restricting the classifier decision to the desired POS. However, while example (12) is still a false positive, it nicely shows that the simple kNN approach is able to distinguish senses byword class even though BERT never learned POS classes explicitly. This effect has been investigated in-depth by, who found that each BERT layer learns different structural aspects of natural language. Example (12) also emphasizes the difficulty of distinguishing verb senses itself, i.e. the correct sense label in this example is watch%2:39:00::look attentively whereas the predicted label and the nearest neighbor is watch%2:41:00::follow with the eyes or the mind; observe. Verb senses in WordNet are very fine grained and thus harder to distinguish automatically and by humans, too.

Post-evaluation experiment
In order to address the issue of mixed POS senses, we run a further experiment, which restricts words to their lemma and their POS tag. shows that including the POS restriction increases the F1 scores for S7-T7 and S7-T17. This can be explained by the number of different POS tags that can be found in the different corpora (c.f.). The results are more stable with respect to their relative performance, i.e. SemCor and WNGT reach comparable scores on S7-T17. Also, the results for SE-2 and SE-3 did not change drastically. This can be explained by the average number of POS tags a certain word is labeled with. This variety is much stronger in the S7-T* tasks compared to SE-*.

Conclusion
In this paper, we tested the semantic properties of contextualized word embeddings (CWEs) to address word sense disambiguation. 5 To test their capabilities to distinguish different senses of a particular word, by placing their contextualized vector (2) Soon after setting off we came to a forested valley along the banks [Sloping Land] of the Gwaun.
In my own garden the twisted hazel, corylus avellana contorta, is underplanted with primroses, bluebells and wood anemones, for that is how I remember them growing, as they still do, along the banks [Sloping Land] of the rive Greta
(3) In one direction only a little earthy bank separates me from the edge of the ocean, while in the other the valley goes back for miles and miles.
The lake has been swept clean of snow by the wind, the sweepings making a huge bank on our side that we have to negotiate.
(4) However, it can be possible for the documents to be signed after you have sent a payment by cheque provided that you arrange for us to hold the cheque and not pay it into the bank until we have received the signed deed of covenant.
The purpose of these stubs in a paying -in book is for the holder to have a record of the amount of money he had deposited in his bank .
(5) He continued: assuming current market conditions do not deteriorate further, the group, with conservative borrowings, a prime land bank and a good forward sales position can look forward to another year of growth.
Crest Nicholson be the exception, not have much of a land bank and rely on its skill inland buying.
(6) The marine said, get down behind that grass bank , sir, and he immediately lobbed a mills grenade into the river.
The guns were all along the riverbank [Sloping Land] as far as I could see.
SemCor S7-T17 (7) Some 30 balloon shows are held annually in the U.S., including the world's largest convocation of ersatz Phineas Foggs -the nine-day Albuquerque International Balloon Fiesta that attracts some 800, 000 enthusiasts and more than 500 balloons, some of which are fetchingly shaped to resemble Carmen Miranda, Garfield or a 12-story-high condom.
Homes and factories and schools and a big wide federal highway, instead of peaceful corn to rest your eyes on while you tried to rest your heart, while you tried not to look at the balloon [Large Tough Nonrigid Bag] and the bandstand and the uniforms and the flash of the instruments..
representation into different regions of the shared vector space, we used a k-nearest neighbor approach, which allows us to investigate their properties on an example basis. For experimentation, we used pre-trained models from Flair NLP, ELMo, and BERT). Further, we tested our hypothesis on four standard benchmark datasets for word sense disambiguation. We conclude that WSD can be surprisingly effective using solely CWEs. We are even able to report improvements over state-ofthe-art results for the two lexical sample tasks of SenseEval-2 and SensEval-3.
Further, experiments showed that CWEs in general are able to capture senses, i.e. words, when used in a different sense, are placed in different regions. This effect appeared strongest using the BERT pre-trained model, where example instances even form clusters. This might give rise to future directions of investigation, e.g. unsupervised word sense-induction using clustering techniques.: Percentage of senses with a certain POS tag in the corpora.
Since the publication of the BERT model, a number of extensions based on transformer architectures and language model pre-training have been released. In future work, we plan to evaluate also XLM (Lample and Conneau, 2019), RoBERTa and XLNet (Yang et al., 2019) with our approach. In our qualitative error analysis, we observed many near-misses, i.e. the target sense and the predicted sense are not particularly faraway. We will investigate if more powerful classification algorithms for WSD based on contextualized embeddings are able to solve this issue even in cases of extremely sparse training data.