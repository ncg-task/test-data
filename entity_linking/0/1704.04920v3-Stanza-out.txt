title
Deep Joint Entity Disambiguation with Local Neural Attention
abstract
We propose a novel deep learning model for joint document - level entity disambiguation , which leverages learned neural representations .
Key components are entity embeddings , a neural attention mechanism over local context windows , and a differentiable joint inference stage for disambiguation .
Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention - entity maps .
Extensive experiments show that we are able to obtain competitive or stateof - the - art accuracy at moderate computational costs .
Introduction
Entity disambiguation ( ED ) is an important stage in text understanding which automatically resolves references to entities in a given knowledge base ( KB ) .
This task is challenging due to the inherent ambiguity between surface form mentions such as names and the entities they refer to .
This many - to - many ambiguity can often be captured partially by name- entity co-occurrence counts extracted from entity - linked corpora .
ED research has largely focused on two types of contextual information for disambiguation : local information based on words that occur in a context window around an entity mention , and , global information , exploiting document - level coherence of the referenced entities .
Many stateof - the - art methods aim to combine the benefits of both , which is also the philosophy we follow in this paper .
What is specific to our approach is that we use embeddings of entities as a common representation to assess local as well as global evidence .
In recent years , many text and language understanding tasks have been advanced by neural network architectures .
However , despite recent work , competitive ED systems still largely employ manually designed features .
Such features often rely on domain knowledge and may fail to capture all relevant statistical dependencies and interactions .
The explicit goal of our work is to use deep learning in order to learn basic features and their combinations from scratch .
To the best of our knowledge , our approach is the first to carryout this program with full rigor .
Contributions and Related Work
There is avast prior research on entity disambiguation , highlighted by .
We will focus hereon a discussion of our main contributions in relation to prior work .
Entity Embeddings .
We have developed a simple , yet effective method to embed entities and words in a common vector space .
This follows the popular line of work on word embeddings , e.g. , which was recently extended to entities and ED by .
In contrast to the above methods that require data about entity - entity co-occurrences which often suffers from sparsity , we rather bootstrap entity embeddings from their canonical entity pages and local context of their hyperlink annotations .
This allows for more efficient training and alleviates the need to compile co-linking statistics .
These vector representations area key component to avoid hand - engineered features , multiple disambiguation steps , or the need for additional ad hoc heuristics when solving the ED task .
Context Attention .
We present a novel attention mechanism for local ED .
Inspired by mem-ory networks of and insights of , our model deploys attention to select words that are informative for the disambiguation decision .
A learned combination of the resulting context - based entity scores and a mention - entity prior yields the final local scores .
Our local model achieves better accuracy than the local probabilistic model of , as well as the feature - engineered local model of .
As an added benefit , our model has a smaller memory footprint and it 's very fast for both training and testing .
There have been other deep learning approaches to define local context models for ED .
For instance use convolutional neural networks ( CNNs ) and stacked denoising auto - encoders , respectively , to learn representations of textual documents and canonical entity pages .
Entities for each mention are locally scored based on cosine similarity with the respective document embedding .
Ina similar local setting , embed mentions , their immediate contexts and their candidate entities using word embeddings and CNNs .
However , their entity representations are restrictively built from entity titles and entity categories only .
Unfortunately , the above models are rather ' blackbox ' ( as opposed to ours which reveals the attention focus ) and were never extended to perform joint document disambiguation .
Collective Disambiguation .
Last , a novel deep learning architecture for global ED is proposed .
Mentions in a document are resolved jointly , using a conditional random field with parametrized potentials .
We suggest to learn the latter by casting loopy belief propagation ( LBP ) as a rolled - out deep network .
This is inspired by similar approaches in computer vision , e.g. , and allows us to backpropagate through the ( truncated ) message passing , thereby optimizing the CRF potentials to work well in conjunction with the inference scheme .
Our model is thus trained end - to - end with the exception of the pre-trained word and entity embeddings .
Previous work has investigated different approximation techniques , including : random graph walks , personalized PageRank , intermention voting , graph pruning , integer linear programming , or ranking SVMs .
Mostly connected to our approach is where LBP is used for inference ( but not learning ) in a probabilistic graphical model and where a single round of message passing with attention is performed .
To our knowledge , we are one of the first to investigate differentiable message passing for NLP problems .
Learning Entity Embeddings
Ina first step , we propose to train entity vectors that can be used for the ED task ( and potentially for other tasks ) .
These embeddings compress the semantic meaning of entities and drastically reduce the need for manually designed features or co-occurrence statistics .
Entity embeddings are bootstrapped from word embeddings and are trained independently for each entity .
A few arguments motivate this decision : ( i ) there is no need for entity co-occurrence statistics that suffer from sparsity issues and / or large memory footprints ; ( ii ) vectors of entities in a subset domain of interest can be trained separately , obtaining potentially significant speed - ups and memory savings that would otherwise be prohibitive for large entity KBs ; 1 ( iii ) entities can be easily added in an incremental manner , which is important in practice ; ( iv ) the approach extends well into the tail of rare entities with few linked occurrences ; ( v ) empirically , we achieve better quality compared to methods that use entity cooccurrence statistics .
Our model embeds words and entities in the same low - dimensional vector space in order to exploit geometric similarity between them .
We start with a pre-trained word embedding map x : W ?
Rd that is known to encode semantic meaning of words w ?
W ; specifically we use word2 vec pretrained vectors .
We extend this map to entities E , i.e. x : E ?
Rd , as described below .
We assume a generative model in which words that co-occur with an entity e are sampled from a conditional distribution p ( w |e ) when they are generated .
Empirically , we collect word - entity cooccurrence counts # ( w , e ) from two sources : ( i ) the canonical KB description page of the entity ( e.g. entity 's Wikipedia page in our case ) , and ( ii ) the windows of fixed size surrounding mentions of the entity in an annotated corpus ( e.g. Wikipedia hyperlinks in our case ) .
These counts define a practical approximation of the above word - entity conditional distribution , i.e.p ( w |e ) ? # ( w , e ) .
We call this the " positive " distribution of words related to the entity .
Next , let q ( w ) be a generic word probability distribution which we use for sampling " negative " words unrelated to a specific entity .
As in , we choose a smoothed unigram distribution q ( w ) = p ( w ) ?
for some ? ?
( 0 , 1 ) .
The desired outcome is that vectors of positive words are closer ( in terms of dot product ) to the embedding of entity e compared to vectors of random words .
Let w + ? p ( w|e ) and w ? ? q ( w ) .
Then , we use a max-margin objective to infer the optimal embedding for entity e:
where ? >
0 is a margin parameter and [] + is the ReLU function .
The above loss is optimized using stochastic gradient descent with projection over sampled pairs ( w + , w ? ) .
Note that the entity vector is directly optimized on the unit sphere which is important in order to obtain qualitative embeddings .
We empirically assess the quality of our entity embeddings on entity similarity and ED tasks as detailed in Section 7 and Appendix A .
The technique described in this section can also be applied , in principle , for computing embeddings of general text documents , but a comparison with such methods is left as future work .
Local Model with Neural Attention
We now explain our local ED approach that uses word and entity embeddings to steer a neural attention mechanism .
We build on the insight that only a few context words are informative for resolving an ambiguous mention , something that has been exploited before in .
Focusing only on those words helps reducing noise and improves disambiguation .
observe the same problem and adopt the restrictive strategy of removing all non-nouns .
Here , we assume that a context word maybe relevant , if it is strongly related to at least one of the entity candidates of a given mention .
Context Scores .
Let us assume that we have computed a mention - entity priorp ( e |m ) ( procedure detailed in Section 6 ) .
In addition , for each mention m , a pruned candidate set ? ( m ) of at most S entities has been identified .
Our model , depicted in , computes a score for each e ? ? ( m ) based on the K-word local context c = {w 1 , . . . , w K } surrounding m , as well as on the prior .
It is a composition of differentiable functions , thus it is smooth from input to output , allowing us to easily compute gradients and backpropagate through it .
Each word w ?
c and entity e ? ? ( m ) is mapped to its embedding via the pre-trained map x ( cf. Section 3 ) .
We then compute an unnormalized support score for each word in the context as follows :
where A is a parameterized diagonal matrix .
The weight is high if the word is strongly related to at least one candidate entity .
We often observe that uninformative words ( e.g. similar to stop words ) receive non-negligible scores which add undesired noise to our local context model .
As a consequence , we ( hard ) prune to the top R ?
K words with the highest scores 2 and apply a softmax function on these weights .
Define the reduced context : c
Then , the final attention weights are explicitly
Finally , we define a ? - weighted context - based entity - mention score via
where B is another trainable diagonal matrix .
We will later use the same architecture for the unary scores of our global ED model .
Local Score Combination .
We integrate these context scores with the context - independent scores encoded inp ( e |m ) .
We find a flexible choice for f to be important and superior to a nave weighted average combination model .
We therefore use a neural network with two fully connected layers of 100 hidden units and ReLU non-linearities , which we regularize as suggested in by constraining the sum of squares of all weights in the linear layer .
We use standard projected SGD for training .
The same network is also used in Section 5 .
Prediction is done independently for each mention mi and context c i by maximizing the ?( e , mi , c i ) score .
Learning the Local Model .
Entity and word embeddings are pre-trained as discussed in Section 3 .
Thus , the only learnable parameters are the diagonal matrices A and B , plus the parameters off .
Having few parameters helps to avoid overfitting and to be able to train with little annotated data .
We assume that a set of known mention - entity pairs { ( m , e * ) } with their respective context windows have been extracted from a corpus .
For model fitting , we then utilize a max - margin loss that ranks ground truth entities higher than other candidate entities .
This leads us to the objective :
g ( e , m ) :
where ? >
0 is a margin parameter and Dis a training set of entity annotated documents .
We aim to find a ? ( i.e. parameterized by ? ) such that the score of the correct entity e * referenced by m is at least a margin ?
higher than that of any other candidate entity e.
Whenever this is not the case , the margin violation becomes the experienced loss .
Document - Level Deep Model
Next , we address global ED assuming document coherence among entities .
We therefore introduce the notion of a document as consisting of a set of mentions m = m 1 , . . . , m n , along with their context windows c = c 1 , . . . c n .
Our goal is to define a joint probability distribution over
Each such e selects one candidate entity for each mention in the document .
Obviously , the state space of e grows exponentially in the number of mentions n.
where C is a diagonal matrix .
Similar to , the above normalization helps balancing the unary and pairwise terms across documents with different numbers of mentions .
The function value g(e , m , c ) is supposedly high for semantically related sets of entities that also have local support .
The goal of a global ED prediction method is to perform maximum - aposteriori on this CRF to find the set of entities e that maximize g( e , m , c ) .
Differentiable Inference .
Training and prediction in binary CRF models as the one above is NP - hard .
Therefore , in learning one usually maximizes a likelihood approximation and during operations ( i.e. in prediction ) one may use an approximate inference procedure , often based on message - passing .
Among many challenges of these approaches , it is worth pointing out that weaknesses of the approximate inference procedure are generally not captured during learning .
Inspired by , we use truncated fitting of loopy belief propagation ( LBP ) to a fixed number of message passing iterations .
Our model directly optimizes the marginal likelihoods , using the same networks for learning and prediction .
As noted by , this method is robust to model mis-specification , avoids inherent difficulties of partition functions and is faster compared to double - loop likelihood training ( where , for each stochastic update , inference is run until convergence is achieved ) .
Our architecture is shown in .
A neural network with T layers encodes T message passing iterations of synchronous max - product LBP 3 which is designed to find the most likely ( MAP ) entity assignments that maximize g ( e , m , c ) .
We also use message damping , which is known to speed - up and stabilize convergence of message passing .
Formally , in iteration t , mention mi votes for entity candidate e ? ? ( m j ) of mention m j using the normalized log - message mt i?j ( e ) computed as :
Herein the first part just reflects the CRF potentials , whereas the second part is defined as
where ? ?
( 0 , 1 ] is a damping factor .
Note that , without loss of generality , we simplify the LBP procedure by dropping the factor nodes .
The messages at first iteration ( layer ) are set to zero .
After T iterations ( network layers ) , the beliefs ( marginals ) are computed as :
Similar to the local case , we obtain accuracy improvement when combining the mention - entity priorp ( e |m ) with marginal i ( e ) using the same non-linear combination function f from Equation 6 as follows :
The learned function f for global ED is nontrivial ( see , showing that the influence of the prior tends to weaken for larger ( e ) , whereas it has a dominating influence whenever the document - level evidence is weak .
We also experimented with the prior integrated directly inside the unary factors ?
i ( e i ) , but results were worse because , in some cases , the global entity interaction is notable to recover from strong incorrect priors ( e.g. country names have a strong prior towards the respective countries as opposed to national sports teams ) .
Parameters of our global model are the diagonal matrices A , B , C and the weights of the f network .
As before , we find a margin based objective to be the most effective and we suggest to fit parameters by minimizing a ranking loss 4 defined as :
Computing this objective is trivial by running T times the steps described by Eqs. ( 10 ) , ( 11 ) , followed in the end by the step in Eq. ( 13 ) .
Each step is differentiable and the gradient of the model parameters can be computed on the resulting marginals and back - propagated over messages using chain rule .
At test time , marginals ? i ( e ) are computed jointly per document using this network , but prediction is done independently for each mention mi by maximizing its respective marginal score .
Candidate Selection
We use a mention - entity priorp ( e |m ) both as a feature and for entity candidate selection .
It is 4 Optimizing a marginal log - likelihood loss function performed worse ..
WLM is a well - known similarity method of computed by averaging probabilities from two indexes build from mention entity hyperlink count statistics from Wikipedia and a large Web corpus .
Moreover , we add the YAGO dictionary of , where each candidate receives a uniform prior .
Candidate selection , i.e. construction of ? ( e ) , is done for each input mention as follows : first , the top 30 candidates are selected based on the prior p ( e |m ) .
Then , in order to optimize for memory and run time ( LBP has complexity quadratic in S ) , we keep only 7 of these entities based on the following heuristic : ( i ) the top 4 entities based on p ( e |m ) are selected , ( ii ) the top 3 entities based on the local context - entity similarity measured using the function from Eq. 5 are selected .
5 . We refrain from annotating mentions without any candidate entity , implying that precision and recall can be different in our case .
Ina few cases , generic mentions of persons ( e.g. " Peter " ) are coreferences of more specific mentions ( e.g. " Peter Such " ) from the same document .
We employ a simple heuristic to address this issue : for each mention m , if there exist mentions of persons that contain m as a continuous subse - Methods AIDA - B Local models priorp ( e|m ) 71.9 86.4 87.9 87.2 our ( local , K=100 , R = 50 ) 88.8 Global models 86.6 ( 87.6 88.7 89.0 91.0 91.5 our 92.22 0.14 quence of words , then we consider the merged set of the candidate sets of these specific mentions as the candidate set for the mention m .
We decide that a mention refers to a person if it s most probable candidate byp ( e |m ) is a person .
Experiments
ED Datasets
We validate our ED models on some of the most popular available datasets used by our predecessors 6 .
We provide statistics in .
AIDA - CoNLL dataset is one of the biggest manually annotated ED datasets .
It contains training ( AIDA - train ) , validation ( AIDA - A ) and test ( AIDA - B ) sets .
MSNBC ( MSB ) , AQUAINT ( AQ ) and ACE2004 ( ACE ) datasets cleaned and updated by WNED - WIKI ( WW ) and WNED - CWEB ( CWEB ) : are larger , but automatically extracted , thus less reliable .
Are built from the ClueWeb and Wikipedia corpora by .
Training Details and ( Hyper ) Parameters
We explain training details of our approach .
All models are implemented in the Torch framework .
Entity Vectors Training & Relatedness Evaluation .
For entity embeddings only , we use Wikipedia ( Feb 2014 ) corpus for training .
Entity vectors are initialized randomly from a 0 mean normal distribution with standard deviation 1 .
We first train each entity vector on the entity 's Wikipedia canonical description page ( title words included ) for 400 iterations .
Subsequently , Wikipedia hyperlinks of the respective entities are used for learning until validation score ( described below ) stops improving .
In each iteration , 20 positive words , each with 5 negative words , are sampled and used for optimization as explained in Section 3 .
We use Adagrad with a learning rate of 0.3 .
We choose embedding size d = 300 , pre-trained ( fixed ) Word2 Vec word vectors 8 , ? = 0.6 , ? = 0.1 and window size of 20 for the hyperlinks .
We remove stop words before training .
Since our method allows to train the embedding of each entity independently of other entities , we decide for efficiency reasons ( and without loss of generality ) to learn only the vectors of all entities appearing as mention candidates in all the test datasets described in Sec. 7.1 , a total of 270000 entities .
Training of those takes 20 hours on a single TitanX GPU with 12 GB of memory .
We test and validate our entity embeddings on the entity relatedness dataset of .
It contains 3319 and 3673 queries for the test and validation sets .
Each query consist of one target entity and up to 100 candidate entities with gold standard binary labels indicating if the two entities are related .
The associated task requires ranking of related candidate entities higher than the others .
Following previous work , we use different evaluation metrics : normalized discounted cumulative gain ( NDCG ) and mean average precision ( MAP ) .
The validation score used during learning is then the sum of the four metrics showed in .
We perform candidate ranking based on cosine similarity of entity pairs .
Local and Global Model Training .
Our local and global ED models are trained on AIDA - train ( multiple epochs ) , validated on AIDA - A and tested on AIDA - B and other datasets mentioned in Section 7.1 .
We use Adam with learning rate of 1e - 4 until validation accuracy exceeds 90 % , afterwards setting it to 1e - 5 .
Variable size mini-batches consisting of all mentions in a document are used during training .
We remove stop words .
Hyper- parameters of the best validated global model are : ? = 0.01 , K = 100 , R = 25 , S = 7 , ? = 0.5 , T = 10 .
For the local model , R = 50 was best .
Validation accuracy is computed after each 5 epochs .
To regularize , we use early stopping , i.e. we stop learning if the validation accuracy does not increase after 500 epochs .
Training on a single GPU takes , on average , 2 ms per mention , or 16 hours for 1250 epochs over AIDA - train .
By using diagonal matrices A , B , C , we keep the number of parameters very low ( approx. 1.2 K parameters ) .
This is necessary to avoid overfitting when learning from a very small training set .
We also experimented with diagonal plus low - rank matrices , but encountered quality degradation .
Entity Similarity Results
Results for the entity similarity task are shown in .
Our method outperforms the well established Wikipedia link measure and the method of using less information ( only word - entity statistics ) .
We note that the best result on this dataset was reported in the unpublished work of .
Their entity embeddings are trained on many more sources of information ( e.g. KG links , relations , entity types ) .
However , our focus was to prove that lightweight trained embeddings useful for the ED task can also perform decently for the entity sim - : ED accuracy on AIDA - B for our best system splitted by Wikipedia hyperlink frequency and mention prior of the ground truth entity , in cases where the gold entity appears in the candidate set .
ilarity task .
We emphasize that our global ED model outperforms Huang 's ED model , likely due to the power of our local and joint neural network architectures .
For example , our attention mechanism clearly benefits from explicitly embedding words and entities in the same space .
ED Baselines & Results
We compare with systems that report state - of - theart results on the datasets .
Some baseline scores from are taken from .
The best results for the AIDA datasets are reported by and .
We do not compare against since , as noted also by , their mention index artificially includes the gold entity ( guaranteed gold recall ) , which is not a realistic setting .
For a fair comparison with prior work , we use in - KB accuracy and micro F1 ( averaged per mention ) metrics to evaluate our approach .
Results are shown in .
We run our system 5 times , each time we pick the best model on the validation set , and report results on the test set for these models .
We obtain state of the art accuracy on AIDA which is the largest and hardest ( by the accuracy of thep ( e |m ) baseline ) manually created ED dataset .
We are also competitive on the other datasets .
It should be noted that all the other methods use , at least partially , engineered features .
The merit of our proposed method is to show that , with the exception of thep ( e |m ) feature , a neural network is able to learn the best features for ED without requiring expert input .
To gain further insight , we analyzed the accuracy on the AIDA - B dataset for situations where gold entities have low frequency or mention prior .
shows that our method performs well in these harder cases . :
Examples of context words selected by our local attention mechanism .
Distinct words are sorted decreasingly by attention weights and only words with non-zero weights are shown .
Hyperparameter Studies
In , we analyze the effect of two hyperparameters .
First , we see that hard attention ( i.e. R < K ) helps reducing the noise from uninformative context words ( as opposed to keeping all words when R = K ) .
Second , we see that a small number of LBP iterations ( hard - coded in our network ) is enough to obtain good accuracy .
This speeds up training and testing compared to traditional methods that run LBP until convergence .
An explanation is that a truncated version of LBP can perform well enough if used at both training and test time .
Qualitative Analysis of Local Model
In we show some examples of context words attended by our local model for correctly solved hard cases ( where the mention prior of the correct entity is low ) .
One can notice that words relevant for at least one entity candidate are chosen by our model inmost of the cases .
Error Analysis
We analyse some of the errors made by our model on the AIDA - B dataset .
We mostly observe three situations : i ) annotation errors , i i ) gold entities that do not appear in mentions ' candidate sets , or iii ) gold entities with very low p ( e |m ) prior whose mentions have an incorrect entity candidate with high prior .
For example , the mention " Italians " refers in some specific context to the entity " Italy national football team " rather than the entity representing the country .
The contextual information is not strong enough in this case to avoid an incorrect prediction .
On the other hand , there are situations where the context can be misleading , e.g. a document heavily discussing about cricket will favor resolving the mention " Australia " to the entity " Australia national cricket team " instead of the gold entity " Australia " ( naming a location of cricket games in the given context ) .
Conclusion
We have proposed a novel deep learning architecture for entity disambiguation that combines entity embeddings , a contextual attention mechanism , an adaptive local score combination , as well as unrolled differentiable message passing for global inference .
Compared to many other methods , we do not rely on hand - engineered features , nor on an extensive corpus for entity co-occurrences or relatedness .
Our system is fully differentiable , although we chose to pre-train word and entity embeddings .
Extensive experiments show the competitiveness of our approach across a wide range of corpora .
In the future , we would like to extend this system to perform nil detection , coreference resolution and mention detection .
Our code and data are publicly available : http://github.com/dalab/deep-ed
