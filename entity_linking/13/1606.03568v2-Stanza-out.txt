title
Word Sense Disambiguation using a Bidirectional LSTM
abstract
In this paper we present a clean , yet effective , model for word sense disambiguation .
Our approach leverage a bidirectional long short - term memory network which is shared between all words .
This enables the model to share statistical strength and to scale well with vocabulary size .
The model is trained end - to - end , directly from the raw text to sense labels , and makes effective use of word order .
We evaluate our approach on two standard datasets , using identical hyperparameter settings , which are in turn tuned on a third set of held out data .
We employ no external resources ( e.g. knowledge graphs , part - of - speech tagging , etc ) , language specific features , or hand crafted rules , but still achieve statistically equivalent results to the best state - of - the - art systems , that employ no such limitations .
*
Introduction
Words are in general ambiguous and can have several related or unrelated meanings depending on context .
For instance , the word rock can refer to both a stone and a music genre , but in the sentence " Without the guitar , there would be no rock music " the sense of rock is no longer ambiguous .
The task of assigning a word token in a text , e.g. rock , to a well defined word sense in a lexicon is called word sense disambiguation ( WSD ) .
From the rock example above it is easy to see that the context surrounding the word is what disambiguates the sense .
However , it may not be so obvious that this is a difficult task .
To see this , consider instead the phrase " Solid rock " where changing the order of words completely changes the meaning , or " Hard rock crushes heavy metal " where individual words seem to indicate stone but together they actually define the word token as music .
With this in mind , our thesis is that to do WSD well we need to go beyond bag of words and into the territory of sequence modeling .
Improved WSD would be beneficial to many natural language processing ( NLP ) problems , e.g. machine translation , information Retrieval , information Extraction , and sense aware word representations .
However , though much progress has been made in the area , many current WSD systems suffer from one or two of the following deficits .
( 1 ) Disregarding the order of words in the context which can lead to problems as described above .
( 2 ) Relying on complicated and potentially language specific hand crafted features and resources , which is a big problem particularly for resource poor languages .
We aim to mitigate these problems by ( 1 ) modeling the sequence of words surrounding the target word , and ( 2 ) refrain from using any hand crafted features or external resources and instead represent the words using real valued vector representation , i.e. word embeddings .
Using word embeddings has previously been shown to improve WSD .
However , these works did not consider the order of words or their operational effect on each other .
The main contributions of this work include :
A purely learned approach to WSD that achieves results on par with state - of - the - art resource heavy systems , employing e.g. knowledge graphs , parsers , part - of - speech tagging , etc .
Parameter sharing between different word types to make more efficient use of labeled data and make full vocabulary scaling plausible without the number of parameters exploding .
Empirical evidence that highlights the importance of word order for WSD .
A WSD system that , by using no explicit window , is allowed to combine local and global information when deducing the sense .
Background
In this section we introduce the most important underlying techniques for our proposed model .
Bidirectional LSTM
Long short - term memory ( LSTM ) is a gated type of recurrent neural network ( RNN ) .
LSTMs were introduced by to enable RNNs to better capture long term dependencies when used to model sequences .
This is achieved by letting the model copy the state between timesteps without forcing the state through a non-linearity .
The flow of information is instead regulated using multiplicative gates which preserves the gradient better than e.g. the logistic function .
The bidirectional variant of LSTM , ( BLSTM ) is an adaptation of the LSTM where the state at each time step consist of the state of two LSTMs , one going left and one going right .
For WSD this means that the state has information about both preceding words and succeeding words , which in many cases are absolutely necessary to correctly classify the sense .
Word embeddings by GloVe
Word embeddings is away to represent words as real valued vectors in a semantically meaningful space .
Global Vectors for Word Representation ( GloVe ) , introduced by Pennington et al. is a hybrid approach to embedding words that combine a log - linear model , made popular by , with counting based co-occurrence statistics to more efficiently capture global statistics .
Word embeddings are trained in an unsupervised fashion , typically on large amounts of data , and is able to capture fine grained semantic and syntactic information about words .
These vectors can subsequently be used to initialize the input layer of a neural network or some other NLP model .
The Model
Given a document and the position of the target word , i.e. the word to disambiguate , the model computes a probability distribution over the possible senses corresponding to that word .
The architecture of the model , depicted in , consist of a softmax layer , a hidden layer , and a BLSTM .
See Section 2.1 for more details regarding the BLSTM .
The BLSTM and the hidden layer share parameters overall word types and senses , while the softmax is parameterized byword type and selects the corresponding weight matrix and bias vector for each word type respectively .
This structure enables the model to share statistical strength across different word types while remaining computationally efficient even fora large total number of senses and realistic vocabulary sizes .
Model definition
The input to the BLSTM at position n in document Dis computed as
Here , v ( w n ) is the one - hot representation of the word type corresponding tow n ?
D. A one - hot representation is a vector with dimension V consisting of | V | ? 1 zeros and a single one which index indicate the word type .
This will have the effect of picking the column from W x corresponding to that word type .
The resulting vector is referred to as a word embedding .
Further , W x can be initialized using pre-trained word embeddings , to leverage large unannotated datasets .
In this work Glo Ve vectors are used for this purpose , see Section 4.1 for details .
The model output ,
is the predicted distribution over senses for the word at position n , where W ay wn and bay wn are the weights and biases for the softmax layer corresponding to the word type at position n .
Hence , each word type will have its own softmax parameters , with dimensions depending on the number of senses of that particular word .
Further , the hidden layer a is computed as
] is the concatenated outputs of the right and left traversing LSTMs of the BLSTM at word n.
W ha and b ha are the weights and biases for the hidden layer .
Loss function
The parameters of the model , ? = { W x , ?
BLST M , W ha , b ha , { W ay w , bay w } ?w? V , } , are fitted by minimizing the cross entropy error
over a set of sense labeled tokens with indices I ?
{ 1 , . . . , | C |} within a training corpus C , each labeled with a target sense ti , ?i ?
I.
Dropword
Dropword is a regularization technique very similar to word dropout introduced by .
Both methods are word level generalizations of dropout but in word dropout the word is set to zero while in dropword it is replaced with a < dropped > tag .
The tag is subsequently treated just like any other word in the vocabulary and has a corresponding word embedding that is trained .
This process is repeated overtime , so that the words dropped changeover time .
The motivation for doing dropword is to decrease the dependency on individual words in the training context .
This technique can be generalized to other kinds of sequential inputs , not only words .
To evaluate our proposed model we perform the lexical sample task of SensEval 2 ( SE2 ) and SensEval 3 ( SE3 ) , part of the SensEval workshops organized by Special Interest Group on the Lexicon at ACL .
For both instances of the task training and test data are supplied , and the task consist of disambiguating one indicated word in a context .
The words to disambiguate are sampled from the vocabulary to give a range of low , medium and high frequency words , and a gold standard sense label is supplied for training and evaluation .
Experimental settings
The hyperparameter settings used during the experiments , presented in , were tuned on a separate validation set with data picked from the SE2 training set .
The source code , implemented using TensorFlow , has been released as open source 1 .
Hyperparameter
Range : Hyperparameter settings used for both experiments and the ranges that were searched during tuning . " - " indicates that no tuning were performed on that parameter .
Embeddings
The embeddings are initialized using a set of freely available 2 Glo Ve vectors trained on Wikipedia and Gigaword .
Words not included in this set are initialized from N ( 0 , 0.1 ) .
To keep the input noise proportional to the embeddings it is scaled by ?
i which is the standard deviation in embedding dimension i for all words in the embeddings matrix , W x . ?
i is updated after each weight update .
Data preprocessing
The only preprocessing of the data that is conducted is replacing numbers with a < number > tag .
This result in a vocabulary size of | V | = 50817 for SE2 and | V | = 37998 for SE3 .
Words not present in the training set are considered unknown during test .
Further , we limit the size of the context to max 140 words centered around the target word to facilitate faster training .
Results
The results of our experiments and the state - of - the - art are shown in .
100 JHU ( R ) was developed by and achieved the best score on the English lexical sample task of SE2 with a F 1 score of 64.2 .
Their system utilized a rich feature space based on raw words , lemmas , POS tags , bag - of - words , bi-gram , and tri-gram collocations , etc. as inputs to an ensemble classifier .
htsa 3 by was the winner of the SE3 lexical sample task with a F 1 score of 72.9 .
This system was based mainly on raw words , lemmas , and POS tags .
These were used as inputs to a regularized least square classifier .
IMS + adapted CW is a more recent system , by , that uses separately trained word embeddings as input .
However , it also relies on a rich set of other features including POS tags , collocations and surrounding words to achieve their reported result .
Our proposed model achieves the top score on SE2 and are tied with IMS + adapted CW on SE3 .
Moreover , we see that dropword consistently improves the results on both SE2 and SE3 .
Randomizing the order of the input words yields a substantially worse result , which provides evidence for our hypothesis that the order of the words are significant .
We also see that the system effectively makes use of the information in the pre-trained word embeddings and that they are essential to the performance of our system on these datasets .
F1 score
Conclusions & future work
We presented a BLSTM based model for WSD that was able to effectively exploit word order and achieve results on state - of - the - art level , using no external resources or handcrafted features .
As a consequence , the model is largely language independent and applicable to resource poor languages .
Further , the system was designed to generalize to full vocabulary WSD by sharing most of the parameters between words .
For future work we would like to provide more empirical evidence for language independence by evaluating on several different languages , and do experiments on large vocabulary all words WSD , where every word in a sentence is disambiguated .
Further , we plan to experiment with unsupervised pre-training of the BLSTM , encouraged by the substantial improvement achieved by incorporating word embeddings .
