title
LEARNING CROSS - CONTEXT ENTITY REPRESENTA - TIONS FROM TEXT
Work done as a Google AI Resident
abstract
Language modeling tasks , in which words , or word - pieces , are predicted on the basis of a local context , have been very effective for learning word embeddings and context dependent representations of phrases .
Motivated by the observation that efforts to code world knowledge into machine readable knowledge bases or human readable encyclopedias tend to be entity - centric , we investigate the use of a fill - in - the - blank task to learn context independent representations of entities from the text contexts in which those entities were mentioned .
We show that large scale training of neural models allows us to learn high quality entity representations , and we demonstrate successful results on four domains : ( 1 ) existing entity - level typing benchmarks , including a 64 % error reduction over previous work on TypeNet ; ( 2 ) a novel few - shot category reconstruction task ; ( 3 ) existing entity linking benchmarks , where we match the state - of - the - art on CoNLL - Aida without linking - specific features and obtain a score of 89.8 % on TAC - KBP 2010 without using any alias table , external knowledge base or in domain training data and ( 4 ) answering trivia questions , which uniquely identify entities .
Our global entity representations encode fine - grained type categories , such as Scottish footballers , and can answer trivia questions such as Who was the last inmate of Spandau jail in Berlin ?
INTRODUCTION
A long term goal of artificial intelligence has been the development and population of an entitycentric representation of human knowledge .
Efforts have been made to create the knowledge representation with knowledge engineers or crowdsourcers .
However , these methods have relied heavily on human definitions of their ontologies , which are both limited in scope and brittle in nature .
Conversely , due to recent advances in deep learning , we can now learn robust general purpose representations of words and contextualized phrases directly from large textual corpora .
In particular , we observe that existing methods of building contextualized phrase representations capture a significant amount of local semantic context .
We hypothesize that by learning an entity encoder which aggregates all of the textual contexts in which an entity is seen , we should be able to extract and condense general purpose knowledge about that entity .
Consider the following contexts in which an entity mention has been replaced a [ MASK ] :
. . . the second woman in space , 19 years after .
. . . , a Russian factory worker , was the first woman in space . . . . . . , the first woman in space , entered politics . . . .
As readers , we understand that first woman in space is a unique identifier , and we are able to fill in the blank unambiguously .
The central hypothesis of this paper is that , by matching entities to the contexts in which they are mentioned , we should be able to build a representation for Valentina Tereshkova that encodes the fact that she was the first woman in space , that she was a politician , etc. and that we should be able to use these representations across a wide variety of downstream entity - centric tasks .
We present RELIC ( Representations of Entities Learned in Context ) , a table of independent entity embeddings that have been trained to match fixed length vector representations of the textual context in which those entities have been seen .
We apply RELIC to entity typing ( mapping each entity to its properties in an external , curated , ontology ) ; entity linking ( identifying which entity is referred to by a textual context ) , and trivia question answering ( retrieving the entity that best answers a question ) .
Through these experiments , we show that :
RELIC accurately captures categorical information encoded by human experts in the Freebase and Wikipedia category hierarchies .
We demonstrate significant improvements over previous work on established benchmarks , including a 64 % error reduction in the TypeNet low data setting .
We also show that given just a few exemplar entities of a given category such as Scottish footballers we can use RELIC to recover the remaining entities of that category with good precision .
Using RELIC for entity linking can match state - of - the - art approaches that make use of non-local and non-linguistic information about entities .
On the CoNLL - Aida benchmark , RELIC achieves a 94.9 % accuracy , matching the state - of - the - art of , despite not using any entity linking - specific features .
On the TAC - KBP 2010 benchmark RELIC achieves 89.8 % accuracy , just behind the top ranked system , which makes use of external knowledge bases , alias tables , and taskspecific hand - engineered features .
RELIC learns better representations of entity properties if it is trained to match just the contexts in which entities are mentioned , and not the surface form of the mention itself .
For entity linking , the opposite is true .
We can treat the RELIC embedding matrix as a store of knowledge , and retrieve answers to questions through nearest neighbor search .
We show that this approach correctly answers 51 % of the questions in the TriviaQA reading comprehension task despite not using the task 's evidence text at inference time .
The questions answered correctly by RELIC are surprisingly complex , such as Who was the last inmate of Spandau jail in Berlin ?
RELATED WORK
Entity linking
The most widely studied entity - level task is entity linking - mapping each entity mention onto a unique entity identifier .
The Wikification task , in particular , is similar to the work presented in this paper , as it requires systems to map mentions to the Wikipedia pages describing the entities mentioned .
There is significant previous work that makes use of neural context and entity encoders in downstream entity linking systems , but that previous work focuses solely on discriminating between entities that match a given mention according to an external alias table .
Here we go further in investigating the degree to which RELIC can capture world knowledge about entities .
Mention - level entity typing
Another well studied task is mention - level entity typing ( e.g. .
In this task , entities are labeled with types that are supported by the immediate textual context .
For example , given the sentence ' Michelle Obama attended her book signing ' , Michelle Obama should be assigned the type author but not lawyer .
Subsequently , mention - level entity typing systems make use of contextualized representations of the entity mention , rather than the global entity representations that we focus on here .
Entity - level typing
An alternative notion of entity typing is entity - level typing , where each entity should be associated with all of the types supported by a corpus .
and introduce entity - level typing tasks , which we describe more in Section 5.2 .
Entity - level typing is an important task in information extraction , since most common ontologies make use of entity type systems .
Such tasks provide a strong method of evaluating learned global representations of entities .
Using knowledge bases
There has been a strong line of work in learning representations of entities by building knowledge base embeddings , and by jointly embedding knowledge bases and information from textual mentions .
extended this work to the SPADES fill - in - the - blank task , which is a close counterpart to RELIC 's training setup .
However , we note that all examples in SPADES correspond to a fully connected sub - graph in Freebase .
Subsequently , the contents are very limited in domain and show that it is essential to use the contents of Freebase to do well on this task .
We consider the unconstrained Trivia QA task , introduced in Section 5.5 , to be a better evaluation for open domain knowledge representations .
Fill - in - the - blank tasks
There has been significant previous work in using fill - in - the - blank losses to learn context independent word representations , and context - dependent word and phrase representations .
Cloze - style tasks , in which a system must choose which of a few entities best fill a blanked out span , have also been proposed as a method of evaluating reading comprehension .
For entities , Long et al .
consider a similar fill - in - the - blank task as ours , which they frame as rare entity prediction .
and train entity representations using a fill - in - the - blank style loss and a bag - of - words representation of mention contexts .
in particular take an approach that is very similar in motivation to RELIC , but which focuses on learning entity representations for use as features in downstream classifiers that model non-linear interactions between a small number of candidate entities .
In Section 5.4 , we show that Yamada et al .
Our training data is a corpus of ( context , entity ) pairs
identifies an entity that corresponds to the single entity mention in x i .
We train RELIC to correctly match the entities in D to their mentions .
We will experiment with settings where the mentions are unchanged from the original corpus , as well as settings wherewith some probability m ( the mask rate ) all of the words in the mention have been replaced with the uninformative [ MASK ] symbol .
We hypothesize that this parameter will play a role in the effectiveness of learned representations in downstream tasks .
For clean training data , we extract our corpus from English Wikipedia 1 .
See Section 4 for details .
CONTEXT ENCODER
We embed each context in D into a fixed length vector using a Transformer text encoder , initialized with parameters from the BERT - base model released by .
All parameters are then trained further using the objective presented below in Section 3.4 .
We take the output of the Transformer corresponding to the initial [ CLS ] token in BERT 's sequence representation as our context encoding , and we linearly project this into Rd using a learned weight matrix W ?
R d768 to get a context embedding in the same space as our entity embeddings .
ENTITY EMBEDDINGS
Each entity e ?
E has a unique and abstract Wikidata QID 2 .
RELIC maps these unique IDs directly onto a dedicated vector in Rd via a | E | d dimensional embedding matrix .
In our experiments , we have a distinct embedding for every concept that has an English Wikipedia page , resulting in 5 m entity embeddings overall .
RELIC TRAINING LOSS
RELIC optimizes the parameters of the context encoder and entity embedding table to maximize the compatibility between observed ( context , entity ) pairs .
Let g ( x ) ?
Rd be a context encoder , and let f ( e ) ?
Rd bean embedding function that maps each entity to its d dimensional representation via a lookup operation .
We define a compatibility score between the entity e and the context x as the scaled cosine similarity 3
where the scaling factor a is a learned parameter , following .
Now , given a context x , the conditional probability that e was the entity seen with x is defined as
and we train RELIC by maximizing the average log probability 1 | D |
In practice , the definition of probability in Equation 2 is prohibitively expensive for large | E| ( we use | E | ? 5 M ) .
Therefore , we use a noise contrastive loss .
We sample K negative entities from a noise distribution p noise ( e ) : e 1 , e 2 , . . . , e K ? p noise ( e ) ( 4 ) Denoting e 0 := e , we then compute our per-example loss using cross entropy :
In practice , we train our model with minibatch gradient descent and use all other entries in the batch as negatives .
That is , in a batch of size 4 , entities for rows 1 , 2 , 3 will be used as negatives for row 0 .
This is roughly equivalent top noise ( e ) being proportional to entity frequency .
EXPERIMENTAL SETUP
To train RELIC , we obtain data from the 2018 - 10 - 22 dump of English Wikipedia .
We take E to be the set of all entities in Wikipedia ( of which there are over 5 million ) .
For each occurrence of a hyperlink , we take the context as the surrounding sentence , replace all tokens in the anchor text with a single [ MASK ] symbol with probability m ( see Section 5.3 fora discussion of different masking rates ) and set the ground truth to be the linked entity .
We limit each context sentence to 128 tokens .
In this way , we collect a high - quality corpus of over 112M ( context , entity ) pairs .
Note in particular that an entity never co-occurs with text on its own Wikipedia page , since a page will not hyperlink to itself .
We set the entity embedding size to d = 300 .
We train the model using Tensor Flow
EVALUATION
We evaluate RELIC 's ability to : ( 1 ) solve the entity linking task without access to any task specific alias tables or features ;
( 2 ) accurately capture entity properties that have been hand - coded into TypeNet and Wikipedia categories ;
( 3 ) capture trivia knowledge specific to individual entities .
First we present results on established entity linking and entity typing tasks , to compare RELIC 's performance to established baselines and we show that the choice of masking strategy ( Section 3 ) has a significant and opposite impact on performance on these tasks .
We hypothesize that RELIC is approaching an upper bound on established entity - level typing tasks , and we introduce a much harder category completion task that uses RELIC to populate complex Wikipedia categories .
We also apply RELIC 's context encoder and entity embeddings to the task of end - to - end trivia question answering , and we show that this approach can capture more than half of the answers identified by the best existing reading comprehension systems .
ENTITY LINKING
RELIC can be used to directly solve the entity linking problem .
We just need to find the single entity that maximizes the cosine similarity in Equation 1 fora given context .
For the entity linking task , we create a context from the document 's first 64 tokens as well as the 64 tokens around the mention to be linked .
This choice of context is well suited to the documents in the CoNLL - Aida and TAC - KBP 2010 datasets , since those documents tend to be news articles in which the introduction is particularly information dense .
In we show performance for RELIC in two settings .
First , we report the accuracy for the pure RELIC model with no in - domain tuning .
Then , we report the accuracy fora RELIC model that has been tuned on the CoNLL - Aida training set .
On the CoNLL - Aida benchmark , we also adopt a standard alias table for this tuned model , as is commonly done in previous entity linking work .
It is clear that for the CoNLL - Aida benchmark in - domain tuning is essential .
We hypothesize that this is because of the dataset 's bias towards certain types of news content that is very unlike our Wikipedia pre-training data - specifically sports reports .
However , when we do adopt the standard CoNLL - Aida training set and alias table , RELIC matches the state of the art on this benchmark , despite using far fewer hand engineered resources use the large Wikidata knowledge base to create entity representations ) .
We do not make use of the TAC - KBP 2010 training set or alias table , and we observe that RELIC is already competitive without these enhancements 5
It is significant that RELIC matches the performance of , which uses the large hand engineered Wikidata knowledge base to represent entities .
This supports our central hypothesis that it is possible to capture the knowledge that has previously been manually encoded 4 Our finetuned CoNLL result uses the alias table of at inference time .
We do reduce the candidate set from the 5 m entities covered by RELIC to the 818 k entities in the TAC - KBP 2010 knowledge base to avoid ontological misalignment .
System
Type Net Type Net - Low Data ( 5 % ) 78.6 58.8 RELIC 90.1 85.3 : Mean Average Precision on Type Net tasks .
RELIC 's gains are particularly striking in the low data setting from .
in knowledge bases , using entity embeddings learned from textual contexts alone .
In the rest of this section , we will show further support for our hypothesis by recreating parts of the Freebase and Wikipedia ontologies , and by using RELIC to answer trivia questions .
Finally , we believe that RELIC 's entity linking performance could be boosted even higher through the adoption of commonly used entity linking features .
As shown in , use a small set of well chosen discrete features to increase the performance of their embedding based approach by 10 points .
These features could be simply integrated into RELIC 's model , but we consider them to be orthogonal to this paper 's investigation of purely learned representations .
F1
P@1 Acc 82.3 91.0 56.5 RELIC 87.9 94.8 68.3 RELIC with 5 % of FIGMENT training data 83.3 90.9
59.3 : Performance on FIGMENT .
We report P@1 ( proportion of entities whose top ranked types are correct ) , Micro F1 aggregated overall ( entity , type ) compatibility decisions , and overall accuracy of entity labeling decisions .
RELIC outperforms prior work , even with only 5 % of the training data .
ENTITY - LEVEL FINE TYPING
We evaluate RELIC 's ability to capture entity properties on the FIGMENT and TypeNet entity - level fine typing tasks which contain 102 and 1,077 types drawn from the Freebase ontology .
The task in both datasets is to predict the set of fine - grained types that apply to a given entity .
We train a simple 2 - layer feedforward network that takes as input RELIC 's embedding f ( e ) of the entity e and outputs a binary vector indicating which types apply to that entity .
show that RELIC significantly outperforms prior results on both datasets .
For FIGMENT , is an ensemble of several standard representation learning techniques : word2 vec skip - gram contexts , structured skip - gram contexts , and FastText representations of the entity names .
For TypeNet , aggregate mention - level types and train with a structured loss based on the TypeNet hierarchy , but is still outperformed by our flat classifier of binary labels .
We expect that including a hierarchical loss is orthogonal to our approach and could improve our results further .
The most striking results in
EFFECT OF MASKING
In Section 3 we introduced the concept of masking entity mentions , and predicting on the basis of the context in which they are discussed , not the manner in which they are named .
show the effect of training RELIC with different mask rates .
It is clear that masking mentions during training is beneficial for entity typing tasks , but detrimental for entity linking .
This is in accordance with our intuitions .
Modeling mention surface forms is essential for linking , since these mentions are given attest time and names are extremely discriminative .
However , once the mention is known the model only needs to distinguish between different entities with the same name ( e.g. President Washington , University of Washington , Washington State ) and this distinction rarely requires deep knowledge of each entity 's properties .
Subsequently , our best typing models are those that are forced : TypeNet entity - level typing m AP on the development set for RELIC models trained with different masking rates .
A higher mask rate leads to better performance , both in low and high - data situations .
to capture more of the context in which each entity is mentioned , because they are not allowed to rely on the mention itself .
The divergence between the trends in suggests that there may not be one set of entity embeddings that are optimum for all tasks .
However , we would like to point out that that a mask rate of 10 % , RELIC nears optimum performance on most tasks .
The optimum mask rate is an open research question , that will likely depend on entity frequency as well as other data statistics .
FEW - SHOT CATEGORY COMPLETION
The entity - level typing tasks discussed above involve an in - domain training step .
Furthermore , due to the incompleteness of the the FIGMENT and TypeNet type systems , we also believe that RELIC 's performance is approaching the upper bound on both of these supervised tasks .
Therefore , to properly measure RELIC 's ability to capture complex types from fill - in - the - blank training alone , we propose :
1 . anew category completion task that does not involve any task specific optimization , 2 . anew Wikipedia category based evaluation set that contains much more complex compound types , such as Scottish footballers , We use this new task to compare RELIC to the embeddings learned by .
In the new category completion task , we represent each category by randomly sampling three exemplar entities , and calculating the centroid of their RELIC embeddings .
We then rank all other entities according to their dot -product with this centroid , and report the mean average precision ( MAP ) of the resultant ranking .
First , we apply this evaluation to the TypeNet type system introduced in .
These types are well - curated , but tend to represent high - level categories .
To measure the degree to which our entity embeddings capture finer grained type information , we construct an aditional dataset based on Wikipedia categories 6 .
These tend to be compound types , such as Actresses from London , which capture many aspects of an entity - in this case gender , profession , and place of birth .
From 45.0 -: Answer exact match on Trivia QA .
RELIC 's fast nearest neighbor search over entities achieves 80 % of the performance of ORQA , which runs a BERT - based reading comprehesion model over multiple retrieved evidence passages .
Unlike ORQA and RELIC , the classifier baseline and SLQA have access to a single evidence document that is known to contain the answer .
As a result they are solving a much easier task .
beddings capture entities which are much closer to the exemplars .
In fact , we identify several false negatives in these examples .
TRIVIA QUESTION ANSWERING
Our final experiment tests RELIC 's ability to answer trivia questions - which can be considered high precision categories that only apply to a single entity - using retrieval of encoded entities .
Trivia QA ) is a question - answering dataset containing questions sourced from trivia websites , and the answers are usually entities with Wikipedia pages .
The standard Trivia QA setup is a reading comprehension task , where answers are extracted from evidence documents .
Here , we answer questions in Trivia QA without access to any evidence documents attest time .
Model and training Given a question , we apply the context encoder g from Section 3.4 , and retrieve 1 out of 5 M entities using cosine similarity .
For training , we initialize both g and f from RELIC training .
We tune only g's parameters by optimizing the loss in Equation 5 applied to ( question , answer entity ) pairs , rather than the ( context , entity ) pairs seen during RELIC 's training .
Results
Trivia
QA results are shown in , and randomly sampled RELIC predictions are illustrated in .
All systems other than RELIC in have access to evidence text at inference time .
In the open domain unfiltered setting , ORQA retrieves this text from a cache of Wikipedia .
In the more standard verified - web reading comprehension task , the classifier baseline and SLQA are provided with a single document that is known to contain the answer .
We consider ORQA to be the most relevant point of comparison for RELIC .
We observe that the retrieve - then - read approach taken by ORQA outperforms the direct answer retrieval approach taken by RELIC .
However , ORQA runs a BERT based reading comprehension model over multiple evidence passages at inference time and we are encouraged to see that RELIC 's much faster nearest neighbor lookup captures 80 % of ORQA 's performance .
It is also significant that RELIC outperforms 's reading comprehension baseline by 20 points , despite the fact that the baseline has access to a single document that is known to and TypeNet , even when only training on a small fraction of the task - specific training data .
We then introduce a novel few - shot category reconstruction task and when comparing to , we found that RELIC is better able to capture complex compound types .
Our method also proves successful for entity linking , where we match the state of the art on CoNLL - Aida despite not using linkingspecific features and fare similarly to the best system on TAC - KBP 2010 despite not using an alias table , any external knowledge bases , linking - specific features or even in - domain training data .
Finally , we show that our RELIC embeddings can be used to answer trivia questions directly , without access to any evidence documents .
We encourage researchers to further explore the properties of our entity representations and BERT context encoder , which we will release publicly .
