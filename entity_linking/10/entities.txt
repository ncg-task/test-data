103	0	18	Question answering
107	6	12	adding
107	13	17	ELMo
107	18	20	to
107	25	39	baseline model
107	42	54	test set F 1
107	55	66	improved by
107	67	72	4.7 %
107	73	77	from
107	78	84	81.1 %
107	85	87	to
107	88	94	85.8 %
107	99	130	24.9 % relative error reduction
107	131	135	over
107	140	148	baseline
107	155	164	improving
107	169	212	overall single model state - of - the - art
107	213	215	by
107	216	221	1.4 %
109	4	12	increase
109	13	15	of
109	16	21	4.7 %
109	22	26	with
109	27	31	ELMo
109	40	60	significantly larger
109	61	65	then
109	70	87	1.8 % improvement
109	88	99	from adding
109	100	104	CoVe
109	105	107	to
109	110	124	baseline model
112	0	18	Textual entailment
116	10	16	adding
116	17	21	ELMo
116	22	24	to
116	29	39	ESIM model
116	40	48	improves
116	49	57	accuracy
116	58	60	by
116	61	71	an average
116	72	74	of
116	75	80	0.7 %
116	81	87	across
116	88	105	five random seeds
118	0	22	Semantic role labeling
124	96	102	adding
124	103	107	ELMo
124	108	116	improved
124	121	132	average F 1
124	133	135	by
124	136	141	3.2 %
124	62	66	from
124	147	151	67.2
124	152	154	to
124	155	159	70.4
124	206	215	improving
124	216	220	over
124	225	254	previous best ensemble result
124	255	257	by
124	258	267	1.6 % F 1
125	0	23	Named entity extraction
128	14	44	our ELMo enhanced biLSTM - CRF
128	45	53	achieves
128	54	66	92. 22 % F 1
128	67	80	averaged over
128	81	90	five runs
12	19	28	introduce
12	31	39	new type
12	40	42	of
12	43	82	deep contextualized word representation
13	0	19	Our representations
13	20	31	differ from
13	32	64	traditional word type embeddings
13	73	83	each token
13	87	95	assigned
13	98	112	representation
13	123	134	function of
13	139	160	entire input sentence
16	70	90	ELMo representations
16	91	94	are
16	95	99	deep
16	131	139	function
16	140	142	of
16	154	169	internal layers
16	147	149	of
16	177	181	biLM
14	3	6	use
14	7	14	vectors
14	15	27	derived from
14	30	48	bidirectional LSTM
14	57	69	trained with
14	72	114	coupled lan - guage model ( LM ) objective
14	115	117	on
14	120	137	large text corpus
15	21	30	call them
15	31	87	ELMo ( Embeddings from Language Models ) representations
17	23	28	learn
17	31	49	linear combination
17	50	52	of
17	57	64	vectors
17	65	78	stacked above
17	79	94	each input word
17	95	98	for
17	99	112	each end task
17	121	138	markedly improves
17	139	150	performance
17	161	166	using
17	171	185	top LSTM layer
19	0	5	Using
19	6	27	intrinsic evaluations
19	33	37	show
19	47	73	higher - level LSTM states
19	74	81	capture
19	82	109	context - dependent aspects
19	110	112	of
19	113	125	word meaning
19	243	260	lowerlevel states
19	261	266	model
19	267	274	aspects
19	275	277	of
19	278	284	syntax
2	0	40	Deep contextualized word representations
4	27	66	deep contextualized word representation
9	0	32	Pre-trained word representations
