title
Learning Distributed Representations of Texts and Entities from Knowledge Base
abstract
We describe a neural network model that jointly learns distributed representations of texts and knowledge base (KB) entities. Given a text in the KB, we train our proposed model to predict entities that are relevant to the text. Our model is designed to be generic with the ability to address various NLP tasks with ease. We train the model using a large corpus of texts and their entity annotations extracted from Wikipedia. We evaluated the model on three important NLP tasks (i.e., sentence textual similarity, entity linking, and factoid question answering) involving both unsupervised and supervised settings. As a result, we achieved state-of-the-art results on all three of these tasks. Our code and trained models are publicly available for further academic research. 1
Our model Skip-gram Europe Eastern Europe (0.67) Western Europe (0.66) Central Europe (0.64) Asia (0.64) North America (0.64) Asia (0.85) Western Europe (0.78) North America (0.76) Central Europe (0.75) Americas (0.73) Golf Golf course (0.76) PGA Tour (0.74) LPGA (0.74) Professional golfer (0.73) U.S. Open (0.71) Tennis (0.74) LPGA (0.72) PGA Tour (0.69) Golf course (0.68) Nicklaus Design (0.66) Tea Coffee (0.82) Green tea (0.81) Black tea (0.80) Camellia sinensis (0.78) Spice (0.76) Coffee (0.78) Green tea (0.76) Black tea (0.75) Camellia sinensis (0.74) Spice (0.73) Smartphone Tablet computer (0.93) Mobile device (0.89) Personal digital assistant (0.88) Android (operating system) (0.86) iPhone (0.85) Tablet computer (0.91) Personal digital assistant (0.84) Mobile device (0.84) Android (operating system) (0.82) Feature phone (0.82) Scarlett Johansson Kirsten Dunst (0.85) Anne Hathaway (0.85) Cameron Diaz (0.85) Natalie Portman (0.85) Jessica Biel (0.84) Anne Hathaway (0.79) Natalie Portman (0.78) Kirsten Dunst (0.78) Cameron Diaz (0.78) Kate Beckinsale (0.77) The Lord of the Rings The Hobbit (0.85) J. R. R. Tolkien (0.84) The Silmarillion (0.81) The Fellowship of the Ring (0.80) The Lord of the Rings (film series) (0.78) The Hobbit (0.77) J. R. R. Tolkien (0.76) The Silmarillion (0.71) The Fellowship of the Ring (0.70) Elvish languages (0.69) Table 7: Examples of top five similar entities with their cosine similarities in our learned entity representations with those of the skip-gram model.
Introduction
Methods capable of learning distributed representations of arbitrary-length texts (i.e., fixed-length continuous vectors that encode the semantics of texts), such as sentences and paragraphs, have recently attracted considerable attention (Le and. These methods aim to learn generic representations that are useful across domains similar to word embedding methods such as Word2vec and GloVe.
Another interesting approach is learning distributed representations of entities in a knowledge 1 https://github.com/studio-ousia/ntee base (KB) such as Wikipedia and Freebase. These methods encode information of entities in the KB into a continuous vector space. They are shown to be effective for various KB-related tasks such as entity search, entity linking, and link prediction.
In this paper, we describe a novel method to bridge these two different approaches. In particular, we propose Neural Text-Entity Encoder (NTEE), a neural network model to jointly learn distributed representations of texts (i.e., sentences and paragraphs) and KB entities. For every text in the KB, our model aims to predict its relevant entities, and places the text and the relevant entities close to each other in a continuous vector space. We use humanedited entity annotations obtained from Wikipedia (see) as supervised data of relevant entities to the texts containing these annotations. Note that, KB entities have been conventionally used to model semantics of texts. A representative example is Explicit Semantic Analysis (ESA), which represents the semantics of a text using a sparse vector space, where each dimension corresponds to the relevance score of the text to each entity. Essentially, ESA shows that text can be accurately represented using a small set of its relevant entities. Based on 2 Entity annotations in Wikipedia can be viewed as supervised data of relevant entities because Wikipedia instructs its contributors to create annotations only where they are relevant in its manual: https://en.wikipedia.org/ wiki/Wikipedia:Manual_of_Style arXiv:1705.02494v3 [cs.CL] 7 Nov 2017 this fact, we hypothesize that we can use the annotations of relevant entities as the supervised data of learning text representations. Furthermore, we also consider that placing texts and entities into the same vector space enables us to easily compute the similarity between texts and entities, which can be beneficial for various KB-related tasks.
In order to test this hypothesis, we conduct three experiments involving both the unsupervised and the supervised tasks. First, we use standard semantic textual similarity datasets to evaluate the quality of the learned text representations of our method in an unsupervised fashion. As a result, our method clearly outperformed the state-of-the-art methods.
Furthermore, to test the effectiveness of our method to perform KB-related tasks, we address the following two important problems in the supervised setting: entity linking (EL) and factoid question answering (QA). In both tasks, we adopt a simple multi-layer perceptron (MLP) classifier with the learned representations as features. We tested our method using two standard datasets (i.e., CoNLL 2003 and TAC 2010) for the EL task and a popular factoid QA dataset based on the quiz bowl quiz game for the factoid QA task. As a result, our method outperformed recent state-of-the-art methods on both the EL and the factoid QA tasks.
Additionally, there have also been proposed methods that map words and entities into the same continuous vector space. Our work differs from these works because we aim to map texts (i.e., sentences and paragraphs) and entities into the same vector space.
Our contributions are summarized as follows:
• We propose a neural network model that jointly learns vector representations of texts and KB entities. We train the model using a large amount of entity annotations extracted directly from Wikipedia.
• We demonstrate that our proposed representations are surprisingly effective for various NLP tasks. In particular, we apply the proposed model to three different NLP tasks, namely semantic textual similarity, entity linking, and factoid question answering, and achieve stateof-the-art results on all three tasks.
The Lord of the Rings is an epic high-fantasy novel written by English author J. R. R. Tolkien.
Entity Annotations: The Lord of the Rings, Epic (genre), High fantasy, J. R. R. Tolkien: An example of a sentence with entity annotations.
• We release our code and trained models to the community at https://github.com/ studio-ousia/ntee to facilitate further academic research.

Our Approach
In this section, we propose our approach of learning distributed representations of texts and entities in KB.

Model
Given a text t (a sequence of words w 1 , ..., w N ), we train our model to predict entities e 1 , ..., en that appear int. Formally, the probability that represents the likelihood of an entity e appearing int is defined as the following softmax function:
where E KB is a set of all entities in KB, and v e ? Rd and v t ? Rd are the vector representations of the entity e and the text t, respectively. We compute v t using the element-wise sum of word vectors int with L 2 normalization and a fully connected layer. Let us denote v s as a vector of the sum of word vectors
where W ? R d×d is a weight matrix, and b ? Rd is a bias vector. Here, we initialize v wand v e using the pre-trained representations described in the next section.
The loss function of our model is defined as follows:
where ? denotes a set of pairs each of which consists of a text t and its entity annotations E tin KB. One problem in training our model is that the denominator in Eq. (1) is computationally very expensive because it involves summation overall entities in KB. We address this problem by replacing E KB in Eq. (1) with E * , which is the union of the positive entity e and the randomly chosen k negative entities that do not appear int. This method can be viewed as negative sampling with a uniform negative distribution.
In addition, because the length of a text t is arbitrary in our model, we test the following two settings: t as a paragraph, and t as a sentence 3 .

Parameters
The parameters to be learned by our model are the vector representations of words and entities in our vocabulary V , the weight matrix W , and the bias vector b. Consequently, the total number of parameters in our model is
We initialize the representations of words and entities using pre-trained representations to reduce the training time. We use the skip-gram model of Word2vec with negative sampling trained with Wikipedia articles. In order to create a corpus for the skip-gram model from Wikipedia, we simply replace the name of each entity annotation in Wikipedia articles with the unique identifier of the entity the annotation refers to. This simple method enables us to easily train the distributed representations of words and entities simultaneously. We used a Wikipedia dump generated in July 2016 4 . For the hyper-parameters of the skip-gram model, we used standard parameters such as the context window size being 10, and the size of negative samples being 5. We used the Python Word2vec implementation in Gensim 5 . Additionally, the entity representations were normalized to unit length before they were used as the pre-trained representations.

Corpus
We trained our model by using the English DBpedia abstract corpus, an open corpus of Wikipedia texts with entity annotations manually created by Wikipedia contributors. 6 It was extracted from the first introductory sections of 4.4 million Wikipedia articles. We train our model by iterating over the texts and their entity annotations in the corpus.
We used words that appear five times or more and entities that appear three times or more in the corpus, and simply ignored the other words and entities. As a result, our vocabulary V consisted of 705,168 words and 957,207 entities. Further, the number of valid words and entity annotations were approximately 382 million and 28 million, respectively.
Additionally, we also introduce one heuristic method to generate entity annotations. For each text, we add a pseudo-annotation that points to the entity of which the KB page is the source of the text. Because every KB page describes its corresponding entity, it typically contains many mentions referring to the entity. However, because hyper-linking to the web page itself does not make sense, these kinds of mentions cannot be observed as annotations in Wikipedia. Therefore, we use the aforementioned heuristic method to address this problem.

Other Details
Our model has several hyper-parameters. Following, the number of dimensions we used was d = 300. The mini-batch size was fixed at 100, the size of negative samples k was set to 30, and the training consisted of one epoch.
The model was implemented using Python and Theano. The training took approximately six days using a NVIDIA K80 GPU. We trained the model using stochastic gradient descent (SGD) and its learning rate was controlled by RMSprop.
In order to evaluate our model presented in the previous section, we conduct experiments on three important NLP tasks using the representations learned by our model. First, we conduct an experiment on a semantic textual similarity task in order to evaluate the quality of the learned text representations. Next, we conduct experiments on two important NLP problems (i.e., EL and factoid QA) in order to test the effectiveness of our proposed representations as features for downstream NLP tasks. Finally, we further qualitatively analyze the learned representations.
Note that we separately describe how we address each task using our representations in the subsection of each experiment.

Semantic Textual Similarity
Semantic textual similarity aims to test how well a model reflects human judgments of the semantic similarity between two sentence pairs. The task has been used as a standard method to evaluate the quality of distributed representations of sentences in past work.

Setup
We followed an existing method for our experimental setup. We used the public quiz bowl dataset proposed in. Following past work, we only used questions belonging to the history and literature categories, and only used answers that appeared at least six times. For questions referring to the same answer, we sampled 20% of each for the development set and test sets, and the remaining 60% for the training set. As a result, we obtained 1,535 training, 511 development, and 511 test questions for history, and 2,524 training, 840 development, and 840 test questions for literature. The number of possible answers was 303 and 424 in the history and literature categories, respectively.

Baselines
We use two types of baselines: two conventional bag-of-words (BOW) models and two state-of-theart neural network models. The details of these models are as follows:
• BOW is a conventional approach using a logistic regression (LR) classifier trained with binary BOW features to predict the correct answer.
• BOW-DT is based on the BOW baseline augmented with the feature set with dependency relation indicators.
• QANTA is an approach based on a recursive neural network to derive the distributed representations of questions. The method also uses the LR classifier with the derived representations as features.
• FTS-BRNN is based on the bidirectional recurrent neural network (RNN) with gated recurrent units (GRU). Similar to QANTA, the method adopts the LR classifier with the derived representations as features. shows the results of our methods compared with those of the baseline methods. The results of BOW, BOW-DT, and QANTA were obtained from. We also include the result reported in (denoted by QANTAfull), which used a significantly larger dataset than ours for training and testing.

Results
The experimental results show that our NTEE model achieved the best performance compared to the other proposed models and all the baseline methods on both the history and the literature datasets.: Accuracies of the proposed method and the state-of-the-art methods for the factoid QA task.
In particular, despite the simplicity of the neural network architecture of our method compared to the state-of-the-art methods (i.e., QANTA and FTS-BRNN), our method clearly outperformed these methods. This demonstrates the effectiveness of our proposed representations as background knowledge for the QA task. We also conducted a brief error analysis using the test set of the history dataset. Our observations indicated that our method mostly performed perfect in terms of predicting the types of target answers (e.g., locations, events, and people). However, our method erred in delicate cases such as predicting Henry II of England instead of Henry I of England, and Syracuse, Sicily instead of Sicily.

Entity Linking
Entity Linking (EL) is the task of resolving ambiguous mentions of entities to their referent entities in KB. EL has recently received considerable attention because of its effectiveness in various NLP tasks such as information extraction and semantic search. The task is challenging because of the ambiguity in the meaning of entity mentions (e.g., "Washington" can refer to the state, the capital of the US, the first US president George Washington, and so forth).
The key to improve the performance of EL is to accurately model the semantic context of entity mentions. Because our model learns the likelihood of an entity appearance in a given text, it can naturally be used for modeling the context of EL.

Our Method
Following past work), we address this task as a classification problem that selects the most relevant answer from the possible answers observed in the dataset. We adopt the same neural network architecture described in Section 3.2.2 (see). We use the following three features: the vector of the entity v e 14 , the vector of the question v t (computed using Eq. (2)), and the dot product of v e and v t . Note that we do not include other features in this task.
The hyper-parameters used in our model (i.e., the number of units in the hidden layer and the dropout probability) are shown in. We tuned these parameters using the development set of each dataset.
Unlike the EL task, we updated all parameters including representations of words and entities for training our QA method. We used stochastic gradient descent (SGD) to train the model. The minibatch size was fixed at 100, and the learning rate was controlled by RMSprop. We used the accuracy on the development set of each dataset to detect the best epoch. Similar to the EL task, we tested the four models to initialize the representations v t and v e , i.e., the NTEE, the fixed NTEE, the SG-proj, and the SGproj-dbp models. Further, the representations of the NTEE model and the fixed NTEE model were those that were trained with the sentences because of their overall superior accuracy compared to those trained with paragraphs.

Mention Disambiguation
We address the mention disambiguation task using a multi-layer perceptron (MLP) with a single hidden layer. shows the architecture of our neural network model. The model selects an entity from among the entity candidates for each mention min a document t. For each entity candidate e, we input the vector of the entity v e 9 , the vector of the document v t (computed with Eq. (2)), the dot product of v e and v t 10,11 , and the small number of features for EL described below. On top of these features, we stack a hidden layer with nonlinearity using rectified linear units (ReLU) and dropout. We also add an output layer onto the hidden layer and select the most relevant entity using softmax over the entity candidates.
Similar to past work, we include a small number of features in our model. First, we use the following three standard EL features: the entity popularity of e, the prior probability of m referring toe, and the maximum prior probability of e of all mentions int. In addition, we optionally add features representing string similarities between the title of e and the surface of m. These similarities include whether the title of e exactly equals or contains the surface of m, and whether the title of e starts or ends with the surface of m.
We tuned the following two hyper-parameters using the micro-accuracy on the development set of each dataset: the number of units in the hidden layer and the dropout probability. The results are listed in.
Further, we trained the model by using stochastic gradient descent (SGD). The learning rate was controlled by RMSprop, and the mini-batch size was set to 100. We also used the micro-accuracy on the development set to locate the best epoch for testing. We tested the NTEE model and the fixed NTEE model to initialize the parameters of representations v t and v e . Furthermore, we also tested two simple methods using the pre-trained representations (i.e., skip-gram). The first method is that the representations of words and entities are initialized using the pre-trained representations presented in Section 2.2, and the other parameters are initialized randomly (denoted by SG-proj). The second method is the same method as in SG-proj except the training corpus of the pre-trained representations is augmented using the DBpedia abstract corpus (denoted by SGproj-dbp). Regarding the NTEE and the fixed NTEE models, sentences (rather than paragraphs) were used to train the proposed representations because of the superior performance of this approach on both the CoNLL and TAC 2010 datasets. Further, we did not update our representations of words (v w ) and entities (v e ) in the training of our EL method, because updating them did not generally improve the performance. Additionally, we used a vector filled with zeros as representations of entities that were not contained in our vocabulary.

Factoid Question Answering
Question Answering (QA) has been one of the central problems in NLP research for the last few decades. Factoid QA is one of the typical types of QA that aims to predict an entity (e.g., events, authors, and actors) that is discussed in a given question. Quiz bowl is a popular trivia quiz game in which players are asked questions consisting of 4-6 sentence questions describing entities. The dataset of the quiz bowl has been frequently used for evaluating factoid QA methods in recent literature on QA.
In this section, we demonstrate that our proposed representations can be effectively used as background knowledge for the QA task.

Qualitative Analysis
In order to investigate what happens inside our model, we conducted a qualitative analysis using our proposed representations trained with sentences. We first inspected the word representations of our model and our pre-trained representations (i.e., the skip-gram model) by computing the top five similar words of five words (i.e., her, dry, spanish, tennis, moon) using cosine similarity. The results are presented in. Interestingly, our model is somewhat more specific than the skip-gram model. For example, there is only one word she whose cosine similarity to the word her is more than 0.5 in our model, whereas all the corresponding similar words in the skip-gram model (i.e., she, his, herself, him, and mother) satisfy that condition. We observe a similar trend for the similar words of dry. Furthermore, all the words similar to tennis are strictly re-lated to the sport itself in our model, whereas the corresponding similar words of the skip-gram model contain broader words such as ball sports (e.g., badminton and volleyball). A similar trend can be observed for the similar words of spanish and moon.
Similarly, we also compared our entity representations with those of the pre-trained representations by computing the top five similar entities of six entities (i.e., Europe, Golf, Tea, Smartphone, Scarlett Johansson, and The Lord of the Rings) with respect to cosine similarity. contains the results. For the entities Europe and Golf, we observe similar trends to our word representations. Particularly, in our model, the most similar entities of Europe and Golf are Eastern Europe and Golf course, respectively, whereas those of the skip-gram model are Asia and Tennis, respectively. However, the similar entities of most entities (e.g., Tea, Smartphone, Scarlett Johansson and The Lord of the Rings) appear to be similar between our model and the skipgram model.

Related Work
Various neural network models that learn distributed representations of arbitrary-length texts (e.g., paragraphs and sentences) have recently been proposed. These models aimed to produce general-purpose text representations that can be used with ease in various downstream NLP tasks. Although most of these models learn text representations from an unstructured text corpus (Le and, there have also been proposed models that learn text representations by leveraging structured linguistic resources. For instance, trained their model using a large number of noisy phrase pairs retrieved from the Paraphrase Database (PPDB). use several public dictionaries to train the model by mapping definition texts in a dictionary to representations of the words explained by these texts. To our knowledge, our work is the first work to learn generic text representations with the supervision of entity annotations.
Several methods have also been proposed for extending the word embedding methods. For example, proposed a method to train word embedding with dependency-based con-. These models are typically based on the skip-gram model and directly model the semantic relatedness between KB entities. Our work differs from these studies because we aim to learn representations of arbitrary-length texts in addition to entities. Another related approach is the relational embedding (or knowledge embedding), which encodes entities as continuous vectors and relations as some operations on the vector space, such as vector addition. These models typically learn representations from large KB graphs consisting of entities and relations. Similarly, the universal schema jointly learned continuous representations of KB relations, entities, and surface text patterns for the relation extraction task.
Finally, recently proposed a method to jointly learn the embeddings of words and entities from Wikipedia using the skip-gram model and applied it to EL. Our method differs from their method in that their method does not directly model arbitrary-length texts (i.e., paragraphs and sentences), which we proved to be highly effective for various tasks in this paper. Moreover, we also showed that the joint embedding of texts and entities can be applied not only to EL but also for wider applications such as semantic textual similarity and factoid QA.

Conclusions
In this paper, we presented a novel model capable of jointly learning distributed representations of texts and entities from a large number of entity annotations in Wikipedia. Our aim was to construct the proposed general-purpose model such that it enables practitioners to address various NLP tasks with ease. We achieved state-of-the-art results on three important NLP tasks (i.e., semantic textual similarity, entity linking, and factoid question answering), which clearly demonstrated the effectiveness of our model. Furthermore, the qualitative analysis showed that the characteristics of our learned representations apparently differ from those of the conventional word embedding model (i.e., the skip-gram model), which we plan to investigate in more detail in the future. Moreover, we make our code and trained models publicly available for future research.
Future work includes analyzing our model more extensively and exploring the effectiveness of our model in terms of other NLP tasks. We also aim to test more expressive neural network models (e.g., LSTM) to derive our text representations. Furthermore, we believe that one of the promising directions would be to incorporate the rich structural data of the KB such as relationships between entities, links between entities, and the hierarchical category structure of entities.