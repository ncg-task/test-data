125	0	3	BOW
125	31	36	using
125	39	76	logistic regression ( LR ) classifier
125	77	89	trained with
125	90	109	binary BOW features
126	0	8	BOW - DT
126	12	20	based on
126	25	37	BOW baseline
126	38	52	augmented with
126	57	68	feature set
126	69	73	with
126	74	104	dependency relation indicators
127	0	5	QANTA
127	21	29	based on
127	32	56	recursive neural network
127	57	66	to derive
127	71	98	distributed representations
127	99	101	of
127	102	111	questions
129	0	10	FTS - BRNN
129	14	22	based on
129	27	73	bidirectional recurrent neural network ( RNN )
129	74	78	with
129	79	108	gated recurrent units ( GRU )
54	59	97	https://github.com/ studio-ousia /ntee
26	19	26	propose
26	27	64	Neural Text - Entity Encoder ( NTEE )
26	69	89	neural network model
26	90	106	to jointly learn
26	107	134	distributed representations
26	135	137	of
26	138	179	texts ( i.e. , sentences and paragraphs )
26	184	195	KB entities
27	0	3	For
27	4	14	every text
27	15	17	in
27	22	24	KB
27	27	36	our model
27	45	52	predict
27	57	74	relevant entities
27	81	87	places
27	92	122	text and the relevant entities
27	123	128	close
27	42	44	to
27	132	142	each other
27	143	145	in
27	148	171	continuous vector space
28	3	6	use
28	7	37	humanedited entity annotations
28	38	51	obtained from
28	52	61	Wikipedia
28	70	72	as
28	73	88	supervised data
28	89	91	of
28	92	109	relevant entities
28	110	112	to
28	117	122	texts
28	123	133	containing
28	134	151	these annotations
29	12	23	KB entities
29	34	53	conventionally used
29	54	62	to model
29	63	72	semantics
29	73	75	of
29	76	81	texts
30	2	24	representative example
30	28	62	Explicit Semantic Analysis ( ESA )
30	71	81	represents
30	86	95	semantics
30	96	98	of
30	101	105	text
30	106	111	using
30	114	133	sparse vector space
30	142	156	each dimension
30	157	171	corresponds to
30	176	191	relevance score
30	192	194	of
30	199	203	text
30	204	206	to
30	207	218	each entity
31	14	17	ESA
31	18	23	shows
31	29	33	text
31	41	63	accurately represented
31	64	69	using
31	72	81	small set
31	82	84	of
31	89	106	relevant entities
35	36	43	placing
35	44	62	texts and entities
35	63	67	into
35	72	89	same vector space
35	90	97	enables
35	104	118	easily compute
35	123	133	similarity
35	134	141	between
35	142	160	texts and entities
2	0	78	Learning Distributed Representations of Texts and Entities from Knowledge Base
26	93	195	jointly learn distributed representations of texts ( i.e. , sentences and paragraphs ) and KB entities
137	88	99	compared to
137	104	134	state - of - the - art methods
137	144	149	QANTA
137	154	164	FTS - BRNN
137	180	200	clearly outperformed
137	77	87	our method
140	32	42	our method
140	43	67	mostly performed perfect
140	68	79	in terms of
140	80	90	predicting
140	95	100	types
140	101	103	of
140	104	118	target answers
140	128	137	locations
140	140	146	events
140	153	159	people
