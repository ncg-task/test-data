16	20	27	develop
16	28	52	our supervised WSD model
16	58	67	leverages
16	70	126	Bidirectional Long Short - Term Memory ( BLSTM ) network
17	13	23	works with
17	24	44	neural sense vectors
17	47	51	i.e.
17	52	68	sense embeddings
17	79	82	are
17	83	90	learned
17	91	97	during
17	98	112	model training
17	119	126	employs
17	127	146	neural word vectors
17	149	153	i.e.
17	154	169	word embeddings
17	180	183	are
17	184	191	learned
17	192	199	through
17	203	238	unsupervised deep learning approach
17	239	245	called
17	246	251	GloVe
17	269	272	for
17	303	316	context words
2	47	72	Word Sense Disambiguation
5	73	106	Word Sense Disambiguation ( WSD )
6	166	169	WSD
110	0	34	Between - all - models comparisons
114	3	7	show
114	8	24	our single model
114	25	35	sits among
114	40	69	5 top - performing algorithms
115	0	5	shows
115	10	17	results
115	18	20	of
115	25	84	top - performing and low - performing supervised algorithms
128	0	32	Within - our - model comparisons
136	14	21	reverse
136	26	58	sequential follow of information
136	59	63	into
136	64	86	our Bidirectional LSTM
136	92	99	shuffle
136	104	109	order
136	110	112	of
136	117	130	context words
136	141	148	replace
136	149	172	our Bidirectional LSTMs
136	173	177	with
136	178	218	two different fully - connected networks
136	219	221	of
136	226	238	same size 50
136	279	287	achieved
136	288	295	results
136	301	325	notably less than 72.5 %
