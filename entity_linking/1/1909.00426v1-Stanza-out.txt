title
Pre-training of Deep Contextualized Embeddings of Words and Entities for Named Entity Disambiguation
abstract
Deep contextualized embeddings trained using unsupervised language modeling ( e.g. , ELMo and BERT ) are successful in a wide range of NLP tasks .
In this paper , we propose a new contextualized embedding model of words and entities for named entity disambiguation ( NED ) .
Our model is based on the bidirectional transformer encoder and produces contextualized embeddings for words and entities in the input text .
The embeddings are trained using a new masked entity prediction task that aims to train the model by predicting randomly masked entities in entityannotated texts .
We trained the model using entity - annotated texts obtained from Wikipedia .
We evaluated our model by addressing NED using a simple NED model based on the trained contextualized embeddings .
As a result , we achieved stateof - the - art or competitive results on several standard NED datasets .
Introduction
Named entity disambiguation ( NED ) refers to the task of assigning entity mentions in a text to corresponding entries in a knowledge base ( KB ) .
This task is challenging owing to the ambiguity between entity names ( e.g. , " World Cup " ) and the entities they refer to ( e.g. , FIFA World Cup and Rugby World Cup ) .
Deep contextualized word embedding models , e.g. , ELMo and BERT , have recently achieved stateof - the - art results on many tasks .
Unlike conventional word embedding models that assign a single , fixed embedding per word , these models produce a contextualized embedding for each word in the input text using a pretrained neural network encoder .
The encoder can be a recurrent neural network or transformer , and is usually trained using an unsupervised objective based on language modeling .
For instance , proposed Masked Language Model ( MLM ) , which aims to train the embeddings by predicting randomly masked words in the text .
In this paper , we describe a new contextualized embedding model for words and entities for NED .
Following , the proposed model is based on the bidirectional transformer encoder .
It takes a sequence of words and entities in the input text , and produces a contextualized embedding for each word and entity .
Inspired by MLM , we propose masked entity prediction , a new task that aims to train the embedding model by predicting randomly masked entities based on words and non-masked entities in the input text .
We trained the model using texts and their entity annotations retrieved from Wikipedia .
We evaluated the proposed model by addressing NED using an NED model based on trained contextualized embeddings .
The NED model addresses the task by capturing word - based and entity - based contextual information using the trained contextualized embeddings .
As a result , we achieved state - of - the - art or competitive results on various standard NED datasets .
We will release our code and trained embeddings for further research .
Background and Related Work
Neural network - based approaches have recently achieved strong results on NED .
A key component of these approaches is an embedding model of words and entities trained using a large knowledge base ( e.g. , Wikipedia ) .
Such embedding models enable us to design NED models that capture the contextual information required to address NED .
These models are typically based on conventional word embedding models ( e.g. , skip - gram ) that assign a fixed embedding to each word and entity .
In this study , we aim to test the effectiveness of the pretrained contextualized embeddings for NED .
Contextualized
Embeddings of Words and Entities
In this section , we introduce our contextualized embedding model for words and entities .
shows the architecture of the proposed model .
Our model adopts a multi - layer bidirectional transformer encoder 1 with input representations described later in this section .
Given a sequence of tokens consisting of words and entities , the model first represents the sequence as a sequence of input embeddings , one for each token , and then the model generates a contextualized output embedding for each token .
Both input and output embeddings have H dimensions .
Hereafter , we denote the number of words and that of entities in the vocabulary of our model by V wand V e , respectively .
Input Representation
Similar to the approach adopted in , the input representation of a given token ( i.e. , word or entity ) is constructed by summing the following three embeddings of H dimensions :
Token embedding is the embedding of the corresponding token .
The matrices of the word and entity token embeddings are represented as A ? R VwH and B ?
R VeH , respectively .
Token type embedding represents the type of token , namely word type ( denoted by C word ) or entity type ( denoted by C entity ) .
Position embedding represents the position of the token in a word sequence .
A word and an entity appearing at i -th position in the sequence are represented as Di and E i , respectively .
If an entity name contains multiple words , we compute its position embedding by averaging the embeddings of the corresponding positions ( e.g. , New York City in ) .
Following , we insert special word tokens [ CLS ] and [ SEP ] to the word sequence as the first and last words , respectively .
Masked Entity Prediction
To train the embeddings , we propose masked entity prediction ( MEP ) , a new task based on MLM .
In particular , we mask some percentage of the input entities at random ; then , we train the embeddings to predict masked entities based on words and non-masked entities .
We represent masked entities using the special [ MASK ] entity token .
We adopt a model equivalent to the one used to predict words in MLM .
Specifically , we predict the original entity of a masked entity by applying the softmax function overall entities in our vocabulary : ?
where b o ? R
Ve is the output bias , and m ? R H is derived as :
is the gelu activation function , and layer norm ( ) is the layer normalization function ( Lei .
Training
We used the same model configuration adopted in the BERT LARGE model .
In particular , we used the bidirectional transformer encoder with H = 1024 hidden dimensions , 24 hidden layers , 16 self - attention heads , and the gelu activation function .
We also set the feed - forward / filter size to 4096 , the dropout probability applied to all layers was 0.1 , and the maximum word length in an input sequence was set to 512 .
Furthermore , we initialized the parameters of our model that were common with BERT ( i.e. , parameters in the transformer encoder and the embeddings for words ) using the uncased version of the pretrained BERT LARGE model .
2
Other parameters , namely the parameters in the MEP and the embeddings for entities , were initialized randomly .
The model was trained via iterations over Wikipedia pages in a random order for two epochs .
We generated input sequences by splitting the content of each page into sequences consisting of ? 512 words and their entity annotations ( i.e. , hyperlinks ) .
We used the December 2018 version of Wikipedia , consisting of approximately 3.5 billion words and 11 million entity annotations .
We masked 30 % of all entities in each sequence at random .
The input text was lowercased and tokenized to words 3 using the BERTs sub - word tokenizer with its vocabulary consisting of V w = 30 , 000 words .
Similar to Ganea and Hofmann , we built an entity vocabulary consisting of V e = 221 , 965 entities that were contained in the entity candidates in the NED datasets described in Section 4.1 .
We used the Adam optimizer with a learning rate of 2 e - 5 , ?1 = 0.9 , ?2 = 0.999 , and L2 weight decay of 0.01 .
The batch size was set to 252 .
We trained the model by maximizing the log likelihood of MEP 's predictions .
To stabilize training , we updated only parameters that were initialized randomly ( i.e. , fixed the parameters initialized using BERT ) at the first epoch , and updated all parameters at the second epoch .
The training took approximately six days using eight Tesla V100 GPUs .
NED Based on Pre-trained Contextualized Embeddings
In this section , we address NED based on the proposed contextualized embeddings .
Experimental Setup
Our experimental setup described in this section follows past work ( Ganea and Hofmann , 2017 ; .
In particular , we test the NED models using in - domain and out - domain scenarios .
In the in - domain scenario , we use the train and test b sets of the AIDA - CoNLL dataset to train and test the models , respectively .
In the out - domain scenario , we test the generalization ability of the models on five test sets : MSNBC ( MSB ) , AQUAINT ( AQ ) , ACE2004 ( ACE ) , which were cleaned by , WNED - CWEB ( CWEB ) , and WNED - WIKI ( WW ) , which were obtained from ClueWeb and Wikipedia .
For all datasets in both scenarios , we use the standard KB + YAGO entity candidates and their associated prior probabilities ( p ( e |m ) ) ( Ganea and Hofmann , 2017 ) , and use only the top 30 candidates based onp ( e |m ) .
We consider only mentions that refer to valid entities in Wikipedia .
We report the accuracy for the indomain scenario , and the micro F1 score ( averaged per mention ) for the out - domain scenario .
Model Inputs
For each mention in the input document , we create an input sequence consisting of ( 1 ) a masked entity corresponding to the mention , ( 2 ) words in the document 4 , and optionally ( 3 ) entities obtained from pseudo entity annotations .
Pseudo entity annotations are created by treating all mentions except the target mention in the document as entity annotations referring to their entity candidates .
For each mention , we create a pseudo entity annotation for each entity candidate of the mention .
For efficiency , we ignore a candidate if itsp ( e |m ) is less than 0.1 .
Note that the contextual information obtained from entities appearing in the same document has been considered as critical in improving NED and a main focus of the past literature on NED .
Because the transformer encoder is based on a neural attention mechanism to compute the embedding of a token by automatically attending relevant tokens in the input document , we assume that the trained transformer encoder can selectively attend to relevant entities if we input noisy and likely irrelevant entities based on the pseudo annotations .
Model
Our NED model is based on our pre-trained contextualized embeddings .
For each entity mention with its K entity candidates , our NED model first takes the input sequence described above , and computes the vector m ?
R H corresponding to the mention using Eq. ( 2 ) .
Then , the model predicts the referent entity using the softmax function over the entity candidates :
where B * ?
R KH and b * o ?
R K consists of the entity token embeddings and the output bias values corresponding to the entity candidates , respectively .
Note that B * and b * o are the subsets of B and b o , respectively .
In the in - domain scenario , we fine - tuned the model by maximizing the log likelihood of the NED predictions on the training set of the AIDA - CoNLL dataset .
During the training , we fixed the entity token embeddings ( B and B * ) and output bias ( b o and b * o ) , and updated all other parameters .
We set the batch size to 32 , and used the Adam optimizer with a learning rate of 2 e - 5 , ?1 = 0.9 , ?2 = 0.999 , and L2 weight decay of 0.01 .
The training consisted of two epochs .
5
In the outdomain scenario , we did not perform fune - tuning .
Results
The results of the in - domain scenario are shown in .
6 Several models , including our models , report the 95 % confidence intervals obtained over five runs .
As shown , our models outperformed all previously proposed models .
Furthermore , using pseudo entity annotations boosted the accuracy by 0.3 % .
Methods
Accuracy 88.7 89.0 91.0 91.5 Ganea and Hofmann 92.22 0.14 93.0 93.07 0.27 Our 94.0 0.28 Our ( + pseudo entities )
94.3 0.25 The results of the out - domain scenario are shown in .
Our models achieved new state - of - the - art results on four of the five datasets , namely MSNBC , AQUAINT , ACE2004 , and WNED - WIKI , and performed competitive on the WNED - CLUEWEB dataset .
Furthermore , using pseudo entity annotations improved the performance on the AQUAINT and ACE2004 datasets .
Note that unlike all the past models shown in except , our models used in the out - domain scenario were trained only on entity annotations retrieved from Wikipedia .
Therefore , our models can be easily applied to any languages in which Wikipedia is available .
Conclusions
In this work , we proposed a contextualized embedding model of words and entities for NED .
We also introduced MEP to train the model using entityannotated texts as inputs .
We trained the model using Wikipedia and evaluated the effectiveness of the model by addressing NED using various standard NED datasets .
The experimental results show the competitiveness of our model across a wide range of NED datasets .
In the future , we intend to improve our NED model by modeling the global coherence of the disambiguated entities .
