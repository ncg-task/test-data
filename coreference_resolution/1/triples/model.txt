(Contribution||has||Model)
(Model||leverage||neural architecture)
(neural architecture||as||our policy network)
(neural architecture||includes||scoring)
(scoring||has||potential entity mentions)
(neural architecture||includes||learning)
(learning||has||span representation)
(neural architecture||includes||generating)
(generating||has||probability distribution)
(probability distribution||has||over all possible coreference linking actions)
(over all possible coreference linking actions||from||current mention)
(current mention||to||antecedents)
(Model||introduce||entropy regularization term)
(entropy regularization term||prevent||policy)
(policy||from||prematurely converging)
(prematurely converging||to||bad local optimum)
(entropy regularization term||to encourage||exploration)
(Model||update||regularized policy network parameters)
(regularized policy network parameters||based on||rewards)
(rewards||associated with||sequences)
(sequences||of||sampled actions)
(regularized policy network parameters||computed on||whole input document)
(Model||has||sequence)
(sequence||of||linking actions)
(sequence||has||our reward function)
(our reward function||directly related to||coreference evaluation metrics)
(our reward function||used to||measure)
(measure||has||how good)
(how good||has||generated coreference clusters)
(Model||propose||goal - directed endto - end deep reinforcement learning framework)
(goal - directed endto - end deep reinforcement learning framework||to resolve||coreference)
