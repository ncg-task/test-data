162	4	12	distance
162	13	20	between
162	21	49	spans and the width of spans
162	54	69	crucial signals
162	70	73	for
162	74	96	coreference resolution
163	5	15	contribute
163	16	22	3.8 F1
163	23	25	to
163	30	42	final result
169	67	70	see
169	73	85	contribution
169	86	88	of
169	89	95	0.9 F1
169	96	100	from
169	101	127	character - level modeling
173	15	19	show
173	22	28	1.3 F1
173	29	40	degradation
173	56	63	without
173	68	87	attention mechanism
173	88	99	for finding
173	100	121	task - specific heads
181	14	21	keeping
181	22	40	mention candidates
181	41	52	detected by
181	57	76	rule - based system
181	77	81	over
181	82	103	predicted parse trees
181	134	142	degrades
181	143	154	performance
181	155	157	by
181	158	162	1 F1
183	0	4	With
183	5	20	oracle mentions
183	26	29	see
183	33	44	improvement
183	45	47	of
183	48	55	17.5 F1
118	4	19	word embeddings
118	25	44	fixed concatenation
118	45	47	of
118	48	114	300 - dimensional GloVe embeddings and 50 - dimensional embeddings
118	127	137	normalized
118	138	143	to be
118	144	156	unit vectors
119	0	24	Outof - vocabulary words
119	29	43	represented by
119	46	52	vector
119	53	55	of
119	56	61	zeros
121	4	16	convolutions
121	17	21	have
121	22	34	window sizes
121	35	37	of
121	38	62	3 , 4 , and 5 characters
121	70	83	consisting of
121	84	94	50 filters
123	4	17	hidden states
123	18	20	in
123	25	30	LSTMs
123	36	50	200 dimensions
124	0	31	Each feedforward neural network
124	32	43	consists of
124	44	61	two hidden layers
124	62	66	with
124	67	81	150 dimensions
124	86	108	rectified linear units
134	4	16	LSTM weights
134	21	37	initialized with
134	38	65	random orthonormal matrices
137	0	13	Dropout masks
137	18	31	shared across
137	32	41	timesteps
137	42	53	to preserve
137	54	81	long - distance information
138	4	17	learning rate
138	21	28	decayed
138	29	31	by
138	32	37	0.1 %
138	38	53	every 100 steps
140	0	8	All code
140	12	26	implemented in
140	27	40	Tensor - Flow
120	0	2	In
120	7	20	character CNN
120	23	33	characters
120	38	52	represented as
120	53	87	learned 8 - dimensional embeddings
133	3	6	use
133	7	11	ADAM
133	12	15	for
133	16	24	learning
133	25	29	with
133	32	46	minibatch size
133	47	49	of
133	50	51	1
135	3	8	apply
135	9	20	0.5 dropout
135	21	23	to
135	28	43	word embeddings
135	48	69	character CNN outputs
136	9	20	0.2 dropout
136	21	23	to
136	24	41	all hidden layers
136	46	64	feature embeddings
139	13	24	trained for
139	25	41	up to 150 epochs
139	44	48	with
139	49	63	early stopping
10	3	10	present
10	44	79	neural coreference resolution model
10	88	95	learned
10	96	107	end - toend
10	108	113	given
10	114	140	only gold mention clusters
12	146	154	training
12	158	185	end - to - end neural model
12	191	205	jointly learns
12	212	217	spans
12	222	237	entity mentions
12	242	248	how to
12	254	261	cluster
13	10	22	reasons over
13	27	32	space
13	33	35	of
13	36	45	all spans
13	46	51	up to
13	54	68	maximum length
13	73	91	directly optimizes
13	96	115	marginal likelihood
13	116	118	of
13	119	135	antecedent spans
13	136	140	from
13	141	166	gold coreference clusters
14	3	11	includes
14	14	34	span - ranking model
14	40	47	decides
14	66	93	which of the previous spans
14	110	125	good antecedent
15	0	11	At the core
15	29	46	vector embeddings
15	47	59	representing
15	60	65	spans
15	12	14	of
15	69	73	text
15	74	76	in
15	81	89	document
15	98	105	combine
15	106	150	context - dependent boundary representations
15	151	155	with
15	158	192	head - finding attention mechanism
15	193	197	over
15	202	206	span
16	4	23	attention component
16	27	38	inspired by
16	39	85	parser - derived head - word matching features
16	86	90	from
16	91	107	previous systems
16	117	133	less susceptible
16	137	153	cascading errors
2	0	44	End - to - end Neural Coreference Resolution
4	23	60	end - to - end coreference resolution
151	0	19	Coreference Results
153	3	13	outperform
153	14	30	previous systems
153	31	33	in
153	34	45	all metrics
154	16	32	our single model
154	33	41	improves
154	46	79	state - of - the - art average F1
154	80	82	by
154	83	86	1.5
154	93	115	our 5 - model ensemble
154	116	124	improves
154	128	130	by
154	131	134	3.1
155	4	26	most significant gains
155	32	36	from
155	37	49	improvements
155	50	52	in
155	53	59	recall
116	0	15	Hyperparameters
147	0	7	Results
