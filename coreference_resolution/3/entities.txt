15	3	12	introduce
15	16	29	approximation
15	30	32	of
15	33	57	higher - order inference
15	63	67	uses
15	72	99	span - ranking architecture
15	105	107	in
15	108	127	an iterative manner
20	15	28	less accurate
20	33	61	more efficient coarse factor
20	62	64	in
20	69	94	pairwise scoring function
16	0	2	At
16	3	17	each iteration
16	24	47	antecedent distribution
16	51	58	used as
16	62	81	attention mechanism
16	82	102	to optionally update
16	103	132	existing span representations
16	135	143	enabling
16	144	157	later corefer
19	0	12	To alleviate
19	13	37	computational challenges
19	38	42	from
19	48	72	higher - order inference
19	83	90	propose
19	93	117	coarseto - fine approach
19	126	138	learned with
19	141	169	single endto - end objective
21	5	22	additional factor
21	23	30	enables
21	34	52	extra pruning step
21	53	59	during
21	60	69	inference
21	75	82	reduces
21	87	108	number of antecedents
22	24	40	cheaply computes
22	43	55	rough sketch
22	56	58	of
22	59	77	likely antecedents
22	78	93	before applying
22	96	127	more expensive scoring function
118	4	12	baseline
118	50	70	span - ranking model
118	76	90	augmented with
118	91	126	both ELMo and hyperparameter tuning
118	135	143	achieves
118	144	151	72.3 F1
119	0	17	Our full approach
119	18	26	achieves
119	27	34	73.0 F1
119	37	44	setting
119	47	67	new state of the art
119	68	71	for
119	72	94	coreference resolution
121	0	13	Despite using
121	14	34	far less computation
121	40	51	outperforms
121	56	64	baseline
122	17	24	observe
122	27	45	much higher recall
122	46	59	when adopting
122	64	91	coarse - to - fine approach
123	16	35	further improvement
123	36	48	by including
123	53	77	second - order inference
2	15	37	Coreference Resolution
112	0	7	Results
