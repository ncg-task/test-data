title
Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression
abstract
Heatmap regression with a deep network has become one of the mainstream approaches to localize facial landmarks. However, the loss function for heatmap regression is rarely studied. In this paper, we analyze the ideal loss function properties for heatmap regression in face alignment problems. Then we propose a novel loss function, named Adaptive Wing loss, that is able to adapt its shape to different types of ground truth heatmap pixels. This adaptability penalizes loss more on foreground pixels while lesson background pixels. To address the imbalance between foreground and background pixels, we also propose Weighted Loss Map, which assigns high weights on foreground and difficult background pixels to help training process focus more on pixels that are crucial to landmark localization. To further improve face alignment accuracy, we introduce boundary prediction and CoordConv with boundary coordinates. Extensive experiments on different benchmarks, including COFW, 300W and WFLW, show our approach outperforms the state-of-the-art by a significant margin on various evaluation metrics. Besides, the Adaptive Wing loss also helps other heatmap regression tasks. Code will be made publicly available at https://github.com/ protossw512/AdaptiveWingLoss.
Introduction
Face alignment, also known as facial landmark localization, seeks to localize pre-defined landmarks on human faces. Face alignment plays an essential role in many face related applications such as face recognition, face frontalization and 3D face reconstruction. In recent years, Convolutional Neural Network (CNN) based heatmap regression has become one of the mainstream approaches for face alignment problems and achieved considerable performance on frontal faces. However, landmarks on faces with large pose, occlusion and significant blur are still challenging to localize.
Heatmap regression, which regresses a heatmap generated from landmark coordinates, is widely used for face  alignment. In heatmap regression, the ground truth heatmap is generated by plotting a Gaussian distribution centered at each landmark on each channel. The model regresses against the ground truth heatmap at pixel level and then use the predicted heatmaps to infer landmark locations. Prediction accuracy on foreground pixels (pixels with positive values), especially the ones near the mode of each Gaussian distribution (, is essential to accurately localize landmarks, even small prediction errors on these pixels can cause the prediction to shift from the correct modes. On the contrary, accurately predicting the values of background pixels (pixels with zero values) is less important, since small errors on these pixels will not affect landmark prediction inmost cases. However, prediction accuracy on difficult background pixels ( background pixels near foreground pixels) are also important since they are often incorrectly regressed as foreground pixels and could cause inaccurate predictions.
From this discussion, we locate two issues of the widely used Mean Square Error (MSE) loss in heatmap regression: i) MSE is not sensitive to small errors, which hurts the capability to correctly locate the mode of the Gaussian dis-tribution; ii) During training all pixels have the same loss function and equal weights, however, background pixels absolutely dominates foreground pixels on a heatmap. As a result of i) and ii), models trained with the MSE loss tend to predict a blurry and dilated heatmap with low intensity on foreground pixels compared to the ground truth (. This low quality heatmap could cause wrong estimation of facial landmarks. Wing loss is shown to be effective to improve coordinate regression, however, according to our experiment, it is not applicable for heatmap regression. Small errors on background pixels will accumulate significant gradients and thus cause the training process to diverge. We thus propose anew loss function and name it Adaptive Wing loss (Sec., that is able to significantly improve the quality of heatmap regression results.
Due to the translation invariance of the convolution operation in bottom-up and top-down CNN structures such as stacked Hourglass (HG), the network is notable to capture coordinate information, which we believe is useful for facial landmark localization, since the structure of human faces is relatively stable. Inspired by the Coord-Conv layer proposed by Liu et al., we encode into our model the full coordinate information and the information only on boundaries predicted from the previous HG module into our model. The encoded coordinate information further improves the performance of our approach. To encode boundary coordinates, we also add a sub-task of boundary prediction by concatenating an additional boundary channel into the ground truth heatmap which is jointly trained with other channels.
In summary, our main contributions include:
• Propose a novel loss function for heatmap regression named Adaptive Wing loss, that is able to adapt its curvature to ground truth pixel values. This adaptive property reduces small errors on foreground pixels for accurate landmark localization, while tolerates small errors on background pixels fora better convergence rate. With proposed Weighted Loss Map it is also able to focus on foreground pixels and difficult background pixels during training.
• Encode coordinate information, including coordinates on boundary, into the face alignment algorithm using CoordConv. Our approach outperforms the state-of-the-art algorithms by a significant margin on mainstream face alignment datasets including 300W, COFW and WFLW. We also show the validity of the Adaptive Wing loss in the human pose estimation task which also utilizes heatmap regression.

Related Work
CNN based heatmap regression models leverage CNN to perform heatmap regression. In recent work, joint bottom-up and top-down architectures such as stacked HG were able to achieve the state-of-the-art performance. Bulat et al. proposed a hierarchical, parallel and multi-scale block as a replacement for the original ResNet block to further improve the localization accuracy of HG. Tang et al. was able to achieve current state-of-the-art with quantized densely connected U-Nets with fewer parameters than stacked HG models. Other architectures are also able to achieve excellent performance. Merget et al. proposed a fully convolutional neural network (FCN) that combines global and local context information fora refined prediction. Valle et al. combined CNN with ensemble of regression trees in a coarse-to-fine fashion to achieve the state-of-the art accuracy. Another focus of this area is the 3D face alignment, that aims to provide 3D dense alignment based on 2D images.
Loss functions for heatmap regression were rarely studied in previous work. GoDP used a distance-aware softmax loss to assign large penalty on incorrectly classified positive samples, while gradually reducing penalty on missclassified negative samples as the distance from nearby positive samples decrease. The Wing loss is a modified log loss for direct regression of landmark coordinates. Compared with MSE, it amplifies the influence of small errors. Although the Wing loss is able to achieve the state-of-theart performance in coordinate regression, it is not applicable to heatmap regression due to its high sensitivity to small errors on background pixels and the discontinuity of gradient at zero. Our proposed Adaptive Wing loss is novel since it is able to adapt its curvature to different ground truth pixel values, such that it can be sensitive to small errors on foreground pixels yet be able to tolerance small errors on background pixels. Hence, our loss can be applied to heatmap regression while the original Wing loss cannot be.
Boundary information was first introduced into face alignment by Wu et al.. LAB proposed a two-stage network with a stacked HG model to generate a facial boundary map, and then regress facial landmark coordinates directly with the help of boundary map. We believe including boundary information is beneficial to the heatmap regression and utilized a modified version to our model.
Coordinate Encoding. Translation invariance is intrinsic to the convolution operation. Although CNN greatly benefited from this parameter sharing scheme, Liu et al. showed the inability of the convolution operation to handle simple coordinate transforms, and proposed anew operation called CoordConv, which encodes coordinate information as additional channels before convolution operation. CoordConv was shown to improve vision tasks such as object detection and generative modeling. For face alignment, the input images are always generated from a face detector with small variance on location and scale. These properties inspire us to include CoordConv to help CNN: An overview of our model. The stacked HG takes a face image cropped with the ground truth bounding box and output one predicted heatmap for each landmark, respectively. An additional channel is used to predict facial boundaries. Due to limited space, we omitted the detailed structure of the stacked HG architecture, please refer for details.

Our Model
Our model is based on the stacked HG architecture from Bulat et al. which improved over the original convolution block design from Newell et al.. For each HG, the output heatmap is supervised with the ground truth heatmap. We also added a sub-task of boundary prediction as an additional channel of the heatmap. Coordinate encoding is added before the first convolution layer of our network and before the first convolution block of each HG module. An overview of our model is shown in

Adaptive Wing Loss for Face Alignment

Loss function rationale
Before starting our analysis, we would like to introduce a concept from robust statistics. Influence is a heuristic tool used in robust statistics to investigate the properties of an estimator. In the context of our paper, the influence function is proportional to the gradient of our loss function. So if the gradient magnitude is large at point y?? (indicting the error), then we say the loss function has a large influence at pointy ??. If the gradient magnitude is close to zero at this point, then we say the loss function has a small influence at pointy ??. Theoretically, for heatmap regression, training is converged only if:
where N is the total number of training samples, H, W and C are the height, width and channels of heatmap, respectively. Loss n is the loss of n ? th sample, y i,j,k and y i,j,k are ground truth pixel intensity and predicted pixel intensity respectively. At convergence, the influence of all errors must balance each other. Hence, a positive error on a pixel with large gradient magnitude (hence large influence) would need to be balanced by negative errors on many pixels with smaller influence. Errors with large gradient magnitude will also be more focused on during training compare to errors with small gradient magnitude. The essence of heatmap regression is to output a Gaussian distribution centered at each ground truth landmark. Thus the accuracy of estimating pixel intensity at the mode of the Gaussian plays a vital role on correctly localizing landmarks. The two issues we illustrated in Sec. 1 result in an inaccurate estimation on the position of landmarks due to lacking of focus during training on foreground pixels. In this section and Sec. 4.2, we will discuss the causes of the first issue and how our proposed Adaptive Wing loss is able to remedy it. The second issue will be discussed in Sec. 4.3.
The first issue is due to the commonly used MSE loss function for Heatmap regression. The gradient of the MSE loss is linear, so pixels with small errors have small influence, as shown in. This property could cause training to converge while many pixels still have small errors. As a result, models trained with MSE loss tend to predict a blurry and dilated heatmap. Even worse, the predicted heatmap often has low intensity on foreground pixels around difficult landmarks, e.g. occluded landmarks or faces with unusual illumination conditions. Accurately localizing landmarks from these low intensity pixels can be difficult. A good example can be found in.
L1 loss has constant gradient so that pixels with small errors have the same influence as pixels with large errors. However, the gradient of L1 loss is not continuous at point zero, which means for convergence, the amount of pixels with positive errors has to be exactly equal to the amount that has negative errors. The difficulty of achieving such delicate balance could cause training process to be unstable and oscillating.
Feng et al. is able to improve the above loss functions by proposing Wing loss that has constant gradient when error is large, and large gradient when the error is small. Thus pixels with small errors will be amplified. The Wing loss is defined as follows:
where y and? are the pixel values on ground truth heatmap and the predicted heatmap respectively, C = ? ? ? ln(1 + ?/ ) is used to make function continuous at |y ??| = ?. The Wing loss is, however, still not be able to overcome the discontinuity of its gradient at y ?? = 0, with its large gradient magnitude around this point, training is even more difficult to converge compared with L1 loss. This property makes the Wing loss not applicable for heatmap regression, since with the Wing loss calculated on all background pixels, small errors on background pixels are having out-ofproportion influence. Training a neural network that outputs zero or small gradient on these pixels is very difficult. According to our experiment, the training of a heatmap regression network with the Wing loss is never able to converge. The above analysis leads us to define the desired properties of an ideal loss function for heatmap regression. We expect our loss function to have a constant influence when error is large, so that it will be robust to inaccurate annotations and occlusions. As the training process continues and errors getting smaller, there will be two scenarios: i) For foreground pixels, the influence (as well as the gradient) should start to increase so that the training is able to focus on reducing these errors. The influence should then decrease rapidly as the errors go very close to zero, so that these "good enough" pixels will no longer be focused on. The reduced influence of correct estimations helps the network to stay converged, instead of oscillating like the L1 and the Wing loss. ii) For background pixels, the gradient should behaves more similar to the MSE loss, that is, it will gradually decrease to zero as the training error decreases, so that the influence will be relatively small when the errors are small. This property reduces the focus of the training on background pixels, stabilizing the training process.
A fixed loss function cannot achieve both properties simultaneously. Thus, the loss function should be able to adapt to different pixel intensities on the ground truth heatmaps. As the ground truth pixels close to the mode (have intensities that are close to 1), the influence of small errors should increase. With ground truth pixel intensities close to 0, the loss function should behave more similar to the MSE loss. Since pixel values on the ground truth heatmap range from 0 to 1, we also expect our loss function to have a smooth transition according to different pixel values.

The Adaptive Wing Loss
Following intuitions above, we propose our Adaptive Wing (AWing) loss, defined as follows:
where y and? are the pixel values on the ground truth heatmap and the predicted heatmap respectively, ?, ?, and ? are positive values,
) are used to make loss function continuous and smooth at |y ??| = ?. Unlike Wing loss which uses ? as the threshold, we introduce anew variable ? as a threshold to switch between linear and nonlinear part. For heatmap regression, we often regress a value between 0 and 1, so we expect our threshold lies in this range. When |y ??| < ?, we consider the error to be small and need stronger influence. More importantly, we adopt an exponential term ??y, which is used to adapt the shape of the loss function toy and makes loss function smooth at point zero. Note ? has to be slightly larger than 2 to maintain the ideal properties we discussed in Sec. 4.1, this is due to the normalization of yin the range of [0, 1]. For pixels on y with values close to 1 (the landmarks we want to localize), the power term ? ? y will be slightly larger than 1, and the nonlinear part will behave like Wing loss, which has large influence on smaller errors. But different from Wing loss, the influence will decrease to zero rapidly as errors are very close to zero (see). As y decreases, the loss function will shift to a MSE-like loss function, which allows the training not to focus on the pixels that still have errors but small influence. shows how the power term ? ? y facilities the smooth transition across different values of y, so that the influence of small errors will gradually increase as the value of y increases.
Larger ? and smaller values will increase the influence on small errors and vice versa, large ? values are shown to be effective according to our experiment. The nonlinear part of our Adaptive Wing loss function behaves similarly to Lorentzian (aka. Cauchy) loss in a more generalized fashion. But different from robust loss functions such as Lorentzian and Geman-McClure, we do not need the gradient to decrease to zero as error increases. This is due to the nature of heatmap regression. In robust regression, the learner learns to ignore noisy outliers with large error. In the context of face alignment, all facial landmarks are annotated with relatively small noises, so we do not have noisy outliers to ignore. A linear loss is sufficient for the training to converge to a location where predictions will be fairly close to the ground truth heatmap, and after that the loss function will switch to its nonlinear part to refine the prediction with increased influence on small errors. In practice, we found the linear form when errors are: The nonlinear part of the Adaptive Wing loss is able to adapt its shape according to different values of y. As y increases, the shape is more similar to the Wing loss, and the influence of small errors (near-side of they axis) will remain strong. As y decreases, the influence on these errors will decrease and the loss function will behave more like MSE.
large to achieve better performance, compared with keep using the nonlinear form when the error is large.
We empirically used ? = 2.1 in our model. In our experiments, we found ? = 14, = 1, ? = 0.5 to be most effective, detailed ablation studies on parameter settings are shown at Sec. 7.6.1.

Weighted loss map
In this section we will discuss the second issue in Sec. 4.1. Ina typical setting for facial landmark localization with a 64 × 64 heatmap, and the size of Gaussian of 7 × 7, foreground pixels only constitute 1.2% of all the pixels. Assigning equal weight for such an unbalanced data could make the training process slow to converge and result in an inferior performance. To further establish the network's ability to focus on foreground pixels and difficult background pixels (background pixels that are close to foreground pixels), we introduce the Weighted Loss Map to balance the loss from different types of pixels. We first define our loss map mask to be:
where H dis generated from ground truth heatmap H by a 3 × 3 gray dilation. The loss map mask M assigns foreground pixels and difficult background pixels 1, and other pixels 0.
With the loss map mask M , We define our Weighted Loss Map as follows:
where ? is element-wise production, Wis a scalar hyperparameter to control how much weight to be added. See fora visualization of weight map generation. In our experiments we use W = 10. The intuition is to assign pixels on heatmap with different weights. Foreground pixels have to be focused on during training, since these pixels are the most useful for localizing the mode of the Gaussian distribution. Difficult background pixels should also be focused on since these pixels are relatively difficult to regress, accurately regressing them could help narrow down the area of foreground pixels to improve localization accuracy.: Important pixels are generated by dilating H from with 3x3 dilation, and then binarizing to with a threshold of 0.2. For visualization purposes, all channels are max-pooled into one channel.

Boundary Information
Inspired by, we introduce boundary prediction into our network as a sub-task, but in a different manner. Instead of breaking boundaries into different parts, we use only one additional channel as the boundary channel that combines all boundary lines to our heatmap. We believe this will efficiently capture the global information on a human face. The boundary information then will be aggregated into the network naturally via convolution operations in a forward pass, and will also be used in Section 6 to generate the boundary coordinate map, which can further improve localization accuracy according to our ablation study in Sec. 7.6.1.

Coordinate aggregation
We integrate CoordConv into our model to improve the capability of traditional convolutional neural network to capture coordinate information. In addition to X, Y and radius coordinate encoding in, we also leverage our boundary prediction to generate X and Y coordinates only at boundary. More specifically, we define X coordinate encoding to be C x , the boundary prediction from previous HG is B, the boundary coordinate encoding B x is defined as:
B y is generated in the similar fashion from C y . The coordinate channels are generated at runtime and then concatenated with the original input to perform regular convolution.

Experiments

Datasets
We tested our approach on the COFW, 300W, 300W private test dataset and the WFLW dataset. The WFLW dataset is the most difficult dataset of them all. For more details on theses datasets, please refer to supplementary materials.

Evaluation Metrics
Normalized Mean Error (NME) is commonly used to evaluate the quality of face alignment algorithms. The NME for each image is defined as:
where P andP are the ground truth and the predicted landmark coordinates for each image respectively, M is the number of landmarks of each image,p i is the i-th predicted landmark coordinates in P and pi is the i-th ground truth landmark coordinates inP , dis the normalization factor. For the COFW dataset, we use inter-pupil (distance of eye centers) as the normalization factor. For the 300W dataset, we provide both inter-ocular distance (distance of outer eye corners) used as the original evaluation protocol in, and inter-pupil distance used in. For the WFLW dataset, we use the inter-ocular distance described in. Failure Rate (FR) is another metric to evaluate localization quality. For one image, if NME is larger than a threshold, then it is considered a failed prediction. For the 300W private test dataset, we use 8% and 10% respectively to compare with different approaches. For the WFLW dataset, we follow and use 10% as the threshold.
Cumulative Error Distribution (CED) curve shows the NME to the proportion of total test samples. The curve is usually plotted from zero up to the NME failure rate threshold (e.g. 10%, 8%). Area Under Curve (AUC) is calculated based on the CED curve. Larger AUC reflects that larger portion of the test dataset is well predicted.

Implementation details
During training and testing, we use provided bounding boxes from dataset (with the longer side as the length of a square) to crop faces from images, except for the 300W private test dataset since no official bounding boxes are provided. For the WFLW dataset, the provided bounding boxes are not very accurate, to ensure all landmarks are preserved from cropping, we enlarge the bounding boxes by 10% on both dimensions. For the 300W private test dataset, we use ground truth landmarks to crop faces.
The input of the network is 256 × 256, the output of each stacked HG is 64 × 64. We use four stacks of HG, same with other baselines. During training, we use RM-SProp with an initial learning rate of 1 × 10 ?4 . We set the momentum to be 0 (adopted from) and the weight decay to be 1 × 10 ?5 . We train for 240 epoches, and the learning rate is reduced to 1 × 10 ?5 and 1 × 10 ?6 after 80 and 160 epoches. Data augmentation is performed with random rotation (±50 • ), translation (±25px), flipping (50%), and rescaling (±15%). Random Gaussian blur, noise and occlusion are also used. All models are trained from scratch. During inference, we adopt the same strategy used in Newell et al., the location on the pixel with the highest response is shifted a quarter pixel to the second highest nearby pixel. The boundary line is generated from landmarks via distance transform similar to, different boundary lines are merged into one channel by selecting maximum values on each pixel across all channels.
Method NME AUC 10% FR 10% Human 5.60 -0.00 TCDCN ECCV 14. Our approach outperforms previous state-of-the-art by a significant margin, especially on the failure rate. We are able to reduce the failure rate measured at 10% NME from 3.73% to 0.99%. As for NME, our method perform much better than human (5.60%). Our performance on the COFW shows the robustness of our approach against faces with large pose and heavy occlusion.

Evaluation on 300W
Our method is able to achieve the state-of-the-art performance on the 300W testing dataset, see. For the challenge subset (iBug dataset), we are able to outperform Wing by a significant margin, which also proves the robustness of our approach against occlusion and large pose variation. Furthermore, on the 300W private test dataset), we again outperform the previous state-of-theart on variant metrics including NME, AUC and FR measured with either 8% NME and 10% NME. Note that we more than halved the failure rate of the next best baseline to 0.83%, which means only 5 faces out of 600 have an NME that is larger than 8%.

Evaluation on WFLW
Our method again achieves the best results on the WFLW dataset in, which is significantly more difficult than COFW and 300W (see for visualizations). On every subset we outperform the previous state-of-the-art ap-: Evaluation on the 300W testset proaches by a significant margin. Note that the baseline Wing is using ResNet50 as the backbone architecture, which already performs better than the CNN6/7 architecture they used in COFW and 300W. We are also able to reduce the failure rate and increase the AUC dramatically and hence improving the overall localization quality significantly. All in all, our approach fails on only 2.84% of all images, more than a two times improvement compared with   7.6. Ablation study

.1 Evaluation on different loss function parameters
To find the optimal parameter settings for the Adaptive Wing loss for heatmap regression, we examined different parameter combinations and evaluated on the WFLW dataset with faces cropped from ground truth landmarks.
However, the search space is too large and we only have limited resources. To reduce the search space, we set our initial ? to 0.5, since the pixel value of the ground truth heatmap is from 0 to 1, we believe focusing on errors that are smaller than 0.5 is more than enough. shows NMEs on different combinations of ? and . As a result, we picked ? = 14 and = 1. The experiments also show our Adaptive Wing loss is not very sensitive to ? and , since the difference of NMEs are not significant within a certain range of different settings. Then we fixed ? and , and examine different ?, the results are shown in.

Evaluation of different modules
Evaluation on the effectiveness of different modules is shown in. The dataset used for ablation study is WFLW. During training and testing, faces are cropped from ground truth landmarks. Note the baseline model (model trained with MSE) underperforms the state-of-theart. To compare with a naive weight mask without focus on hard negative pixels, we introduced a baseline weight map W M base =?W + 1, where W = 10. The major contribution comes from Adaptive Wing loss, which improves the benchmark by 0.74%. All other modules contributed incrementally to the localization performance, our Weighted Loss Map improves 0.25%, boundary prediction and coordinates encoding are able to contribute another 0.09%. Our Weighted Loss Map also outperforms W M base by a considerable margin, thanks to its ability to focus on hard background pixels.

Evaluation on human pose estimation
Although this paper mainly deals with face alignment, we have also performed experiments to prove the ability of the proposed Adaptive Wing loss in another heatmap regression task, human pose estimation. We choose LSP (using person-centric (PC) annotations) as evaluation dataset. LSP dataset consists of 11,000 training images and 1,000 testing images. Each image is labeled with 14 keypoints. The goal of this experiment is to examine the capability of the proposed Adaptive Wing loss to handle the pose estimation task compared with baseline MSE loss, rather   than achieving the state-of-the-art inhuman pose estimation. Some other works obtain better results by adding MPII into training or as pre-training, or use re-annotated labels with high resolution images in. Besides the MSE loss baseline, we also reported baselines from methods that trained solely on the LSP dataset. We trained our model from scratch with original labeling and low resolution images to see how well our Adaptive Wing loss could handle labeling noise and low quality images. Percentage Correct Keypoints (PCK) is used as the evaluation metric with torso dimension as the normalization factor. Please refer to the supplemental materials for more implementation details. Results are shown in. Our proposed Adaptive Wing loss significantly boosts performance compared with MSE, which proves the general applicability of the proposed Adaptive Wing loss on more heatmap regression tasks.

Conclusion
In this paper, we located two issues in the MSE loss function in heatmap regression. To resolve these issues, we proposed the Adaptive Wing loss and Weighted Loss Map for accurate localization of facial landmarks. To further improve localization results, we also introduced boundary prediction and CoordConv with boundary coordinates into our model. Experiments show that our approach is able to outperform the state-of-the-art on multiple datasets by a significant margin, using various evaluation metrics, especially on failure rate and AUC, which indicates our approach is more robust to difficult scenarios.

Acknowledgement
This paper is partially supported by the National Science Foundation under award 1751402.

Datasets Used in Our Experiments
The COFW [8] dataset includes 1,345 training images and 507 testing images annotated with 29 landmarks. This dataset is aimed to test the effectiveness of face alignment algorithms on faces with large pose and heavy occlusion. Various types of occlusions are introduced and result in a 23% occlusion on facial parts in average.
The 300W is widely used as a 2D face alignment benchmark with 68 annotated landmarks. 300W consists of the following subsets: LFPW, HELEN, AFW, XM2VTS and an additional dataset with 135 images with large pose, occlusion and expressions called iBUG. To compare with other approaches, we adopt the widely used protocol described in to train and evaluate our approach. More specifically, we use the training dataset of LFPW, HELEN, and the full AFW dataset as training dataset, and the test dataset of LFPW, HELEN and the full iBUG dataset as full test dataset. The full test dataset is then further split into two subsets, the test dataset of LFPW and HELEN is called the common test dataset, and iBUG is called the challenge test dataset. There is also a 300W private test dataset for the 300W contest, which contains 300 indoor and 300 outdoor faces. We also evaluated our approach on this dataset.
The WFLW is a newly introduced dataset with 98 manually annotated landmarks that constitutes of 7,500 training images and 2,500 testing images. In addition to denser annotations, it also provides attribute annotations including pose, expression, illumination, make-up, occlusion and blur. The six different subsets can be used for analyzing algorithm performance on subsets with different properties separately. The WFLW is considered more difficult than commonly used datasets such as AFLW and 300W due to its more densely annotated landmarks and difficult faces with occlusion, blur, large pose, makeup, expression and illumination.
For the LSP dataset, we used original label from author's official website . Although images with original resolutions are also provided, we choose not to use them. Also, we did not use re-annotated labels on LSP extended 10,000 training images from. Note that occluded keypoints are annotated in LSP original dataset but not in LSP extended training dataset. During training, we did not calculate loss on occluded keypoints for LSP extended training dataset. During training and testing, we did not follow [?] to crop single person from images with multiple persons to retain the difficulties of this dataset. Data augmentations is performed similarly to training with face alignment datasets.

Evaluation on AFLW
The AFLW dataset contains 24,368 faces with large poses. All faces are annotated by up to 21 landmarks per image, while the occluded landmarks were not labeled. For fair comparison with other methods we adopt the protocol from, which provides revised annotations with 19 landmarks. The training dataset contains 20,000 images, the full testing dataset contains 4,368 iamges. A subset of 1,314 frontal faces (no landmarks are occluded) are selected from the full test dataset as the frontal test set.

Method
Full(%) Frontal(%) RCPR CVPR 13 3.73 2.87 ERT CVPR 14 4.35 2.75 LBF CVPR 14 4.25 2.74 CFSS CVPR 15 3.92 2.68 CCL CVPR 16 2.72 2.17 TSR CVPR 17 2.17 -DAC-OSR CVPR 17 2.27 1.81 DCFE ECCV 18 2.17 -CPM+SBR CVPR 18 2.14 -SAN CVPR 18 1.91 1.85 DSRN CVPR 18 1.86 -LAB CVPR 18 1.85 1.62 Wing CVPR 18 1.65 -RCN + (L+ELT+A)CVPR

Experiment on different number of HG stacks
We compare the performance of different number of stacks of HG module (see details in). With reduced number of HGs, the performance of our approach remains outstanding. Even with only one HG block, our approach still outperforms previous state-of-the-arts in all datasets except the common subset and the full dataset of 300W. Note that the one HG model is able to run at 120 FPS with Nvidia GTX 1080Ti graphics card. The result reflects the effectiveness of our approach on limited computation resources.

Result Visualization
For visualization purpose, some localization results are shown in and: NME (%) on different number of stacks. The NMEs of 300W are normalized by inter-pupil/inter-ocular distance, the NMEs of COFW are normalized by inter-pupil distance, and the NMEs of 300W Private and WFLW are normlaized by inter-ocular distance. NMEs in the "Previous Best" row are selected from to 4 in our main paper. Runtime is evaluated on Nvidia GTX 1080Ti graphics card with batch size of 1.