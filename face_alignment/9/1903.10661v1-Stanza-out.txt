title
Semantic Alignment : Finding Semantically Consistent Ground - truth for Facial Landmark Detection
abstract
Recently , deep learning based facial landmark detection has achieved great success .
Despite this , we notice that the semantic ambiguity greatly degrades the detection performance .
Specifically , the semantic ambiguity means that some landmarks ( e.g. those evenly distributed along the face contour ) do not have clear and accurate definition , causing inconsistent annotations by annotators .
Accordingly , these inconsistent annotations , which are usually provided by public databases , commonly work as the groundtruth to supervise network training , leading to the degraded accuracy .
To our knowledge , little research has investigated this problem .
In this paper , we propose a novel probabilistic model which introduces a latent variable , i.e. the ' real ' ground - truth which is semantically consistent , to optimize .
This framework couples two parts ( 1 ) training landmark detection CNN and ( 2 ) searching the ' real ' groundtruth .
These two parts are alternatively optimized : the searched ' real ' ground - truth supervises the CNN training ; and the trained CNN assists the searching of ' real ' groundtruth .
In addition , to recover the unconfidently predicted landmarks due to occlusion and low quality , we propose a global heatmap correction unit ( GHCU ) to correct outliers by considering the global face shape as a constraint .
Extensive experiments on both image - based ( 300 W and AFLW ) and video - based ( 300 - VW ) databases demonstrate that our method effectively improves the landmark detection accuracy and achieves the state of the art performance .
Introduction
Deep learning methods have achieved great success on landmark detection due to the strong modeling capacity .
Despite this success , precise and credible landmark detection still has many challenges , one * equal contribution .
Non semantic moving
Semantic moving .
The landmark updates in training after the model is roughly converged .
Due to ' semantic ambiguity ' , we can see that many optimization directions , which are random guided by random annotation noises along with the contour and ' non semantic ' .
The others move to the semantically accurate positions .
Red and green dots denote the predicted and annotation landmarks , respectively .
of which is the degraded performance caused by ' semantic ambiguity ' .
This ambiguity results from the lack of clear definition on those weak semantic landmarks on the contours ( e.g. those on face contour and nose bridge ) .
In comparison , strong semantic landmarks on the corners ( e.g. eye corner ) suffer less from such ambiguity .
The ' semantic ambiguity ' can make human annotators confused about the positions of weak semantic points , and it is inevitable for annotators to introduce random noises during annotating .
The inconsistent and imprecise annotations can mislead CNN training and cause degraded performance .
Specifically , when the deep model roughly converges to the ground - truth provided by public databases , the network training is misguided by random annotation noises caused by ' semantic ambiguity ' , shown in .
Clearly these noises can make the network training trapped into local minima , leading to degraded results .
In this paper , we propose a novel Semantic Alignment method which reduces the ' semantic ambiguity ' intrinsi-cally .
We assume that there exist ' real ' ground - truths which are semantically consistent and more accurate than human annotations provided by databases .
We model the ' real ' ground - truth as a latent variable to optimize , and the optimized ' real ' ground - truth then supervises the landmark detection network training .
Accordingly , we propose a probabilistic model which can simultaneously search the ' real ' ground - truth and train the landmark detection network in an end - to - end way .
In this probabilistic model , the prior model is to constrain the latent variable to be close to the observations of the ' real ' ground truth , one of which is the human annotation .
The likelihood model is to reduce the Pearson Chi-square distance between the expected and the predicted distributions of ' real ' ground - truth .
The heatmap generated by the hourglass architecture represents the confidence of each pixel and this confidence distribution is used to model the predicted distribution of likelihood .
Apart from the proposed probabilistic framework , we further propose a global heatmap correction unit ( GHCU ) which maintains the global face shape constraint and recovers the unconfidently predicted landmarks caused by challenging factors such as occlusions and low resolution of images .
We conduct experiments on 300W , AFLW and 300 - VW databases and achieve the state of the art performance .
Related work
In recent years , convolutional neural networks ( CNN ) achieves very impressive results on many computer vision tasks including face alignment .
Sun et al proposes to cascade several DCNN to predict the shape stage by stage .
Zhang et al proposes a single CNN and jointly optimizes facial landmark detection together with facial attribute recognition , further enhancing the speed and performance .
The methods above use shallow CNN models to directly regress facial landmarks , which are difficult to cope the complex task with dense landmarks and large pose variations .
To further improve the performance , many popular semantic segmentation and human pose estimation frameworks are used for face alignment .
For each landmark , they predict a heatmap which contains the probability of the corresponding landmark .
Yang et al .
uses a two parts network , i.e. , a supervised transformation to normalize faces and a stacked hourglass network to get prediction heatmaps .
Most recently , JMFA and FAN also achieve the state of the art accuracy by leveraging stacked hourglass network .
However , these methods do not consider the ' semantic ambiguity ' problem which potentially degrades the detection performance .
Two recent works , LAB and SBR , are related to this ' semantic ambiguity ' problem .
By introducing more information than pixel intensity only , they implicitly alle - viate the impact of the annotation noises and improve the performance .
LAB trains a facial boundary heatmap estimator and incorporates it into the main landmark regression network .
LAB uses the well - defined facial boundaries which provide the facial geometric structure to reduce the ambiguities , leading to improved performance .
However , LAB is computational expensive .
SBR proposes a registration loss which uses the coherency of optical flow from adjacent frames as its supervision .
The additional information from local feature can mitigate the impact of random noises .
However , the optical flow is not always credible in unconstrained environment and SBR trains their model on the testing video before the test , limiting its applications .
To summarize , LAB and SBR do not intrinsically address the problem of ' semantic ambiguity ' because the degraded accuracy is actually derived from the inaccurate labels ( human annotations provided by databases ) .
In this work , we solve the ' semantic ambiguity ' problem in a more intrinsic way .
Specifically , we propose a probabilistic model which can simultaneously search the ' real ' ground - truth without semantic ambiguity and train a hourglass landmark detector without using additional information .
Semantic ambiguity
The semantic ambiguity indicates that some landmarks do not have clear and accurate definition .
In this work , we find the semantic ambiguity can happen on any facial points , but mainly on those weak semantic facial points .
For example , the landmarks are defined to evenly distribute along the face contour without any clear definition of the exact positions .
This ambiguity can potentially affect : ( 1 ) the accuracy of the annotations and ( 2 ) the convergence of deep model training .
For ( 1 ) , when annotating a database , annotators can introduce random errors to generate inconsistent ground - truths on those weak semantic points due to the lack of clear definitions .
For ( 2 ) , the inconsistent ground - truths generate inconsistent gradients for back - propagation , leading to the difficulty of model convergence .
In this section , we qualitatively analyze the influence of semantic ambiguity on landmark detection .
Before this analysis , we briefly introduce our heatmapbased landmark detection network .
Specifically , we use a four stage Hourglass ( HGs ) .
It can generate the heatmap which provides the probability of the corresponding landmark located at every pixel , and this probability can facilitate our analysis of semantic ambiguity .
Firstly , we find CNN provides a candidate region rather than a confirmed position fora weak semantic point .
In ( a ) , we can see that the heatmap of a strong semantic point is nearly Gaussian , while the 3D heatmap of a weak semantic point has a ' flat hat ' , meaning that the confidences in that area are very similar .
Since the position with the highest confidence is chosen as the output .
The landmark ( a ) The difference between the heatmap of the eye corner ( strong semantic ) points and the eye contour ( weak semantic ) points .
Col 2 and 3 represent 2 D and 3D heatmaps respectively .
In the 3D Gaussian , the x , y axes are image coordinates and z axis is the prediction confidence .
We can seethe 3D heatmap of a weak semantic point has a ' flat hat ' .
( b )
The predictions from a series of checkpoints after convergence .
When the model has roughly converged , we continue training and achieve the predictions from different iterations .
Red and green dots denote the predicted and annotation landmarks , respectively .
We can seethe predicted landmarks from different checkpoints fluctuate in the neighborhood area of the annotated position ( green dots ) .
Secondly , we analyze the ' semantic ambiguity ' by visualizing how the model is optimized after convergence .
When the network has roughly converged , we continue training the network and save a series of checkpoints .
In , the eyebrow landmarks , from different checkpoints fluctuate along with the edge of eyebrow , which always generates considerable loss to optimize .
However , this loss is ineffectual since the predicted points from different checkpoints also fluctuate in the neighborhood area of the annotated position ( green dots in ) .
It can be concluded that the loss caused by random annotation noises dominate the back - propagated gradients after roughly convergence , making the network training trapped into local minima .
Semantically consistent alignment
In this section , we detail our methodology .
In Section 4.1 , we model the landmark detection problem using a probabilistic model .
To deal with the semantic ambiguity caused by human annotation noise , we introduce a latent variabl y which represents the ' real ' ground - truth .
Then we model the prior model and likelihood in Section 4.2 and 4.3 , respectively .
Section 4.4 proposes an alternative optimization strategy to search ?
and train the landmark detector .
To recover the unconfidently predicted landmarks due to occlusion and low quality , we propose a global heatmap correction unit ( GHCU ) in Section 4.5 , which refines the predictions by considering the global face shape as a constraint , leading to a more robust model .
A probabilistic model of landmark prediction
In the probabilistic view , training a CNN - based landmark detector can be formulated as a likelihood maximization problem :
where o ?
R 2N is the coordinates of the observation of landmarks ( e.g. the human annotations ) .
N is the number of landmarks , x is the input image and Wis the CNN parameters .
Under the probabilistic view of Eq. ( 1 ) , one pixel value on the heatmap works as the confidence of one particular landmark at that pixel .
Therefore , the whole heatmap works as the probability distribution over the image .
As analyzed in Section 3 , the annotations provided by public databases are usually not fully credible due to the ' semantic ambiguity ' .
As a result , the annotations , in particular those of weak semantic landmarks , contain random noises and are inconsistent among faces .
In this work , we assume that there exists a ' real ' ground - truth without semantic ambiguity and can better supervise the network training .
To achieve this , we introduce a latent variable ?
as the ' real ' ground - truth which is optimized during learning .
Thus , Eq. ( 1 ) can be reformulated as :
where o is the observation of ? , for example , the annotation can be seen as an observation of ?
from human annotator .
P ( o|? ) is a prior of ?
given the observation o and P ( ? |x ; W ) is the likelihood .
Prior model of ' real ' ground - truth
To optimize Eq. , an accurate prior model is important to regularize ?
and reduce searching space .
We assume that the kth landmark ?
k is close to the o k , which is the observation of ?.
Thus , this prior is modeled as Gaussian similarity overall {o k , ? k } pairs : where ?
1 can control the sensitivity to misalignment .
To explain o k , we should know in advance that our whole framework is iteratively optimized detailed in Section 4.4 .
o k is initialized as the human annotation in the iteration , and will be updated by better observation with iterations .
Network likelihood model
We now discuss the likelihood P ( ?|x ; W ) of Eq . .
The point - wise joint probability can be represented by the confidence map , which can be modelled by the heatmap of the deep model .
Note that our hourglass architecture learns to predict heatmap consisting of a 2D Gaussian centered on the ground - truth ?
k .
Thus , for any position y , the more the heatmap region around y follows a standard Gaussian , the more the pixel at y is likely to be ? k .
Therefore , the likelihood can be modeled as the distribution distance between the predicted heatmap ( predicted distribution ) and the standard Gaussian region ( expected distribution ) .
In this work , we use Pearson Chi-square test to evaluate the distance of these two distributions :
where E is a standard Gaussian heatmap ( distribution ) , which is a template representing the ideal response ; i is the pixel index ; ?
is a cropped patch ( of the same size as Gaussian template ) from the predicted heatmap centered on y .
Finally , the joint probability can also be modeled as a product of Gaussian similarities maximized overall landmarks :
where k is the landmark index , ?
2 is the bandwidth of likelihood .
To keep the likelihood credible , we first train a network with the human annotations .
Then in the likelihood , we can consider the trained network as a super annotator to guide the searching of the real ground - truth .
It results from the fact that a well trained network is able to capture the statistical law of annotation noise from the whole training set , so that it can generate predictions with better semantic consistency .
Optimization
Combining Eq. , and and taking log of the likelihood , we have :
Reduce Searching Space
To optimize the latent semantically consistent ' real ' landmark ?
k , the prior Eq. indicates that the latent ' real ' landmark is close to the observed landmark o k .
Therefore , we reduce the search space of y k to a small patch centered on o k .
Then , the optimization problem of Eq. can be re-written as :
where N ( o k ) represents a region centered on o k .
Alternative Optimization
To optimize Eq. , an alternative optimization strategy is applied .
In each iteration , y is firstly searched with the network parameter W fixed .
Then ?
is fixed and Wis updated ( landmark prediction network training ) under the supervision of newly searched ?.
Step 1 : When
Wis fixed , to search the latent variable ? , the optimization becomes a constrained discrete optimization problem for each landmark :
where all the variables are known except ? k .
We search ?
k by going through all the pixels in N ( o k ) ( a neighborhood area of o k as shown in ) and the one with minimal loss in Eq. ( 8 ) is the solution .
Since the searching space N ( o k ) is very small , i.e. 17 17 in this work for 256256 heatmap , the optimization is very efficient .
Note that in the prior part of Eq. ( 8 ) , o k is the observation of ? k :
In the 1st iteration , o k is set to the human annotations which are the observation of human annotators ; From the 2nd iteration , o k is set to ?
k t?1 ( where t is the iteration ) .
Note that ?
k t?1 is the estimated ' real ' ground - truth from the last iteration .
With the iterations , ?
kt is converging to the ' real ' ground - truth because both the current observation o k ( i.e. ? k t?1 ) and CNN prediction iteratively become more credible .
Step 2 : When ? is fixed , the optimization becomes :
The optimization becomes atypical network training process under the supervision of ?.
Here ?
is set to the estimate of the latent ' real ' ground - truth obtained in Step 1 .
shows an example of the gradual convergence from the observation o ( ? of the last iteration ) to the estimate of real ground - truth ?.
The optimization of ?
in our semantic alignment can easily converge to a stable position , which does not have hard convergence problem like the traditional landmark training as shown in .
Global heatmap correction unit
Traditional heatmap based methods predict each landmark as an individual task without considering global face shape .
The prediction might fail when the model fits images of low - quality and occlusion as shown in .
The outliers such as occlusions destroy the face shape and significantly reduce overall performance .
Existing methods like local feature based CLM and deep learning based LGCN apply a 2D shape PCA as their post-processing step to remove the outliers .
However , PCA based method is weak to model out - of - plane rotation and very slow ( about 0.8 fps in LGCN ) .
In this work , we propose a Global Heatmap Correction Unit ( GHCU ) to recover the outliers efficiently .
We view the predicted heatmaps as input and directly regress the searched / optimized ?
through alight weight CNN as shown in Tab .
1 . The GHCU implicitly learns the whole face shape constraint from the training data and always gives facialshape landmarks , as shown in .
Our experiments demonstrate the GHCU completes fitting with the speed 8 times faster than PCA on the same hardware platform and achieves higher accuracy than PCA .
Experiments
Datesets .
We conduct evaluation on three challenging datasets including image based 300W , AFLW , and video based 300 - VW .
300 W is a collection of multiple face datasets , including the LFPW , HELEN , AFW and XM2 VTS which have 68 landmarks .
The training set contains 3148 training samples , 689 testing samples which are further divided into the common and challenging subsets .
AFLW is a very challenging dataset which has a wide range of pose variations in yaw ( ?90 to 90 ) .
In this work , we follow the AFLW - Full protocol which ignores two landmarks of ears and use the remaining 19 landmarks .
300 - VW is a large dataset for video - based face alignment , which consists of 114 videos in various conditions .
Following , we utilized all images from 300 W and 50 sequences for training and the remaining 64 sequences for testing .
The test set consists of three categories : well - lit , mild unconstrained and challenging .
Evaluation metric .
To compare with existing popular methods , we conduct different evaluation metrics on different datasets .
For 300 W dataset ,
We follow the protocol in and use Normalized mean errors ( NME ) which normalizes the error by the inter-pupil distance .
For AFLW , we follow to use face size as the normalizing factor .
For 300 - VW dataset , we employed the standard normalized root mean squared error ( RMSE ) which normalizes the error by the outer eye corner distance .
Implementation Details .
In our experiments , all the training and testing images are cropped and resized to 256256 according to the provided bounding boxes .
To perform data augmentation , we randomly sample the angle of rotation and the bounding box scale from Gaussian distribution .
We use a four - stage stacked hourglass network as our backbone which is trained by the optimizer RMSprop .
As described in Section 4 , our algorithm comprises two parts : network training and real groundtruth searching , which are alternatively optimized .
Specifically , at each epoch , we first search the real ground - trut ?
y and then use ?
to supervise the network training .
When training the roughly converged model with human annotations , the initial learning rate is 2.5 10 ?4 which is decayed to 2.5 10 ? 6 after 120 epochs .
When training with Semantic Alignment from the beginning of the aforementioned roughly converged model , the initial learning rate is 2.5 10 ? 6 and is divided by 5 , 2 and 2 at epoch 30 , 60 and 90 respectively .
During semantic alignment , we search the latent variable ?
from a 1717 region centered at the current observation point o , and we crop a no larger than 2525 patch from the predicted heatmap around current position for Pearson Chi - square test in Eq. ( 4 ) .
We set batch size to 10 for network training .
For GHCU , the network architecture is shown in Tab .
1 . All our models are trained with PyTorch [ 18 ] on 2 Titan X GPUs .
Comparison experiment
300 W .
We compare our approach against the state - of the - art methods on 300W in Tab .
2 . The baseline ( HGs in Tab .
2 ) uses the hourglass architecture with human annotations , which is actually the traditional landmark detector training .
From Tab.
2 , we can see that HGs with our Semantic Alignment ( HGs + SA ) greatly outperform hourglass ( HGs ) only , 4.37 % vs 5.04 % in terms of NME on Full set , showing the great effectiveness of our Semantic Alignment ( SA ) .
By adding GHCU , we can see that HGs + SA + GHCU slightly outperforms the HGs + SA .
The improvement is not significant because the images of 300W are of high resolution , while GHCU works particularly well for images of low resolution and occlusions verified in the following evaluations .
Following and which normalize the in - plane - rotation by training a preprocessing network , we conduct this normalization ( HGs + SA + GHCU + Norm ) and achieve state of the art performance on Challenge set and Full set : 6.38 % and 4.02 % .
In particular , on Challenge set , we significantly outperform the state of the art method : 6.38 % ( HGs + SA +GHCU + Norm ) vs 6.98 % ( LAB ) , meaning that our method is particularly effective on challenging scenarios .
AFLW . 300W has 68 facial points which contain many weak semantic landmarks ( e.g. those on face contours ) .
In comparison , AFLW has only 19 points , most of which are strong semantic landmarks .
Since our SA is particularly effective on weak semantic points , we conduct experiments on AFLW to verify whether SA generalizes well to the point set , most of which are strong semantic points .
For fair comparison , we do not compare methods using additional outside training data , e.g.
LAB used additional boundary information from outside database .
As shown in Tab. 3 , HGs + SA outperforms
HGs , 1.62 % vs 1.95 % .
It means that even though corner points are easily to be recognized , there is still random error in annotation , which can be corrected by SA .
It is also observed that HGs + SA + GHCU works better than HGs + SA .
300 - VW .
Unlike the image - based databases 300 W and AFLW , 300 - VW is video - based database , which is more challenging because the frame is of low resolution and with strong occlusions .
The subset Category 3 is the most challenging one .
From Tab.
4 , we can see that HGs + SA greatly outperforms HGs in each of these three test sets .
Furthermore , compared with HGs + SA , HGs + SA + GHCU reduce the error rate ( RMSE ) by 18 % on Category 3 test set , meaning that GHCU is very effective for video - based challenges such as low resolution and occlusions because .
Comparison with state of the art on AFLW dataset .
The error ( NME ) is normalized by the face bounding box size .
AFLW - Full ( % ) LBF 4.25 CFSS 3.92 CCL ( CVPR16 ) 2.72 TSR ( CVPR17 ) 2.17 DCFE ( ECCV18 ) 2.17 SBR ( CVPR18 ) 2.14 DSRN ( CVPR18 ) 1.86 Wing ( CVPR18 ) 1.65 HGs 1.95 HGs + SA 1.62 HGs + SA + GHCU 1.60
GHCU considers the global face shape as constraint , being robust to such challenging factors .
Self evaluations
Balance of prior and likelihood As shown in Eq. ( 6 ) , the ' real ' ground - truth is optimized using two parts : prior and likelihood , where ? 1 and ?
2 determine the importance of these two parts .
Thus , we can use one parameter ? 2 2 /?
2 1 to estimate this importance weighting .
We evaluate different values of ? 2 2 /? 2 1 in Tab .
5 . Clearly , the performance of ? 2 2 /? 2 1 = 0 ( removing Semantic Alignment and using human annotations only ) is worst , showing the importance of the proposed Semantic Alignment .
We find that ? 2 2 /? 2 1 = 0.1 achieves the best performance , meaning that the model relies much more ( 10 times ) on prior than likelihood to achieve the best trade - off .
Template size .
As discussed in the Section 3 , fora position y , the similarity between the heatmap region around it and standard Gaussian template is closely related to the detection confidence .
Therefore , the size of the Gaussian template , which is used to measure the network confidence in Eq. ( 5 ) , can affect the final results .
reports the results under different template sizes using the model HGs + SA .
Too small size ( size = 1 ) means that the heatmap value is directly used to model the likelihood instead of Chi-square test .
Not surprisingly , the performance with size = 1 is not promising .
Large size ( size = 25 ) introduces more useless information , degrading the performance .
In our experiment , we find size = 15 for AFLW and size = 19 for 300 W can achieve the best result .
Analysis of the training of semantic alignment .
To verify the effectiveness of Semantic Alignment , we train a baseline network using hourglass under the supervision of human annotation to converge .
Use this roughly converged baseline , we continue training using 3 strategies as shown in : baseline , SA w / o update ( always using human annotation as the observation , see Eq. ) and SA ( the observation is iteratively updated ) .
visualize the changes of training loss and NME on test set against the training epochs , respectively .
We can see that the baseline curve in do not decrease because of the ' semantic ambiguity ' .
By introducing SA , the training loss and test NME steadily drop .
Obviously , SA reduces the random optimizing directions and helps the roughly converged network to further improve the detection accuracy .
We also evaluate the condition that uses semantic alignment without updating the observation o ( ' SA w/ o update ' in .
It means o is always set to the human annotations .
We can see that the curve of ' SA w/ o update ' can be further optimized but quickly trapped into local optima , leading to worse performance than SA .
We assume that the immutable observation o reduces the capacity of searching ' real ' ground - truth ?. after each epoch .
To explore the effects of the number of epochs on model convergence , we train different models by stopping semantic alignment at different epochs .
In it is observed that the final performance keeps improving with the times of semantic alignment , which demonstrates that the improvement is highly positive related to the quality of the learned ?.
From our experiment , 10 epochs of semantic alignment are enough for our data sets .
Quality of the searched ' real ' ground - truth .
One important assumption of this work is that there exist ' real ' ground - truths which are better than the human annotations .
To verify this , we train two networks which are supervised by the human annotations provided by public database and the searched ' real ' ground - truth , respectively .
These two detectors area Hourglass model ( HGs ) and a ResNet model which directly regresses the landmark coordinates as .
As shown in Tab .
7 , we can see that on both models the ' real ' ground - truth ( SA ) outperforms the human annotations ( HA ) .
Clearly , our learned labels are better than the human annotations , verifying our assumption that the se-mantic alignment can find the semantic consistent groundtruths .
Global heatmap correction unit .
The 2D shape PCA can well keep the face constraint and can be conducted as a post-processing step to enhance the performance of heatmap based methods , like CLM and most recently LGCN .
We apply the powerful PCA refinement method in LGCN and compare it with our GHCU .
We evaluate on 300 - VW where the occlusion and low - quality are particularly challenging .
As shown in Tab. 8 , our CNN based GHCU outperforms PCA based method in terms of both accuracy and efficiency .
Ablation study .
To verify the effectiveness of different components in our framework , we conduct this ablation study on 300 - VW .
For a fair comparison , all the experiments use the same parameter settings .
As shown in Tab .
9 , Semantic alignment can consistently improve the performance on all subset sets , demonstrating the strong generalization capacity of SA .
GHCU is more effective on the challenge data set ( Category 3 ) : 8.15 % vs 9.91 % ; Combining SA and GHCU works better than single of them , showing the complementary of these two mechanisms .
Conclusion
In this paper , we first analyze the semantic ambiguity of facial landmarks and show that the potential random noises of landmark annotations can degrade the performance considerably .
To address this issue , we propose a a novel latent variable optimization strategy to find the semantically consistent annotations and alleviate random noises during training stage .
Extensive experiments demonstrated that our method effectively improves the landmark detection accuracy on different data sets .
