title
DeCaFA : Deep Convolutional Cascade for Face Alignment In The Wild
abstract
Face Alignment is an active computer vision domain , that consists in localizing a number of facial landmarks that vary across datasets .
State - of - the - art face alignment methods either consist in end - to - end regression , or in refining the shape in a cascaded manner , starting from an initial guess .
In this paper , we introduce DeCaFA , an end - to - end deep convolutional cascade architecture for face alignment .
DeCaFA uses fully - convolutional stages to keep full spatial resolution throughout the cascade .
Between each cascade stage , DeCaFA uses multiple chained transfer layers with spatial softmax to produce landmark - wise attention maps for each of several landmark alignment tasks .
Weighted intermediate supervision , as well as efficient feature fusion between the stages allow to learn to progressively refine the attention maps in an end - to - end manner .
We show experimentally that DeCaFA significantly outperforms existing approaches on 300W , CelebA and WFLW databases .
In addition , we show that DeCaFA can learn fine alignment with reasonable accuracy from very few images using coarsely annotated data .
Introduction
Face alignment consists in localizing landmarks ( e.g. lips and eyes corners , pupils , nose tip ) on an face image .
It is an important computer vision field , as it is an essential preprocess for face recognition , tracking , expression analysis , and face synthesis .
Most recent face alignment approaches either belongs to cascaded regression methods , or to deep end - to - end regression methods .
On the one 's hand , cascaded regression consists in learning a sequence of updates , starting from an initial guess , to refine the landmark localization in a coarse - to - fine manner .
This allows to robustly learn rigid transformations , such as translation and rotation , in the first cascade stages , while learning non-rigid deformation ( e.g. due to facial expression or non-planar rotation ) later on .
On the other hand , many deep approaches aim at regressing the landmark position from the original image directly .
However , because annotating several landmarks on a face image is a tedious task , data is rather scarce and the nature of the annotations usually vary a lot between the databases .
Because of the scarcity of the data , end - to - end approaches usually rely on learning an intermediate representation , such as edges detection to drive the alignment process .
However , these representations are usually ad hoc and do not guarantee to be optimal to address landmark localization tasks .
In this paper , we introduce a Deep convolutional Cascade for Face Alignment ( DeCaFA ) .
DeCaFA is composed of several stages that each produce landmark - wise attention maps , relatively to heterogeneous annotation markups .
shows attention maps extracted by the subsequent DeCaFA stages ( horizontally ) and for three different markups ( vertically ) .
It illustrates how these attention maps are refined through the successive stages , and how the different prediction tasks can benefit from each other .
The contributions of this paper are tree - fold :
We introduce a fully - convolutional Deep Cascade for Face Alignment ( DeCaFA ) that unifies cascaded regression and end - to - end deep approaches , by using landmark - wise attention maps fused to extract local information around a current landmark estimate .
We show that intermediate supervision with increasing weights helps DeCaFA to learn coarse attention maps in its early stages , that are refined in the later stages .
Through chaining multiple transfer layers , DeCaFA integrates heterogeneous data annotated with different numbers of landmarks and model the intrinsic relationship between these tasks .
We show experimentally that DeCaFA significantly outperforms existing approaches on multiple datasets , inluding the recent WFLW database .
Additionally , we highlight how coarsely annotated data helps the network to learn fine landmark alignment even with very few annotated images .
Related work
Popular examples of cascaded regression methods include SDM : in their pioneering work ,
Xiong et al show that using simple linear regressors upon SIFT features in a cascaded manner already provides satisfying alignment results .
LBF [ 14 ] is a refinement that employs randomized decision trees to dramatically speedup feature extraction .
DAN uses deep networks to learn each cascade stage .
However , one downside of these approaches is that the update regressors are not learned jointly in a end - to - end fashion , thus there is no guarantee that the learned feature point alignment sequences might be optimal .
MDM improves the feature extraction process by sharing the convolutional layer among all steps of the cascade that are performed through a recurrent neural network .
This results in memory footprint reduction as well as better representation learning and a more optimized landmark trajectory throughout the cascade .
TCDCN was perhaps the first end - to - end framework that could compete with cascaded regression approaches .
It relies on supervised pretraining on a wide database of facial attributes .
More recently , PCD - CNN uses head pose information to drive the training process .
CPM + SBR employs landmark registration to regularize training .
SAN uses adversarial networks to convert images from different styles to an aggregated style , upon which regression is performed .
This aggregated style space thus serve as an intermediate representation that is more convenient for training .
In the authors propose to use edge map estimation as an intermediate representation to drive the landmark prediction task , as well as to provide a unified representation when images are annotated in terms of different markups , that correspond to different alignment tasks .
Finally , DSRN relies on Fourier Embedding and low - rank learning to produce such representation .
However , the use of such intermediate representation is usually ad hoc and it is hard to know which one would be all - around better for face alignment .
Recently , AAN proposes to use intermediate feature maps as attentional masks to select relevant spatial regions .
It also uses intermediate supervision to constrain those maps to correspond to attention maps relatively to landmark localization .
However , there is no guarantee that the network will learn to align landmarks in a cascaded , coarse - to - fine manner .
Furthermore , annotating images in term of several face landmarks is a time - consuming task .
As a result , data is rather scarce and annotated in terms of varying number of landmarks .
For instance , 300W database contains approximately 3000 images labelled with 68 landmarks for train , whereas WFLW database contains 7500 images with 98 landmarks .
Thus , one can wonder if we can use all those images within the same framework to learn more robust landmark predictions .
In the authors address this problem by using a classical multi-task formulation .
However , this essentially ignores the intrinsic relationship between the structure of different landmark alignment tasks .
Likewise , if we can predict the position of 68 landmarks , we can also easily deduce the position of landmarks fora coarser markup , such as eye / mouth corners and nose tip .
DeCaFA overview
In this Section , we introduce our Deep convolutional Cascade for Face Alignment ( DeCaFA ) model , as illustrated on .
DeCaFA consists of S stages , each of which contains a fully - convolutional U - net backbone that preserves the full spatial resolution , as well as an attention map generation sub-network .
Section 3.1 shows how we derive landmarkwise attention maps for one landmark prediction task .
Section 3.2 explains how several transfer layers can be chained to produce such attention maps , relatively to K landmark prediction tasks .
the input of the next stage is obtained by applying a feature fusion algorithm that involves the attention maps , as explained in Section 3.3 .
In Section 3.4 we describe how DeCaFA is trained in an end - to - end manner with weighted intermediate supervision .
Finally , in Section 3.5 we provide implementation details to facilitate reproducibility of the results .
Landmark - wise attention maps
The U - net at stage i takes an input I i and gives rise to an embedding H i with parameters ?
i .
In order to produce a suitable embedding from H i for predicting L landmarks , we apply a 1 1 convolutional layer with L filters with : DeCaFA architecture overview .
Several stages with fully - convolutional U-nets are stacked , multiple transfer layers are chained and intermediate supervision with increasing weights is applied to produce landmark estimates for heterogeneous alignment tasks .
Landmark - wise attention maps are fused with the input image and the embeddings of the previous stage U - net to enable end - to - end cascaded alignment .
parameters ?
i .
We denote the embeddings outputted by this transfer layer as T Li .
In order to highlight its dominant mode we apply a spatial softmax operator .
Formally , fora pixel with coordinates ( x , y) and a landmark l:
An estimation ?
Li of the landmark coordinates can be obtained by computing the first order moments of ?
Li :
Where ?
L i , x and ?
L i , y are two vectors of size L containing the x and y landmark coordinates ?
Li .
The soft - argmax operator is inspired by the work in in the frame of human pose estimation and provides differentiable landmark coordinates estimate from the attention map ?
Li .
Chaining landmark localization tasks
As it will be explained in Section 4.1 , existing datasets for face alignment usually have heterogeneous annotations and varying numbers of annotated landmarks .
In order to deal with these heterogeneous annotations , we integrate K tasks that consist in predicting various numbers of landmarks
we chain the landmark - wise attention maps in an decreasing order of the number of landmarks to predict ) .
To do so , we apply K transfer layers T L1 i , ... ,
, at it is depicted on ( a ) .
We have :
The advantages of stacking the landmarks prediction pipelines in a descending order of the number of landmarks to be localized are two - fold :
First , from a semantic perspective , who can do more can do less , meaning that it shall be easier for the network to learn the sequence of transfer layers in that order ( i.e. if we can precisely localize a 68 - points markup it will be easy to also localize the nose tip , as well as mouth / eyes corners ) .
Second , labelling images with large amounts of landmarks is a tedious task , thus generally the more annotated landmarks in a database , the less images we have at our disposal .
Using such architecture ensures that the former ( harder ) tasks benefit from all the images annotated with the latter ( easier ) task .
This can be seen as weakly supervised learning , where images labelled in terms of coarse markups can help to learn finer alignment tasks .
Also note that as these 1 1 convolutional layers have very few parameters , thus a lot of gradient can be backpropagated down to the U - net backbone and benefit the K prediction tasks .
Finally , as illustrated on , we use attention maps ?
L k 0 i from markup k 0 to provide richer embeddings for the subsequent stages by applying feature fusion .
Feature fusion
Ina standard feedforward deep network with S stacked stages , the i + 1 th stage takes an input I i = F 1 that corresponds to the embeddings H i outputted by the previous stage ( with the convention I 0 = I the original image ) .
By contrast , in cascade - based approaches , each stage shall learn an update to bring the feature points closer to the ground truth localizations , by using information sampled around current feature point localizations .
Within an end - to - end fully - convolutional deep network , an analogous statement would be that the i + 1 th stage shall use a local embedding F 2 that is calculated using information from the original image I highlighted by landmark - wise attention maps ?
In our method , we aggregate these maps by summing all the landmark - wise attention maps
.
Thus , we can write the feature fusion model for the basic deep approach as :
and the cascade - like approach as :
Where denotes the Hadamard product .
This fusion scheme between the input image and the mask only preserves local information , for which the values of M i are high .
Alternatively , we can reinject the original image I inside each stage so that it can use global information in case where the mask M i is not precise enough or contains localizations errors ( as it is the case early in the cascade ) :
With || the channel - wise concatenation operation .
Furthermore we can also fuse the relevant parts ( as highlighted by mask M i ) of the embedding H i of the previous stage U - net to provide the subsequent stages a richer , more semantically abstract information to estimate the landmarks coordinates :
Finally , we can aso use global information from not only the image I , but also from the embeddings H i :
This fusion model is more efficient and is used in De - CaFA ) , as it allows using both global and local information around the estimated landmarks so as to learn cascade - like alignment in an end - to - end fashion .
Learning DeCaFA model
DeCaFA models can be trained end - to - end by optimizing the following loss function w.r.t. parameters of the U-nets ?
i and ?
With z L k * the ground truth landmark position fora L klandmarks markup .
In practice , the summation in equation have less terms since usually each example is annotated with only one markup .
With this configuration , however , if the whole network is deep enough , few gradient will ever pass through the firsts attention maps .
Even worse , there is no guarantee that these feature maps will correspond to landmark - wise attention maps in the early stages , which is key to ensure cascade - like behavior of DeCaFA .
To ensure this , we add a differentiable soft - argmax layer after each spatial softmax and a supervised cost at stage i:
In practice , we use a L 1 loss function , as it has been shown to overfit lesson very bad examples and lead to more precise results for face alignment .
However , we need to make sure that the ( relatively ) shallow sub-networks does not overfit on these losses , which would result in very narrow heat maps with very localized dominant modes early in the cascade , and thus an overall lower accuracy .
This is ensured by applying increasing ?
i weights in ( 10 ) .
Implementation details
The DeCaFA models that will be investigated below use 1 to 4 stages that each contains 12 3 3 convolutional layers with 64 ? 64 ? 128 ? 128 ? 256 ? 256 channels for the downsampling portion , and vice - versa for the upsampling portion .
The input images are resized to 128 128 grayscale images prior to being processed by the network .
Each convolution is followed by a batch normalization layer with ReLU activation .
In order to generate smooth feature maps we do not use transposed convolution but bilinear image upsampling followed with 3 3 convolutional layers .
The whole architecture is trained using ADAM optimizer with a 5e ? 4 learning rate with momentum 0.9 and learning rate annealing with power 0.9 .
We apply 400000 updates with batch size 8 for each database , with alternating updates between the databases .
Experiments
Datasets
The 300W database , introduced in , is considered as the benchmark dataset for training and testing face alignment models , with moderate variations in head pose , facial expressions and illuminations .
It consists in four databases : LFPW ( 811 images for train / 224 images for test ) , HELEN ( 2000 images for train / 330 images for test ) , AFW ( 337 images for train ) and IBUG ( 135 images for test ) , fora total of 3148 images annotated with 68 landmarks for training the models .
For comparison with state - of - the art methods , we refer to LFPW and HELEN test sets as the common subset and I - BUG as the challenging subset of 300W .
The Celeb
A database is a large - scale face attribute database which contains 202599 celebrity images coming from 10177 identities , each annotated with 40 binary attributes and the localization of 5 landmarks ( nose , left and right pupils , mouth corners ) .
In our experiments , we train our models using the train partition that contains 162770 images from 8 k identities .
The test partition contains 19962 instances from 1 k identities that are not seen in the train set .
The Wider Facial Landmarks in the Wild or WFLW database contains 10000 faces ( 7500 for training and 2500 for testing ) with 98 annotated landmarks .
This database also features rich attribute annotations in terms of occlusion , head pose , make - up , illumination , blur and expressions .
In what follows , we train our models using the train partition of 300W , WFLW and CelebA , and evaluate of the test partition of these datasets .
As in we measure the average point - to - point distance between feature points ( ME ) , normalized by the inter-ocular distance ( distance between outer eye corners on ground truth markup ) .
As there is no consensus on how to measure the error we also report AUC and failure rates fora maximum error of 0.1 , as well as cumulative error distribution ( CED ) curves .
Ablation study
In this section , we validate the architecture and hyperparameters of our model : the number of stages S , the number of landmark prediction tasks K , the fusion and task ordering scheme as well as the intermediate supervision weights .
shows CED curves for models with S = 1 , 2 , 3 and 4 cascade stages .
The accuracy steadily increases as we add more stages , and saturates after the third on LFPW and HELEN , which is a well - known behavior of cascaded models , showing that DeCaFA with weighted intermediate supervision indeed works as a cascade , by first providing coarse estimates and refining in the later stages .
On IBUG , this difference is more conspicuous , thus there is for improvement by stacking more cascade stages .
shows the interest of chaining multiple tasks , most notably on LFPW , that contains low - resolution images , and IBUG , which contains strong head pose variations as well as occlusions .
Coarsely annotated data ( 5 landmarks ) significantly helps the fine - grained landmark localization , as it is integrated a kind of weakly supervised scheme .
This will be discussed more thoroughly in Section 4.4 .
shows a comparison between multiple fusion , task ordering and intermediate supervision weighting schemes .
We test our model on 300W ( full and challenging ) , WFLW ( All and challenging , i.e. pose subset ) as well as CelebA and report the average accuracy on those 5 subsets .
First , reinjecting the whole input image ( F 3 - Equation vs F 2 - Equation ) significantly improves the accuracy on challenging data such as 300 W - challenging or WFLW - pose , where the first cascade stages may commit errors .
F 4 - Equation ( 7 ) and F 3 fusion ( cascaded models ) using local + global information rivals the basic deep approach F 1 - Equation ( 4 ) .
Furthermore , F 5 - Equation fusion , which uses local and global cues is the best by a significant margin .
Furthermore , chaining the transfer layers is better than using independant transfer layers : likewise , in such a case , the first transfer layer benefits from the gradients from the subsequents layer at train time .
Last but not least , using increasing intermediate supervision weights in Equation ( 10 ) ( i.e. ? 1 = 1 /8 , ? 2 = 1 /4 , ? 3 = 1 /2 , ? 4 = 1 ) is better than both using constant weights ( ? 1 = ? 2 = ? 3 = ? 4 = 1 ) and decreasing weights (? 1 = 1 , ? 2 = 1 /2 , ? 3 = 1 / 4 , ? 4 = 1 / 8 ) , as it enables proper cascade - like training of the network , with the first stage outputting coarser attention maps that can be refined in the latter stages of the network .
shows a comparison between DeCaFA and recent state - of - the - art approaches on 300W database .
Our approach performs better than most existing approaches on the common subset , and performs very close to its best contenders on the challenging subset .
Note that DeCaFA trained only on 300 W trainset has a ME of 3.69 % and is already very competitive with recent approaches , thanks to its end - to - end cascade architecture .
DeCaFA is competitive with the best approaches , LAB and DAN - MENPO as well as JMFA - MENPO , which also use external data .
shows a comparison between our method and LAB on WFLW database .
As in we report the aver-age point - to - point error on WFLW test partition , normalized by the outer eye corners .
We also report the error on multiple test subsets containing variations in head pose , facial expressions , illumination , make - up as well as partial occlusions and occasional blur .
DeCaFA performs better than LAB and Wing by a significant margin on every subset .
Also , note that DeCaFA trained solely on WFLW already as a ME of 5.01 on the whole test set , which is still better that these two methods .
Lastly , there is room for improvement on this benchmark as we do not excplicitely handle any of the factors of variation such as pose or occlusions .
Comparisons with state - of - the - art methods
Finally , shows a comparison of our method and state - of - the - art approaches on Celeb A .
As in we report the average point - to - point error on the test partition , normalized by the distance between the two eye centers .
Our 49.87 5.08 Densereg + MDM 52 . 3.67 JMFA 54.9 1.00 JMFA- MENPO 60.7 0.33 LAB 58.9 0.83 DeCaFA 0.661 0.15 : Comparison with state - of - the - art approaches on CelebA database ( lower is better ) .
Method Mean error ( % ) SDM 4.35 CFSS 3,95 DSRN 3.08 AAN 2.99 DeCaFA 2.10 approach is the best by a significant margin .
Noteworthy , even though we use auxiliary data from 300 W and WFLW , we do not use data from the val partition of CelebA , contrary to , thus there is significant room for improvement .
Overall , DeCaFA sets a new state - of - the - art on the three databases with several evaluation metrics .
Also notice that it embraces few parameters ( ?
10M ) compared to state - of - theart approaches , and is also modular : attest time , DeCaFA allows to find a good trade - of between speed and accuracy ( by evaluating only a fraction of the cascade ) , as well as to predict various numbers of landmarks .
Weakly supervised learning
In this Section , we study the capability of DeCaFA to learn with few examples annotated with 68 and 98 - landmarks .
To do so , we train DeCaFA using only a small , randomly sampled fraction of 300W ( 100 and 500 images , 3 % and 15 % of trainset ) and WFLW ( 100 and 500 images , 1 % and 6 % of trainset ) and the whole CelebA trainset , and report results on 300 W and WFLW testsets on .
Using coarsely annotated data from Celeb
A allows to substantially improve the landmark localization on both datasets , most notably when the number of training images is very low .
For instance , DeCaFA trained with 3 % of 300 W trainset and 1 % of WFLW trainset already outputs decent fine - grained landmark estimations , as it is better than CFSS and DVLN ( , see ) on WFLW .
DeCaFA trained with 15 % of 300 W trainset and 6 % of WFLW trainset is on par with SAN on 300W ( , see ) , and is substantially better than DVLN on WFLW .
This indicates that weakly
Qualitative results
Image 7 shows vizualisations of aligned facial landmarks on WFLW , I - BUG and CelebA , as well as visualizations of attention maps after the cascade stages 1 and 4 .
Notice how these attention maps are coarse after stage 1 and get refined after stage 4 , better highlighting the individual landmarks .
Also notice that the predicted landmarks are close to the corresponding ground truth , even in the presence of rotations and occlusions ( WFLW ) or facial expressions ( CelebA ) .
Conclusion
In this paper , we introduced DeCaFA for face alignment .
DeCaFA unifies cascaded regression approaches and an endto - end trainable deep methods .
It s fully - convolutional U - net backbone ensures to keep full spatial resolution throughout the network , and the intermediate supervisions between the cascade stages with increasing weights ensures that the network learns cascaded alignment .
Furthermore , by chaining multiple transfer layers to produce attention maps that correspond to multiple alignment tasks , DeCaFA can benefit from heterogeneous data .
We empirically show that DeCaFA significantly outperforms state - of - the - art approaches on 300W , CelebA and WFLW databases .
In addition , DeCaFA architecture is very modular and is suited for weakly supervised learning using coarsely annotated data with few landmarks .
Future work will consist in integrating other sources of data , or possibly other representations and tasks , such as head pose estimation , partial occlusion handling , as well as facial expressions , Action Unit and / or attributes ( such as age or gender estimation ) recognition within DeCaFA framework .
Furthemore , we will study the application of DeCaFA to closely related fields , such as human pose estimation . :
From left to right : images , attention maps outputted by stages 1 and 4 , alignment results , and ground truth for images from 300W ( I -bug , 68 landmarks ) and WFLW ( 98 landmarks ) .
Notice how the summed attention maps are iteratively refined , and how closely the predicted landmarks usually matches the ground truth , even under difficult illumination , non-frontal head poses , make - up , or occlusions .
