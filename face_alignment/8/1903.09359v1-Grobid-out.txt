title
Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image with 2D-Assisted Self-Supervised Learning
abstract
: Dense face alignment (odd rows) and 3D face reconstruction (even rows) results from our proposed method. For alignment, only 68 key points are plotted for clear display; for 3D reconstruction, reconstructed shapes are rendered with headlight for better view. Our method offers strong robustness and good performance even in presence of large poses (the 3th, 4th and 5th columns) and occlusions (the 6th, 7th and 8th columns). Best viewed in color.
3D face reconstruction from a single 2D image is a challenging problem with broad applications. Recent methods typically aim to learn a CNN-based 3D face model that regresses coefficients of 3D Morphable Model (3DMM) from 2D images to render 3D face reconstruction or dense face alignment. However, the shortage of training data with 3D annotations considerably limits performance of those methods. To alleviate this issue, we propose a novel 2D-assisted self-supervised learning (2DASL) method that can effectively use "in-the-wild" 2D face images with noisy landmark information to substantially improve 3D face model learning. Specifically, taking the sparse 2D facial landmarks as additional information, 2DSAL introduces four novel self-supervision schemes that view the 2D landmark and 3D landmark prediction as a self-mapping process, including the 2D and 3D landmark self-prediction consistency, cycle-consistency over the 2D landmark prediction and self-critic over the predicted 3DMM coefficients based on landmark predictions. Using these four self-supervision schemes, the 2DASL method significantly relieves demands on the the conventional paired 2D-to-3D annotations and gives much higher-quality 3D face models without requiring any additional 3D annotations. Experiments on multiple challenging datasets show that our method outperforms state-of-the-arts for both 3D face reconstruction and dense face alignment by a large margin.
narrative
: Dense face alignment (odd rows) and 3D face reconstruction (even rows) results from our proposed method. For alignment, only 68 key points are plotted for clear display; for 3D reconstruction, reconstructed shapes are rendered with headlight for better view. Our method offers strong robustness and good performance even in presence of large poses (the 3th, 4th and 5th columns) and occlusions (the 6th, 7th and 8th columns). Best viewed in color.

Abstract
3D face reconstruction from a single 2D image is a challenging problem with broad applications. Recent methods typically aim to learn a CNN-based 3D face model that regresses coefficients of 3D Morphable Model (3DMM) from 2D images to render 3D face reconstruction or dense face alignment. However, the shortage of training data with 3D annotations considerably limits performance of those methods. To alleviate this issue, we propose a novel 2D-assisted self-supervised learning (2DASL) method that can effectively use "in-the-wild" 2D face images with noisy landmark information to substantially improve 3D face model learning. Specifically, taking the sparse 2D facial landmarks as additional information, 2DSAL introduces four novel self-supervision schemes that view the 2D landmark and 3D landmark prediction as a self-mapping process, including the 2D and 3D landmark self-prediction consistency, cycle-consistency over the 2D landmark prediction and self-critic over the predicted 3DMM coefficients based on landmark predictions. Using these four self-supervision schemes, the 2DASL method significantly relieves demands on the the conventional paired 2D-to-3D annotations and gives much higher-quality 3D face models without requiring any additional 3D annotations. Experiments on multiple challenging datasets show that our method outperforms state-of-the-arts for both 3D face reconstruction and dense face alignment by a large margin.

Introduction
3D face reconstruction is an important task in the field of computer vision and graphics. For instance, the recovery of 3D face geometry from a single image can help address many challenges (e.g., large pose and occlusion) for 2D face alignment through dense face alignment. Traditional 3D face reconstruction methods are mainly based on optimization algorithms, e.g., iterative closest point, to obtain coefficients for the 3D Morphable Model (3DMM) model and render the corresponding 3D faces from a single face image. However, such methods are usually time-consuming due to the high optimization complexity and suffer from local optimal solution and bad initialization. Recent works thus propose to use CNNs to learn to regress the 3DMM coefficients and significantly improve the reconstruction quality and efficiency.
CNN-based methods have achieved remarkable success in 3D face reconstruction and dense face alignment. However, obtaining an accurate 3D face CNN regression model (from input 2D images to 3DMM coefficients) requires a large amount of training faces with 3D annotations, which are expensive to collect and even not achievable in some cases. Even some 3D face datasets, like 300W-LP, are publicly available, they generally lack diversity in face appearance, expression, occlusions and environment conditions, limiting the generalization performance of resulted 3D face regression models. A model trained on such datasets cannot deal well with various potential cases in-the-wild that are not present in the training examples. Although some recent works bypass the 3DMM parameter regression and use image-to-volume or image-to-image strategy instead, the ground truths are all still needed and generated from 3DMM using 300W-LP, still lacking diversity.
In order to overcome the intrinsic limitation of existing 3D face recovery models, we propose a novel learning method that leverages 2D "in-the-wild" face images to effectively supervise and facilitate the 3D face model learning. With the method, the trained 3D face model can perform 3D face reconstruction and dense face alignment well. This is inspired by the observation that a large number of 2D face datasets are available with obtainable 2D landmark annotations, that could provide valuable information for 3D model learning, without requiring new data with 3D annotations.
Since these 2D images do not have any 3D annotations, it is not straightforward to exploit them in 3D face model learning. We design a novel self-supervised learning method that is able to train a 3D face model with weak supervision from 2D images. In particular, the proposed method takes the sparse annotated 2D landmarks as input and fully leverage the consistency within the 2Dto-2D and 3D-to-3D self-mapping procedure as supervi-sion. The model should be able to recover 2D landmarks from predicted 3D ones via direct 3D-to-2D projection. Meanwhile, the 3D landmarks predicted from the annotated and recovered 2D landmarks via the model should be the same. Additionally, our proposed method also exploits cycle-consistency over the 2D landmark predictions, i.e., taking the recovered 2D landmarks as input, the model should be able to generate 2D landmarks (by projecting its predicted 3D landmarks) that have small difference with the annotated ones. By leveraging these self-supervision derived from 2D face images without 3D annotations, our method could substantially improve the quality of learned 3D face regression model, even though there is lack of 3D samples and no 3D annotations for the 2D samples. To facilitate the overall learning procedure, our method also exploits self-critic learning. It takes as input both the latent representation and 3DMM coefficients of an face image and learns a critic model to evaluate the intrinsic consistency between the predicted 3DMM coefficients and the corresponding face image, offering another supervision for 3D face model learning.
Our proposed method is principled, effective and fully exploits available data resources. As shown in, our method can produce 3D reconstruction and dense face alignment results with strong robustness to large poses and occlusions. Our code, models and online demos will be available upon acceptance. Our contributions are summarized as follows:
• We propose anew scheme that aims to fully utilize the abundant "in-the-wild" 2D face images to assist 3D face model learning. This is new and different from most common practices that pursues to improve 3D face model by collecting more data with 3D annotations for model training.
• We introduce anew method that is able to train 3D face models with 2D face images by self-supervised learning. The devised multiple forms of self-supervision are effective and data efficient.
• We develop anew self-critic learning based approach which could effectively improve the 3D face model learning procedure and give a better model, even though the 2D landmark annotations are noisy.
• Comparison on the AFLW2000-3D and AFLW-LFPA datasets shows that our method achieves excellent performance on both tasks of 3D face reconstruction and dense face alignment.

Related work
3D Face Reconstruction Various approaches have been proposed to tackle the inherently ill-posed problem of 3D face reconstruction from a single image. In, Vetter and Blanz observe that both the geometric structure and the texture of human faces can be approximated by a linear combination of orthogonal basis vectors obtained by PCA over 100 male and 100 female identities. Based on this, they propose the 3DMM to represent the shape and texture of a 3D face. After that, large amount of efforts have been proposed to improve 3DMM modeling mechanism. Most of them devote to regressing the 3DMM coefficients by solving the non-linear optimization problem to establish the correspondences of the points between a single face image and the canonical 3D face model, including facial landmarks and local features. Recently, various attempts have been made to estimate the 3DMM coefficients from a single face image using CNN as a regressor, as opposed to non-linear optimization. In, cascaded CNN structures are used to regress the 3DMM coefficients, which are time-consuming due to multi-stage. Besides, end-to-end approaches are also proposed to directly estimate the 3DMM coefficients in a holistic manner. More recently, works are proposed to use CNN directly obtain the reconstructed 3D face bypassing the 3DMM coefficients regression. In, Jackson et al. propose to map the image pixels to a volumetric representation of the 3D facial geometry through CNN-based regression. While their method is not restricted to the 3DMM space anymore, it needs a complex network structure and a lot of time to predict the voxel information. Ina later work, Feng et al. store the 3D facial geometry into UV position map and train an imageto-image CNN to directly regress the complete 3D facial structure along with semantic information from a single image.
Face Alignment Traditional 2D face alignment methods aim at locating a sparse set of fiducial facial landmarks. Initial progresses have been made with the classic Active Appearance Model (AAM) and Constrained Local Model (CLM). Recently, CNN-based methods have achieved state-of-the-art performance on 2D landmark localization. However, 2D face alignment only regresses visible landmarks on faces, which are unable to address large pose or occlusion situations, where partial face regions are invisible. With the development of this field, 3D face alignment have been proposed, aiming to fit a 3DMM or register a 3D facial template to a 2D face image, which makes it possible to deal with the invisible points. The original 3DMM fitting method fits the 3D model by minimizing the pixel-wise difference between image and the rendered face model. It is the first method that can address arbitrary poses, which, however, suffers from the one-minute-per-image computational cost. After that, some methods estimate 3DMM coefficients and then project the estimated 3D landmarks onto 2D space, such methods could significantly improve the efficiency. Recently, the task of dense face alignment starts to attract more and more research attention, aiming to achieve very dense 3D alignment for large pose face images (including invisible parts). In, Liu et al. use multi-constraints to train a CNN model, jointly estimating the 3DMM coefficient and provides very dense 3D alignment. directly learn the correspondence between a 2D face image and a 3D template via a deep CNN, while only visible face-region is considered.
Overall, CNN-based methods have achieved great success in both 3D face reconstruction and dense face alignment. However, they need a huge amount of 3D annotated images for training. Unfortunately, currently face datasets with 3D annotations are very limited. As far as we know, only the 300W-LP dataset has been widely used for training. However, the 300W-LP is generated by profiling faces of 300W into larger poses, which is not strictly unconstrained and cannot coverall possible scenes in-thewild.

Proposed method
In this section we introduce the proposed 2D-Aided Selfsupervised Learning (2DASL) method for simultaneous 3D face reconstruction and dense face alignment. We first review the popular 3D morphable model that we adopt to render the 3D faces. Then we explain our method in details, in particular the novel cycle-consistency based self-supervised learning and the self-critic learning.

3D morphable model
We adopt the 3D morphable model (3DMM) to recover the 3D facial geometry from a single face image. The 3DMM renders 3D face shape S ? R 3N that stores 3D coordinates of N mesh vertices with linear combination over a set of PCA basis. Following, we use 40 basis from the Basel Face Model (BFM) to generate the face shape component and 10 basis from the Face Warehouse dataset to generate the facial expression component. The rendering of a 3D face shape is thus formulated as:
where S ? R 3N is the mean shape, As ? R 3N ×40 is the shape principle basis trained on the 3D face scans, ? s ? R 40 is the shape representation coefficient; A exp ? R 3N ×10 is the expression principle basis and ? exp ? R 10 denotes the corresponding expression coefficient. The target of singleimage based 3D face modeling is to predict the coefficients ? expand ? s for 3D face rendering from a single 2D image.
After obtaining the 3D face shape S, it can be projected onto the 2D image plane with the scale orthographic projection to generate a 2D face from specified viewpoint:
where V stores the 2D coordinates of the 3D vertices projected onto the 2D plane, f is the scale factor, Pr is the orthographic projection matrix 1 0 0 0 1 0 , ? is the projection matrix consisting of 9 parameters, and t is the translation vector. Putting them together, we have in total 62 pa-rameters ? = [f, t, ?, ? s , ? exp ] to regress for the 3D face regressor model.

Model overview
As illustrated in, the proposed 2DASL model contains 3 modules, i.e., a CNN-based regressor that predicts 3DMM coefficients from the input 2D image, an encoder that transforms the input image into a latent representation, and a self-critic that evaluates the input (latent representation, 3DMM coefficients) pairs to be consistent or not.
We use ResNet-50 to implement the CNN regressor. The encoder contains 6 convolutional layers, each followed by a ReLU and a max pooling layer. The critic consists of 4 fully-connected layers with 512, 1024, 1024 and 1 neurons respectively, followed by a softmax layer to output a score on the consistency degree of the input pair. The CNN regressor takes a 4-channel tensor as input that concatenates a 3-channel RGB face image and a 1-channel 2D Facial Landmark Map (FLM). The FLM is a binary-value image, where the locations corresponding to facial landmarks take the value of 1 and others take the value of ?1.
Our proposed 2DSAL method trains the model using two sets of images, i.e., the images with 3DMM ground truth annotations and the 2D face images with only 2D facial landmark annotations provided by an off-the-shelf facial landmark detector. The model is trained by minimizing the following one conventional 3D-supervision and four selfsupervision losses.
The first one is the weighted coefficient prediction loss L 3d over the 3D annotated images that measures how accurate the model can predict 3DMM coefficients. The second one is the 2D landmark consistency loss L 2d-con that measures how well the predicted 3D face shapes can recover the 2D landmark locations for the input 2D images. The third one is the 3D landmark consistency loss L 3d-con . The fourth one is the cycle consistency loss L cyc . The last one is the self-critic loss L sc that estimates the realism of the predicted 3DMM coefficients for 3D face reconstruction, conditioned on the face latent representation. Thus the overall training loss is:
where ?'s are the weighting coefficients for different losses. The details of these losses are described in the following sections one by one.

Weighted 3DMM coefficient supervision
Following, we deploy the ground truth 3DMM coefficients to supervise the model training where the contribution of each 3DMM coefficient is re-weighted according to their importance. It trains the model to predict closer coefficients? to its 3DMM ground truth ? * . Instead of calculating the conventional 2 loss, we explicitly consider im-: Illustration on the self-supervision introduced by our 2DSAL for utilizing sparse 2D landmark information. The 2D landmark prediction can be viewed as a self-mapping: X 2d ? Y 2d (forward training) constrained by L2d-con. To further supervise the model training, we introduce the Lcyc by mapping back from Y 2d ?X 2d (backward training). The L3d-con is employed to constrain landmarks matching in 3D space during the cycle training.
Here i indexes the landmark. Best viewed in color.
portance of each coefficient and re-weigh their contribution to the loss computation accordingly. Thus we obtain the weighted coefficient prediction loss as follows:
where, W = diag(w 1 , . . . , w 62 ),
Here w i indicates importance of the i th coefficient, computed from how much error it introduces to locations of 2D landmarks after projection. Here H(·) is the sparse landmark projection from rendered 3D shape, ? * is the ground truth and? i is the coefficient whose i th element comes from the predicted parameter and the others come from ? * . With such a reweighting scheme, during training, the CNN model would first focus on learning the coefficients with larger weight (e.g., the ones for rotation and translation). After decreasing their error and consequently their weights, the model will change to optimize the other coefficients (e.g., the ones for shape and expression).

2D assisted self-supervised learning
To leverage the 2D face images with only annotation of sparse 2D landmark points offered by detector, we develop the following self-supervision scheme that offers three different self-supervision losses, including the 2D landmark consistency loss L 2d-con , the 3D landmark consistency loss L 3d-con and the cycle-consistency loss L cyc . gives a systematic overview. The intuition behind this scheme is: if the 3D face estimation model is trained well, it should present consistency in the following three aspects. First, the 2D landmarks Y 2d recovered from the predicted 3D landmarks X 3d via 3D-2D projection should have small difference with the input 2D landmarks X 2d . Second,: Illustration of the weight mask used for computing L2d-con. We assign the highest weight to the red points, the medium weight to the pinky points, the yellow points has the lowest weight. Best viewed in color. the predicted 3D landmarks X 3d from the input 2D landmarks X 2d should be consistent with the 3D landmarksX 3d recovered from the predicted 2D landmarks Y 2d by passing it through the same 3D estimation model. Third, the pro-jectedX 2d fromX 3d should be consistent with the original input X 2d , i.e., forming a consistent cycle.
Thus, we define following two landmark consistency losses in our model correspondingly. The L 3d-con is formulated as:
where x 3d i is the i th 3D landmark output from the forward pass (see red arrow in),x 3d i is the i th landmark predicted from the backward pass (see green arrow in.
For computing the L 2d-con , we first create a weight mask V = {v 1 , v 2 , ..., v N } based the contribution of each point. Since the contour landmarks of a 2D face are inaccurate to represent the corresponding points of 3D face, we discard them and sample 18 landmarks from the 68 2D facial landmarks. The weight mask is shown in. Here, the mouth center landmark is the midpoint of two mouth corner points. The L 2d-con is defined as:
where x 2d i is the i th 2D landmark of the input face, y 2d i is the i th 2D landmark inferred from the output LMP, and vi is its corresponding weight. The weight values are specified in. We use the following relative weights in our experiments: (red points) : (pinky points) : (yellow points) = 4:2:1 that are set empirically.
We model the 2D facial landmarks prediction as a selfmapping process, and denote F : X 2d ? Y 2d as the forward mapping, Q : Y 2d ? X 2d as the backward mapping. The backward mapping brings the output landmarks y i back to its original position xi , i.e., x ? F (x) ? Q(F (X)) ? x. We constrain this mapping using the cycle consistency loss:
where x 2d are the input 2D facial landmarks, andx 2d are the landmarks output from Q(F (X)).: Qualitative results on AFLW2000-3D dataset. The predictions by 2DASL show that our predictions are more accurate than ground truth in some cases (only 68 points are plotted to show). Green: landmarks predicted by our 2DASL. Red: ground truth from. The thumbnails on the top right corner of each image are the dense alignment results. Best viewed in color.

Self-critic learning
We further introduce a self-critic scheme to weakly supervise the model training with the "in-the-wild" 2D face images. Given a set of face images I = {I 1 , . . . , In } without any 3D annotations and a set of face images J = {(J 1 , ? * 1 ), . . . , (J m , ? * m )} with accurate 3DMM annotations, the CNN regressor model R : I i ? ? i would output 62 coefficients for each image. We use another model as the critic C(·) to evaluate whether the predicted coefficients are consistent with the input images as the pairs of (J i , ? * i ). Since each coefficient is closely related to its corresponding face image, the critic model would learn to distinguish the realism of the coefficients conditioned on the latent representation of the input face images. To this end, we feed the input images to an encoder to obtain the latent representation z and then concatenate with their corresponding 3DMM coefficients as the inputs to the critic C(·). The critic is trained in the same way as the adversarial learning by optimizing the following loss:
where z * is the latent representation of a 3D annotated image J, ? * is the 3DMM ground truth, I is the input "inthe-wild" face image, and z is its latent representation. The above self-critic loss encourages the model to output 3D faces that lie on the manifold of human faces, and predict landmarks that have the same distribution with the true facial landmarks.

Experiments
We evaluate 2DASL qualitatively and quantitatively under various settings for 3D face reconstruction and dense face alignment.

Training details and datasets
Our proposed 2DASL is implemented with Pytorch. We use SGD optimizer for the CNN regressor with a learning rate beginning at 5 × 10 ?5 and decays exponentially, the discriminator uses the Adam as optimizer with the fixed learning rate 1 × 10 ?4 . The batch size is set as 32. ? 1 , ? 2 , ? 3 and ? 4 are set as 0.005, 0.005, 1 and 0.005 respectively. We use a two-stage strategy to train our model. In the first stage, we train the model using the overall loss L. In the second stage, we fine-tune our model using the Vertex Distance Cost, following.
The dataset 300W-LP is used to train our model. This dataset contains more than 60K face images with annotated 3DMM coefficients. The "in-the-wild" face images are all from the UMDFaces dataset that contains 367,888 still face images for 8,277 subjects. The 2D facial landmarks of all the face images are detected by an advanced 2D facial landmarks detector. The input images are cropped to the size 120 × 120. We use the test datasets below to evaluate our method: AFLW2000-3D is constructed by selecting the first 2000 images from AFLW. Each face is annotated with its corresponding 3DMM coefficients and the 68 3D facial landmarks. We use this dataset to evaluate our method on both 3D face reconstruction and dense face alignment. is another extension of AFLW. It is constructed by picking images from AFLW according to the poses. It contains 1,299 test images with a balanced distribution of yaw angle. Each image is annotated with 34 facial landmarks. We use this dataset to evaluate performance for the dense face alignment task. The 34 landmarks are used as the ground truth to measure the accuracy of our results.

AFLW-LFPA

Dense face alignment
We first compare the qualitative results from our method and corresponding ground truths in. Although all the state-of-the-art methods of dense face alignment conduct evaluation on AFLW2000-3D, the ground truth of AFLW2000-3D is controversial, since its annotation pipeline is based on the Landmarks Marching method in. As can be seen, our results are more accurate than the ground truth in some cases. This is mainly because 2DASL involves a number of the "in-the-wild" images for training, enabling the model to perform well in cases even unseen in the 3D annotated training data. For fair comparison, we adopt the normalized mean error (NME) as the metric to evaluate the alignment performance. The NME is the mean square error normalized by face bounding box size. Since some images in AFLW2000-3D contains more than 2 faces, and the face detector sometimes gives the wrong face for evaluation (not the test face with ground truth), leading to high NME. Therefore, we discard the worst 20 cases of each method and only 1,980 images from AFLW2000-3D are used for evaluation. We evaluate our 2DASL using a sparse set of 68 facial landmarks and also the dense points with both 2D and 3D coordinates, and compare it with other state-of-the-arts. The 68 sparse facial landmarks can be viewed as sampling from the dense facial points. Since PRNet and VRN-Guided are not 3DMM based, and the point cloud of these two methods are not corresponding to 3DMM, we only compare with them on the sparse 68 landmarks. The results are shown in, where we can see our 2DASL achieves the lowest NME (%) on the evaluation of both 2D and 3D coordinates among all the methods. For 3DMM-based methods: 3DDFA and DeFA, our method outperforms them by a large margin on both the 68 spare landmarks and the dense coordinates.
To further investigate performance of our 2DASL across poses and datasets, we report the NME of faces with small, medium and large yaw angles on AFLW2000-3D dataset and the mean NME on both AFLW2000-3D and AFLW-LPFA datasets. The comparison results are shown in Tab. 1. Note that all the images from these two datasets are used for evaluation to keep consistent with prior works. The results of the compared method are directly from their published papers. As can be observed, our method achieves the lowest mean NME on both of the two datasets, and the lowest NME across all poses on AFLW2000-3D. Our 2DASL even performs better than PRNet, reducing NME by 0.09 and 0.08 on AFLW2000-3D and AFLW-LFPA, respectively. Es-  pecially on large poses (from 60 • to 90 • ), 2DASL achieves 0.2 lower NME than PRNet. We believe more "in-the-wild" face images used for training ensures better performance of 2DASL.

3D face reconstruction
In this section, we evaluate our 2DASL on the task of 3D face reconstruction on AFLW2000-3D by comparing with 3DDFA and DeFA. The VRN-Guided and PRNet are not compared because of the mis-match of point cloud between them and our method. Following, we first employ the Iterative Closest Points (ICP) algorithm to find the corresponding nearest points between the reconstructed 3D face and the ground truth point cloud. We then calculate the NME normalized by the face bounding box size. shows the comparison results on AFLW2000-3D. As can be seen, the 3D reconstruction results of 2DASL outperforms 3DDFA by 0.39, and 2.29 for DeFA, which are significant improvements.
We show some visual results of our 2DASL and compare with PRNet and VRN-Guided in. As can be seen, the reconstructed shape of our 2DASL are more smooth, however, both PRNet and VRN-Guided introduce some artifacts into the reconstructed results, which makes the reconstructed faces look unnaturally.

Ablation study
In this section, we perform ablation study on AFLW2000-3D by evaluating several variants of our model: (1) 2DASL (base), which only takes the RGB images as input without self-supervision and self-critic supervision;
(2) 2DASL (cyc), which takes as input the combination of RGB face images and the corresponding 2D FLMs with self-supervison, however without self-critic supervision; (3) 2DASL (sc), which takes as input the RGB face images only using self-critic learning. (4) 2DASL (cyc+sc), which contains both self-supervision and self-critic supervision. For each variant, we use the L 2d-con with (w/) or without (w/o) weight mask. Therefore, there are in total 6 variants.
The ablation study results are shown in Tab. 2. Adding weights to central points of the facial landmarks reduces the NME by 0.09 to 0.23 on the two stages, respectively. Both self-critic and the self-supervision are effective to improve the performance. If the self-critic learning is not used, the NME increases by 0.04/0.18 for with/without weight mask, respectively. While the self-supervision scheme reduce NME by 0.1 when the weight mask is used, and 0.23 if the weight mask is removed, no significant improvement is observed. The best result is achieved when both these two modules are used. Moreover, in our experiments, we found taking the FLMs as input can accelerate the convergence of training process. Therefore, the first training stage just takes one or two epochs to reach a good model.
To explore how the performance is affected by the number of "in-the-wild" face images involved in training, we train our model using different numbers. Since the UMD-Faces dataset divides the whole dataset into 3 batches, each contains 77,228, 115,126, and 175,534 images respectively. We use the 3 batches and also the whole dataset to train our model. The results are reported in Tab. 3, where we can seethe more data that used for aiding training, the lower NME is achieved by 2DASL.: The results (NME (%)) of 2DASL by training with different number of "in-the-wild" face images. "Num. # ITW" indicates the number of the "in-the-wild" face images used for training. The numbers in bold are the best results of each stage.

Conclusion
In this paper, we propose a novel 2D-Assisted Selfsupervised Learning (2DASL) method for 3D face reconstruction and dense face alignment based on the 3D Morphable face Model. The sparse 2D facial landmarks are taken as input of CNN regressor and learn themselves via 3DMM coefficients regression. To supervise and facilitate the 3D face model learning, we introduce four selfsupervision losses, including the self-critic which is employed to weakly supervise the training samples that without 3D annotations. Our 2DASL make the abundant "inthe-wild" face images could be used to aid 3D face analysis without any 2D-to-3D supervision. Experiments on two challenging face datasets illustrate the effectiveness of 2DASL on both 3D face reconstruction and dense face alignment by comparing with other state-of-the-arts.