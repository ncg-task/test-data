201	0	15	Disentanglement
201	16	18	of
201	19	38	style and structure
201	46	49	key
201	55	65	influences
201	66	73	quality
201	74	76	of
201	77	102	style - augmented samples
210	0	34	Style - augmented synthetic images
210	35	42	improve
210	43	66	detectors ' performance
210	67	69	by
210	72	84	large margin
210	97	108	improvement
210	112	123	even larger
210	124	128	when
210	133	139	number
210	140	142	of
210	143	158	training images
210	162	173	quite small
225	3	11	evaluate
225	12	22	our method
225	23	32	by adding
225	37	43	number
225	44	46	of
225	47	70	random sampled styles k
225	71	73	of
225	74	98	each annotated landmarks
225	99	101	on
225	104	124	ResNet - 50 baseline
227	0	9	By adding
227	12	18	number
227	19	21	of
227	22	38	augmented styles
227	45	50	model
227	51	79	continue gaining improvement
11	39	81	https://github.com/thesouthfrog/stylealign
164	0	4	WFLW
170	4	27	light - weight Res - 18
170	31	39	improved
170	40	42	by
170	43	49	13.8 %
172	43	53	our method
172	59	65	brings
172	66	84	15.9 % improvement
172	85	87	to
172	88	97	SAN model
172	104	113	9 % boost
172	114	116	to
172	117	120	LAB
172	121	125	from
172	126	136	5.27 % NME
172	137	139	to
172	140	146	4.76 %
171	3	12	utilizing
171	15	32	stronger baseline
171	35	44	our model
171	45	53	achieves
171	54	64	4.39 % NME
171	65	70	under
171	71	97	style - augmented training
171	100	111	outperforms
171	112	140	state - of the - art entries
171	141	143	by
171	146	158	large margin
175	0	4	With
175	5	63	additional " style- augmented " synthetic training samples
175	66	75	our model
175	76	84	based on
175	87	102	simple backbone
175	103	114	outperforms
175	115	154	previous state - of - the - art methods
177	0	4	300W
183	26	32	yields
183	33	60	1.8 % and 3.1 % improvement
183	61	63	on
183	64	75	LAB and SAN
183	97	105	manifest
183	110	128	consistent benefit
183	134	139	using
183	144	174	" style - augmented " strategy
185	0	41	Common Cross - dataset Evaluation on COFW
188	0	9	Our model
188	10	18	performs
188	23	27	best
188	28	32	with
188	33	39	4.43 %
188	40	50	mean error
188	55	61	2.82 %
188	62	74	failure rate
189	0	4	AFLW
196	0	10	Exploiting
196	11	28	style information
196	34	40	boosts
196	41	59	landmark detectors
196	60	64	with
196	67	93	large - scale training set
197	16	26	our method
197	27	35	improves
197	36	48	SAN baseline
197	49	60	in terms of
197	61	64	NME
197	65	67	on
197	68	76	Full set
197	77	81	from
197	82	88	6.94 %
197	89	91	to
197	92	98	6.01 %
198	4	21	visual comparison
198	25	30	shows
198	31	47	hidden face part
198	51	65	better modeled
198	66	70	with
198	71	83	our strategy
154	23	29	Before
154	30	38	training
154	41	51	all images
154	56	75	cropped and resized
154	76	78	to
154	79	86	256 256
154	87	92	using
154	93	116	provided bounding boxes
156	3	6	use
156	7	32	6 residual encoder blocks
156	33	36	for
156	37	49	downsampling
156	54	72	input feature maps
156	81	100	batch normalization
156	104	115	removed for
156	116	140	better synthetic results
158	0	3	For
158	4	12	training
158	13	15	of
158	20	38	disentangling step
158	44	47	use
158	48	52	Adam
158	53	57	with
158	61	82	initial learning rate
158	83	85	of
158	86	90	0.01
158	99	116	descends linearly
158	117	119	to
158	120	126	0.0001
158	127	131	with
158	132	147	no augmentation
159	16	25	detectors
159	31	44	first augment
159	45	62	each landmark map
159	63	67	with
159	68	83	k random styles
159	84	96	sampled from
159	97	114	other face images
161	8	29	detector architecture
161	34	57	simple baseline network
161	58	66	based on
161	67	78	ResNet - 18
161	89	100	by changing
161	105	121	output dimension
161	122	124	of
161	129	143	last FC layers
161	144	146	to
161	147	155	landmark
30	17	24	propose
30	27	40	new framework
30	41	51	to augment
30	52	60	training
30	61	64	for
30	65	90	facial landmark detection
30	91	104	without using
30	105	120	extra knowledge
31	49	52	map
31	53	64	face images
31	65	69	into
31	74	79	space
31	8	10	of
31	83	102	structure and style
32	58	64	design
32	67	111	conditional variational auto - encoder model
32	123	184	Kullback - Leiber ( KL ) divergence loss and skip connections
32	189	205	incorporated for
32	206	228	compact representation
32	33	35	of
32	232	251	style and structure
32	0	12	To guarantee
32	17	32	disentanglement
32	229	231	of
32	36	52	these two spaces
33	33	40	perform
33	41	65	visual style translation
33	66	73	between
33	74	98	existing facial geometry
34	0	5	Given
34	6	102	existing facial structure , faces with glasses , of poor quality , under blur or strong lighting
34	107	122	rerendered from
34	123	142	corresponding style
34	171	176	train
34	181	206	facial landmark detectors
34	207	210	for
34	220	245	general and robust system
34	246	258	to recognize
34	259	274	facial geometry
5	0	25	Facial landmark detection
5	31	45	face alignment
