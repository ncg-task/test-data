title
Aggregation via Separation : Boosting Facial Landmark Detector with Semi-Supervised
Style Translation
abstract
Facial landmark detection , or face alignment , is a fundamental task that has been extensively studied .
In this paper , we investigate a new perspective of facial landmark detection and demonstrate it leads to further notable improvement .
Given that any face images can be factored into space of style that captures lighting , texture and image environment , and a style - invariant structure space , our key idea is to leverage disentangled style and shape space of each individual to augment existing structures via style translation .
With these augmented synthetic samples , our semi-supervised model surprisingly outperforms the fully - supervised one by a large margin .
Extensive experiments verify the effectiveness of our idea with state - of - the - art results on WFLW [ 69 ] , 300W [ 56 ] , COFW [ 7 ] , and AFLW [ 36 ] datasets .
Our proposed structure is general and could be assembled into any face alignment frameworks .
The code is made publicly available at https://github.com/thesouthfrog/stylealign.
Introduction
Facial landmark detection is a fundamentally important step in many face applications , such as face recognition , 3 D face reconstruction , face tracking and face editing .
Accurate facial landmark localization was intensively studied with impressive progress made in these years .
The main streams are learning a robust and discriminative model through effective network structure , usage of geometric information , and correction of loss functions .
It is common wisdom now that factors such as variation of expression , pose , shape , and occlusion could greatly affect performance of landmark localization .
Almost all prior work aims to alleviate these problems from the perspective of structural characteristics , such as disentangling 3 D pose to provide shape constraint , and utilizing dense bound - :
Problem in a well - trained facial landmark detector .
It is biased towards unconstrained environment factors , including lighting , image quality , and occlusion .
We regard these degradations as " style " in our analysis. ary information .
The influence of " environment " still lacks principled discussion beyond structure .
Also , considering limited labeled data for this task , how to optimally utilize limited training samples remains unexplored .
About " environment " effect , distortion brought by explicit image style variance was observed recently .
We instead utilize style transfer and disentangled representation learning to tackle the face alignment problem , since style transfer aims at altering style while preserving content .
In practice , image content refers to objects , semantics and sharp edge maps , whereas style could be color and texture .
Our idea is based on the purpose of facial landmark detection , which is to regress " facial content " - the principal component of facial geometry - by filtering unconstrained " styles " .
The fundamental difference to define " style " from that of is that we refer it to image background , lighting , quality , existence of glasses , and other factors that prevent detectors from recognizing facial geometry .
We note every face image can be decomposed into its facial structure along with a distinctive attribute .
It is a natural conjecture that face alignment could be more robust if we augment images only regarding their styles .
To this end , we propose a new framework to augment training for facial landmark detection without using extra knowledge .
Instead of directly generating images , we first map face images into the space of structure and style .
To guarantee the disentanglement of these two spaces , we design a conditional variational auto - encoder model , in which Kullback - Leiber ( KL ) divergence loss and skip connections are incorporated for compact representation of style and structure respectively .
By factoring these features , we perform visual style translation between existing facial geometry .
Given existing facial structure , faces with glasses , of poor quality , under blur or strong lighting are rerendered from corresponding style , which are used to further train the facial landmark detectors for a rather general and robust system to recognize facial geometry .
Our main contribution is as follows .
1 .
We offer a new perspective for facial landmark localization by factoring style and structure .
Consequently , a face image is decomposed and rendered from distinctive image style and facial geometry .
2 .
A novel semi-supervised framework based on conditional variational auto - encoder is built upon this new perspective .
By disentangling style and structure , our model generates style - augmented images via style translation , further boosting facial landmark detection .
3 .
We propose a new dataset based on AFLW with new 68 - point annotation .
It provides challenging benchmark considering large pose variation .
With extensive experiments on popular benchmark datasets including WFLW , 300W , COFW and AFLW , our approach outperforms previous state - of the - arts by a large margin .
It is general to be incorporated into various frameworks for further performance improvement .
Our method also works well under limited training computation resource .
Related Work
This work has close connection with the areas of facial landmark detection , disentangled representation and selfsupervised learning .
Facial Landmark Detection
This area has been extensively studied over past years .
Classic parameterized methods , such as active appearance models ( AAMs ) and constrained local models ( CLMs ) provide satisfying results .
SDM , cascaded regression , and their variants were also proposed .
Recently , with the power of deep neural networks , regression - based models are able to produce better results .
They are mainly divided into two streams of direct coordinate regression and heatmap - based regression .
Meanwhile , in , auxiliary attributes were used to learn a discriminative representation .
Recurrent modules were introduced then .
Lately , methods improved performance via semi-supervised learning .
Influence of style variance was also discussed in , where a style aggregated component provides a stationary environment for landmark detector .
Our solutions are distinct with definition of " style " , different from prior work .
Our solution does not rely on the aggregation architecture , and instead is based on a semi-supervised scheme .
Disentangled Representation
Our work is also related to disentangled representation learning .
Disentanglement is necessary to control and further alter the latent information in generated images .
Under the unsupervised setting , InfoGAN and MINE learned disentangled representation by maximizing the mutual information between latent code and data observation .
Recently , imageto - image translation explored the disentanglement between style and content without supervision .
In structured tasks such as conditional image synthesis , keypoints and person mask were utilized as self - supervision signals to disentangle factors , such as foreground , background and pose information .
As our " style " is more complex while " content " is represented by facial geometry , traditional style transfer is inapplicable since it may suffer from structural distortion .
In our setting , by leveraging the structure information base on landmarks , our separation component extracts the style factor from each face image .
Self - Supervised Learning
Our method also connects to self - supervised learning .
The mainstream work , such as , directly uses image data to provide proxy supervision through multi-task feature learning .
Another widelyadopted approach is to use video data .
Visual invariance of the same instance could be captured in a consecutive sequence of video frames .
Also , there is work focusing on fixed characteristics of objects from data statistics , such as image patch level information .
These methods learn visual invariance , which could essentially provide a generalized feature of objects .
Our landmark localization involves computing the visual invariance .
But our approach is different from prior selfsupervised frameworks .
Our goal lies in extracting facial structure and keypoints considering different environment factors , including occlusion , lighting , makeup and soon .
Eliminating the influence of style makes it possible to reliably alter or process face structure and accordingly recog -
Proposed Framework
Our framework consists of two parts .
One learns the disentangled representation of facial appearance and structure , while the other can be any facial landmark detectors .
As illustrated in , during the first phase , conditional variational auto - encoder is proposed for learning disentangled representation between style and structure .
In the second phase , after translating style from other faces , " stylized " images with their structures are available for boosting training performance and our style - invariant detectors .
Learning Disentangled Style and Structure
Given an image x , and its corresponding structure y .
Two essential descriptors of a face image are facial geometry and image style .
Facial geometry is represented by labeled landmarks , while style captures all environmental factors that are mostly implicit , as described above .
With this setting , if the latent space of style and shape is mostly uncorrelated , using Cartesian product of z and y latent space should capture all variation included in a face image .
Therefore , the generator that re-renders a face image based on style and structure can be modeled as p ( x |y , z ) .
To encode the style and structure information and compute the parametric distribution p ( x |y , z ) , a conditional variational auto - encoder based network , which introduces two encoders , is applied .
Our network consists of a structure estimator E struct to encode landmark heatmaps into structure latent space , a style encoder E style that learns the style embedding of images , and a decoder that re-renders the style and structure to image space .
As landmarks available in this task , the facial geometry is represented by stacking landmarks to heat maps .
Our goal therefore becomes inferring disentangled style code z from a face image and its structure by maximizing the conditional likelihood of
In particular , the generator G full ?
contains two encoders and a decoder ( renderer ) , i.e. , E style ? , E struct and D render , where G full ?
and E style ?
respectively estimate parameters of p ( x |y , z ) and q ( z |x , y ) .
Consequently , the full loss function on learning separating information of style and structure is written as
( 2 )
KL - Divergence Loss Kullback - Leiber ( KL ) divergence loss severs as a key component in our design to help the encoder to learn decent representation .
Basically , the KLdivergence measures the similarity between the variational posterior and prior distribution .
In our framework , it is taken as regularization that discourages E style to encode structure - related information .
As the prior distribution is commonly assumed to be a unit Gaussian distribution p ? N ( 0 , 1 ) , the learned style feature is regularized to suppress contained structure information through reconstruction .
The KL - divergence loss limits the distribution range and capacity of the style feature .
By fusing inferred style code z with encoded structure representation , sufficient structure information can be obtained from prior through multi - level skip connection .
Extra structure encoded in z incurs penalty of the likelihood p ( x |y , z ) during training with no new information captured .
In this way , E style is discouraged from learning structure information that is provided by E struct during training .
To better reconstruct the original image , E style is enforced to learn structure - invariant style information .
Reconstruction Loss
The second term L rec in Eq. refers to the reconstruction loss in the auto - encoder frame - work .
As widely discussed , basic pixel - wise L 1 or L 2 loss can not model rich information within images well .
We instead adopt perceptual loss to capture style information and better visual quality .
L rec is formulated as
where we use VGG - 19 network ?
structure that measures perceptual quality .
l indexes the layer of network ?.
Since the style definition could be complicated , E style here encodes semantics of the style signal that simulates different types of degradation .
It does not have to maintain fine - grained visual details .
Besides , to reserve the strong prior on structure information encoded from landmarks y , skip connection between E struct and D render is established to avoid landmark inaccuracy through style translation .
In this design , the model is capable of learning complementary representation of facial geometry and image style .
Augmenting Training via Style Translation
Disentanglement of structure and style forms a solid foundation for diverse stylized face images under invariant structure prior .
Given a dataset X that contains n face images with landmarks annotation , each face image xi ( 1 ? i ? n ) within the dataset has its explicit structure denoted by landmark y i , as well as an implicit style code z i depicted and embedded by E style .
To perform style translation between two images x i and x j , we pass their latent style and structure code embedded by E style and E struct to D render .
To put the style of image x j on x i 's structure , the stylized synthetic image is denoted as
As illustrated in , the first stage of our framework is to train the disentangling components .
In the second phase , by augmenting and rendering a given sample x in the original dataset X with styles from random k other faces , we produce k n " stylized " synthetic face images with respective annotated landmarks .
These samples are then fed into training of facial landmark detectors together with the original dataset .
Visualization of style translation results is provided in .
The input facial geometry is maintained under severe style variation , indicating its potential at augmenting training of facial landmark detectors .
Albeit with cohesive structure , the decoder generally does not re-render perfect - quality images , since the complexity of plentiful style information has been diminished to a parametric Gaussian distribution , confined by its capacity .
Also , as discussed before , each face image xi has its own style .
Theoretically , the renderer could synthesize n 2 images by rendering each available landmark with any other images ' style .
To understand how the quantity of stylized synthetic samples helps improve the facial landmark detectors , we analyze the effect of our design in following experiments and ablation study .
Experiments
Datasets
WFLW dataset is a challenging one , which contains 7,500 faces for training and 2,500 faces for testing , based .
Following the widely - adopted protocol , the AFLW - full dataset has 20,000 images for training and 4,386 for testing .
It is originally annotated with 19 sparse facial landmarks .
To provide a better benchmark for evaluating pose variation and allow cross - dataset evaluation , we re-annotate it with 68 facial landmarks , which follow the common standard in 300W .
Based on the new 68 - point annotation , we conduct more precise evaluation .
Cross - dataset evaluation is also provided among existing datasets .
COFW dataset contains 1,345 images for training and 507 images for testing , focusing on occlusion .
The whole dataset is originally annotated with 29 landmarks and has been re-annotated with 68 landmarks in to allow crossdataset evaluation .
We utilize 68 annotated landmarks provided by to conduct comparison with other approaches .
Experimental Setting
Evaluation Metrics
We evaluate performance of facial landmark detection using normalized landmarks mean error and Cumulative Errors Distribution ( CED ) curve .
For the 300 W dataset , we normalize the error using inter-pupil distance .
In , we also report the NME using inter-ocular distance to compare with algorithms of , which also use it as the normalizing factor .
For other datasets , we follow the protocol used in and apply inter-ocular distance for normalization .
Implementation Details Before training , all images are cropped and resized to 256 256 using provided bounding boxes .
For the detailed conditional variational autoencoder network structures , we use a two - branch encoderdecoder structure as shown in .
We use 6 residual encoder blocks for downsampling the input feature maps , where batch normalization is removed for better synthetic results .
The facial landmark detector backbone is substitutable and different detectors are usable to achieve improvement , which we will discuss later .
For training of the disentangling step , we use Adam with an initial learning rate of 0.01 , which descends linearly to 0.0001 with no augmentation .
For training of detectors , we first augment each landmark map with k random styles sampled from other face images .
The number is set to 8 if not specially mentioned in experiments .
For the detector architecture , a simple baseline network based on ResNet - 18 is chosen by changing the output dimension of the last FC layers to landmark 2 to demonstrate the increase brought by style translation .
To compare with state - of - thearts and further validate the effectiveness of our approach , we replace our baseline model with similar structures proposed in , with the same affine augmentation .
Comparison with State - of - the - arts
WFLW
We evaluate our approach on WFLW dataset .
WFLW is a recently proposed challenging dataset with images from in - the - wild environment .
We compare algorithm in terms of NME ( % ) , Failure Rate ( % ) and AUC ( @0.1 ) following protocols used in .
The Res - 18 baseline receives strong enhancement using synthetic images .
To further verify the effectiveness and generality of using style information , we replace the network by two strong baselines and report the result in .
The light - weight Res - 18 is improved by 13.8 % .
By utilizing a stronger baseline , our model achieves 4.39 % NME under style - augmented training , outperforms state - of the - art entries by a large margin .
In particular , for the strong baselines , our method also brings 15.9 % improvement to SAN model , and 9 % boost to LAB from 5.27 % NME to 4.76 % .
The elevation is also determined by the model capacity . , we report different facial landmark detector performance ( in terms of normalized mean error ) on 300 W dataset .
The baseline network follows Res - 18 structure .
With additional " style- augmented " synthetic training samples , our model based on a simple backbone outperforms previous state - of - the - art methods .
We also report results of models that are trained on original data , which reflect the performance gain brought by our approach .
300W
In
Similarly , we replace the baseline model with a state - of the - art method .
Following the same setting , this baseline is also much elevated .
Note that the 4 - stack LAB and SAN are open - source frameworks .
We train the models from scratch , which perform less well than those reported in their original papers .
However , our model still yields 1.8 % and 3.1 % improvement on LAB and SAN respectively , which manifest the consistent benefit when using the " style - augmented " strategy .
Method
Common Cross - dataset Evaluation on COFW
To comprehensively evaluate the robustness of our method towards occlusion , COFW - 68 is also utilized for cross - dataset evaluation .
We perform comparison against several state - of - theart methods in .
Our model performs the best with 4.43 % mean error and 2.82 % failure rate , which indicates high robustness to occlusion due to our proper utilization of style translation .
AFLW
We further evaluate our algorithm on the AFLW dataset following the AFLW Full protocol .
AFLW is also challenging for its large pose variation .
It is originally annotated with 19 facial landmarks , which are relatively sparse .
To make it more useful , we richen the dataset by re-annotating it with 68 - point facial landmarks .
This new set of data is also publicly available .
We compare our approach with several models in Table 3 , by re-implementing their algorithms on the new dataset along with our style - augmented samples .
Exploiting style information also boosts landmark detectors with a large - scale training set ( 25 , 000 images in AFLW ) .
Interestingly , our method improves SAN baseline in terms of NME on Full set from 6.94 % to 6.01 % , which indicates that augmenting in style level brings promising improvement on solving large pose variation .
The visual comparison in shows hidden face part is better modeled with our strategy .
Ablation Study
Improvement on Limited Data
Disentanglement of style and structure is the key that influences quality of style - augmented samples .
We evaluate the completeness of disentanglement especially when the training samples are limited .
To evaluate the performance and relative gain of our approach when training data is limited .
The training set is split into 10 subsets and respectively we evaluate our model on different portions of training data .
Note that for different portions , we train the model from scratch with no extra data used .
The quantitative result is reported in .
In : Normalized mean error ( % ) on 300W common and WFLW datasets when using different percentages of the training set , with the same protocol as in on a stronger baseline .
The baseline network here follows SAN structure .
show the relative improvement on different training samples .
Style - augmented synthetic images improve detectors ' performance by a large margin , while the improvement is even larger when the number of training images is quite small .
In , a stronger baseline SAN is chosen .
Surprisingly , the baseline easily reaches state - of - theart performance using only 50 % labeled images , compared to former methods provided in .
Besides , provides an intuitive visualization of the resulting generated faces when part of the data is used .
Each column contains output that is rendered from the input structure and given style , when using a portion of face image data .
It shows when the data is limited , our separation component tends to capture weak style information , such as color and lighting .
Given more data as examples , the style becomes complex and captures detailed texture and degradation , like occlusion .
The results verify that even using limited labeled images , our design is capable of disentangling style information and keeps improve those baseline methods that are already very strong .
Estimating the Upper-bound
As discussed before , our method conceptually and empirically augments training with n 2 synthetic samples .
By aug - : Results of style translation using different numbers of data .
The left 2 images are the input , with 2 different reference styles .
The percentage refers to how much data is used to train the disentangle module .
menting each face image with k random styles , the training set could be very large and slows down convergence .
In this section , we experiment with choosing the style augmenting factor k and test the upper bound of style translation .
We evaluate our method by adding the number of random sampled styles k of each annotated landmarks on a ResNet - 50 baseline .
The result is reported in .
By adding a number of augmented styles , the model continue gaining improvement .
However , when k ? 8 , the performance grow slows down .
It begins to decrease if k reaches 32 .
The reason is that due to the quantity imbalance between real and synthetic faces , a very large k makes the model overfit to synthetic image texture when the generated image quantity is large .
Conclusion and Future Work
In this paper , we have analyzed the well - studied facial landmark detection problem from a new perspective of implicit style and environmental factor separation and utilization .
Our approach exploits the disentanglement representation of facial geometry and unconstrained style to provide synthetic faces via style translation , further boosting quality of facial landmarks .
Extensive experimental results manifest its effectiveness and superiority .
We also note that utilizing synthetic data for more highlevel vision tasks still remains an open problem , mainly due to the large domain gap between generated and real images .
In our future work , we plan to model style in a more realistic way by taking into account the detailed degradation types and visual quality .
We also plan to generalize our structure to other vision tasks .
S1 . More Ablation Studies
In this section , we provide additional analysis about each design in our framework to facilitate understanding of our structure .
Two key loss terms in our framework are studied to give insights into their respective roles .
Qualitative visualization and quantitative results are reported for a comprehensive comparison .
KL divergence loss and perceptual loss , are incorporated into our framework during the disentangled learning procedure .
shows their respective effect on style translation via visual comparisons of several incomplete variants .
Through visual observations , their roles could be inferred intuitively .
The perceptual loss , as discussed , is designed to capture better style information and visual quality .
Thus , removing this term leads to " over-smoothness " and poor diversity on synthetic images .
Removing KL divergence term shows severe structure distortion on translated results , which indicates that KL divergence loss plays a key role on disentangling structure and style information .
Quantitative results of each variants are also reported in 7 .
The normalized mean error ( NME ) is evaluated on WFLW test set when the model is trained on style augmented dataset using each variant .
We observe that NME will increase if any loss function is removed .
In particular , the detector performance drops significantly lower than the baseline if L KL is removed .
Both the qualitative and quantitative result interprets the role of each component , indicating their essentialness in our framework .
S2 . Additional Discussion
In this section , we provide more discussion on our approach along with our analysis towards some existing alternatives .
S2.1 Comparison with GAN - based approaches
Generative adversarial network ( GAN ) and its applications are widely studied these days , using GAN - synthetic data to aid training , has also been explored along this line .
Some works have utilized GANs to perform data augmentation .
However , its effect still remains questionable especially on high - level vision challenges .
For instance , in our task , face images need to be labeled with accurate landmarks .
Existing generative models are incapable of handling these tasks with fine - grained annotations , e.g. semantic segmentation , constrained by its limited generalizability .
We choose to escape the difficulties of GAN training , starting from a new perspective of internal representation .
With decent representation of separating style and structure , different interactions within a face image can be simulated by re-rendering from existing style and structure code .
In other words , our choice depends upon fully exploiting available information by mixing them , instead of creating new information and visually perfect results via adversarial learning procedure .
However , if two codes of structure and style are factored well , advances on high fidelity images synthesis could theoretically bring more gains based on our framework .
S2.2 Comparison with Style Transfer
Our method is motivated by advances in style transfer .
A common doubt could be why not directly conducting style transfer as a augmentation or how basic style transfer could help training .
As discussed , our definition of style includes environments and degradation that prevent the model from recognizing while content refers to facial geometry .
Applying " vanilla " style transfer would leads to structural distortion on stylized images , as illustrated in .
Our definition of " style " helps preserve structure on synthetic images .
Besides , synthetic images using style transfer have a large domain gap with real - world face images .
Simply augmenting training with these samples would instead hurt model 's localization ability on real images ..
Our results are more realistic than stylized images , with better structure coherence .
Database
Environment Number Multi- PIE
Controlled 750000 XM2VTS 2360 LFPW 1035 HELEN In - the-wild 2330 AFW 468 IBUG 135 COFW - 68 507 AFLW - 68 ( Ours ) 25993
S3 . Details of AFLW 68 - point dataset
We propose a new facial landmark dataset based on AFLW , to facilitate benchmarking on large pose performance .
To allow a more precise evaluation and crossdataset comparison , we follow the widely - used Multi -
