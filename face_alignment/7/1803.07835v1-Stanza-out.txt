title
Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network
abstract
We propose a straightforward method that simultaneously reconstructs the 3D facial structure and provides dense alignment .
To achieve this , we design a 2D representation called UV position map which records the 3D shape of a complete face in UV space , then train a simple Convolutional Neural Network to regress it from a single 2D image .
We also integrate a weight mask into the loss function during training to improve the performance of the network .
Our method does not rely on any prior face model , and can reconstruct full facial geometry along with semantic meaning .
Meanwhile , our network is very light - weighted and spends only 9.8 ms to process an image , which is extremely faster than previous works .
Experiments on multiple challenging datasets show that our method surpasses other state - of - the - art methods on both reconstruction and alignment tasks by a large margin .
Code is available at https://github.com/YadiraF/PRNet.
Introduction
3 D face reconstruction and face alignment are two fundamental and highly related topics in computer vision .
In the last decades , researches in these two fields benefit each other .
In the beginning , face alignment that aims at detecting a special 2D fiducial points is commonly used as a prerequisite for other facial tasks such as face recognition and assists 3 D face reconstruction to a great extent .
However , researchers find that 2D alignment has difficulties in dealing with problems of large poses or occlusions .
With the development of deep learning , many computer vision problems have been well solved by utilizing Convolution Neural Networks ( CNNs ) .
Thus , some works start to use CNNs to estimate the 3D Morphable Model ( 3 DMM ) coefficients or 3D model warping functions to restore the corresponding 3D information from a single 2D facial image , which provides both dense face alignment and 3D face reconstruction results .
However , the performance of these methods is restricted due to the limitation of the 3D space defined by face model basis or arXiv : 1803.07835v1 [ cs . CV ] 21 Mar 2018 templates .
The required operations including perspective projection or 3D Thin Plate Spline ( TPS ) transformation also add complexity to the overall process .
Recently , two end - to - end works , which bypass the limitation of model space , achieve the state - of - the - art performances on their respective tasks .
trains a complex network to regress 68 facial landmarks with 2D coordinates from a single image , but needs an extra network to estimate the depth value .
Besides , dense alignment is not provided by this method .
develops a volumetric representation of 3 D face and uses a network to regress it from a 2D image .
However , this representation discards the semantic meaning of points , thus the network needs to regress the whole volume in order to restore the facial shape , which is only part of the volume .
So this representation limits the resolution of the recovered shape , and need a complex network to regress it .
We find that these limitations do not exist in previous model - based methods , it motivates us to find anew approach to obtaining the 3D reconstruction and dense alignment simultaneously in a model - free manner .
In this paper , we propose an end - to - end method called Position map Regression Network ( PRN ) to jointly predict dense alignment and reconstruct 3 D face shape .
Our method surpasses all other previous works on both 3 D face alignment and reconstruction on multiple datasets .
Meanwhile , our method is straightforward with a very light - weighted model which provides the result in one pass with 9.8 ms .
All of these are achieved by the elaborate design of the 2D representation of 3 D facial structure and the corresponding loss function .
Specifically , we design a UV position map , which is a 2D image recording the 3D coordinates of a complete facial point cloud , and at the same time keeping the semantic meaning at each UV place .
We then train a simple encoder - decoder network with a weighted loss that focuses more on discriminative region to regress the UV position map from a single 2 D facial image .
Figure1 shows our method is robust to poses , illuminations and occlusions .
In summary , our main contributions are :
- For the first time , we solve the problems of face alignment and 3D face reconstruction together in an end - to - end fashion without the restriction of low - dimensional solution space .
- To directly regress the 3D facial structure and dense alignment , we develop a novel representation called UV position map , which records the position information of 3 D face and provides dense correspondence to the semantic meaning of each point on UV space .
- For training , we proposed a weight mask which assigns different weight to each point on position map and compute a weighted loss .
We show that this design helps improving the performance of our network .
- We finally provide a light - weighted framework that runs at over 100 FPS to directly obtain 3 D face reconstruction and alignment result from a single 2 D facial image .
- Comparison on the AFLW2000 - 3D and Florence datasets shows that our method achieves more than 25 % relative improvements over other stateof - the - art methods on both tasks of 3 D face reconstruction and dense face alignment .
Related Works
As described above , the main problems our method can solve are 3 D face reconstruction and face alignment .
We will talk about closely related works on these tasks in the following subsections .
3D
Face Reconstruction
In this part , we evaluate our method on 3D face reconstruction task and compare with 3 DDFA , DeFA and VRN - Guided on AFLW2000 - 3D and Florence datasets .
We use the same set of points as in evaluating dense alignment and changes the metric so as to keep consistency with other 3 D face reconstruction evaluation methods .
We first use Iterative Closest Points ( ICP ) algorithm to find the corresponding nearest points between the network output and ground truth point cloud , then calculate Mean Squared Error ( MSE ) normalized by outer interocular distance of 3D coordinates .
The result is shown in .
our method greatly exceeds the performance of other two state - of - the - art methods .
Since AFLW2000 -3D dataset is labeled with results from 3 DMM fitting , we further evaluate the performance of our method on Florence dataset , where ground truth 3 D point cloud is obtained from structured - light 3D scanning system .
Here we compare our method with 3 DDFA and VRN - Guided , using experimental settings in .
The evaluation images are the renderings with different poses from Florence database , we calculate the bounding box from the ground truth point cloud and using the cropped image as network input .
Although our method output more complete face point clouds than VRN , we only choose the common face region to compare the performance , 19 K points are used for the evaluation .
shows that our method achieves 28.7 % relative higher performance compared to VRN - Guided on Florence dataset , which is a significant improvement .
To better evaluate the reconstruction performance of our method across different poses , we calculated the NME for different yaw angle range .
As shown in figure 9 , all the methods perform well in near frontal view , however , 3 DDFA and VRN - Guided fail to keep low error as pose becomes large , while our method keeps relatively stable performance in all pose ranges .
We also illustrate the qualitative comparison in , our restored point cloud covers a larger region than in VRN - Guided , which ignores the lateral facial parts .
Besides , due to the limitation on resolution of VRN , our method provides finer details of face , especially on the nose and mouth region .
We also provide additional qualitative results on 300 VW and Multi - PIE datasets , please refer to supplementary material for full details . :
Left : CED curves on Florence dataset with different yaw angles .
Right : the qualitative comparison with VRN - Guided .
The first column is the input images from Florence dataset and the Internet , the second column is the reconstructed face from our method , the third column is the results from VRN - Guided .
Face Alignment
In the field of computer vision , face alignment is a long - standing problem which attracts a lot of attention .
In the beginning , there area number of 2D facial alignment approaches which aim at locating a set of fiducial 2 D facial landmarks , such as classic Active Appearance Model ( AMM ) and Constrained Local Models ( CLM ) .
Then cascaded regression and CNNbased methods are largely used to achieve state - of - the - art performance in 2D landmarks location .
However , 2D landmarks location only regresses visible points on faces , which leads to difficulties and is limited to describe face shape when the pose of faces is large .
Recent works then research the 3D facial alignment , which begins with fitting a 3 DMM or registering a 3 D facial template with a 2 D facial image .
Obviously , 3D reconstruction methods based on model can easily complete the task of 3D face alignment by selecting x ,y coordinates of landmarks vertices in reconstructed geometry .
Actually , are specially designated methods to achieve 3 D face alignment by means of 3 DMM fitting .
Recently use a deep network to directly predict the heat map to obtain the 3D facial landmarks and achieves state - of - the - art performance .
Thus , as sparse face alignment tasks are highly completed by aforementioned methods , the task of dense face alignment begins to develop .
Notice that , the dense face alignment means the methods should offer the correspondence between two face images as well as between a 2 D facial image and a 3 D facial reference geometry .
use multi-constraints to train a CNN which estimates the 3 DMM parameters and then provides a very dense 3D alignment .
directly learn the correspondence between a 2D input image and a 3D template via a deep network , while those predicted correspondence is not complete , only visible face - region is considered .
Compared to prior works , our method can directly establish the dense correspondence between all region in faces and 3D template once the position map is regressed .
No intermediate parameters such as 3 DMM coefficients and TPS warping parameters are needed in our method , which means our network can run very fast bypassing complex operations including perspective projection and TPS transformation .
Proposed Method
This section describes the framework and the details of our proposed method .
Firstly , we introduce the characteristics of the position map for our 3D face representation .
Then we elaborate the CNN architecture and the loss function designed specially for learning the mapping from unconstrained RGB image to its 3D structure .
The implementation details of our method are shown in the last subsection .
3D
Face Representation
Our goal is to regress the 3D facial geometry and its dense correspondence information from a single 2D image .
Thus we need a proper representation which can be directly predicted via a deep network .
One simple and commonly used idea is to concatenate the coordinates of all points in 3 D face as a vector and use a network to predict it .
However , this transformation increases the difficulties in training since projection from 3D space into 1 D vector discards the spatial adjacency information among points .
While it 's natural to think that spatially adjacent points could share weights in predicting their positions , which can be easily achieved by using convolutional layers .
The coordinates as a 1 D vector needs a fully connected layer to predict each point with much more parameters , which increases the network size and is hard to train .
proposed a point set generation network to directly predict the point cloud of 3 D object as a vector from a single image .
However , the max number of points is only 1024 , far from enough to represent an accurate 3 D face .
So model - based methods ] regress a few model parameters rather than the coordinates of points , which usually needs special care in training such as using Mahalanobis distance and inevitably limits the estimated face geometry to the their model space .
proposed 3D binary volume as the representation of 3D structure and uses Volumetric Regression Network ( VRN ) to output a 192 192 200 volume as the discretized version of point cloud .
By using this representation , VRN can be built with full convolutional layers .
However , discretization limits the resolution of point cloud , and most part of the network output correspond to non-surface points which are of less usage .
To address the problems in previous works , we propose UV position map as the presentation of full 3 D facial structure .
UV position map or position map for short , is a 2D image recording 3D positions of all points in UV space .
In the past years , UV space or UV coordinates , which is a 2D image plane parameterized from the 3D space , has been utilized as away to express information including the texture of faces ( texture map ) , 2.5D geometry ( height map ) and the correspondences between 3 D facial meshes .
Different from previous works , we use UV space to store the 3D coordinates of points from 3 D face model .
As shown in , we define the 3D face point cloud in Left - handed Cartesian Coordinate system .
The origin of the 3D space overlaps with the upper-left of the input image , with the positive x - axis pointing to the right of the image .
The ground truth 3 D face point cloud exactly matches the face in the 2D image when projected to the x -y plane .
Thus our position map can be easily comprehended as replacing the r , g , b value in texture map by x , y , z coordinates .
In order to keep the semantic meaning of points within position map , we create our UV coordinates based on 3 DMM .
Since we want to regress the 3D full structure directly , the unconstrained 2 D facial images and their corresponding 3D shapes are needed for end - to - end training .
300W - LP is a large dataset that contains more than 60K unconstrained images with fitted 3 DMM parameters , which is suitable to form our training pairs .
Besides , the 3 DMM parameters of this dataset are based on the Basel Face Model ( BFM ) .
Thus , in order to make full use of this dataset , we conduct the UV coordinates corresponding to BFM .
To be specific , we use the parameterized UV coordinates from which computes a Tutte embedding with conformal Laplacian weight and then maps the mesh boundary to a square .
Since the number of vertices in BFM is more than 50 K , we choose 256 as the position map size , which get a high precision point cloud with negligible re-sample error .
Thus our position map records a dense set of points from 3 D face with its semantic meaning , we are able to simultaneously obtain the 3D facial structure and dense alignment result by using a CNN to regress the position map directly from unconstrained 2D images .
The network architecture in our method could be greatly simplified due to this convenience .
Notice that the position map contains the information of the whole face , which makes it different from other 2D representations such as Projected Normalized Coordinate Code ( PNCC ) , an ordinary depth image or quantized UV coordinates , which only reserve the information of visible face region in the input image .
Our proposed position map also infers the invisible parts of face , thus our method can predict a complete 3 D face .
Since our network transfers the input RGB image into position map image , we employ an encoder - decoder structure to learn the transfer function .
The encoder part of our network begins with one convolution layer followed by 10 residual blocks which reduce the 256 256 3 input image into 8 8 512 feature maps , the decoder part contains 17 transposed convolution layers to generate the predicted 256 256 3 position map .
We use kernel size of 4 for all convolution or transposed convolution layers , and use ReLU layer for activation .
Given that the position map contains both the full 3D information and dense alignment result , we do n't need extra network module for multi -task during training or inferring .
The architecture of our network is shown in .
Network Architecture and Loss Function
In order to learn the parameters of the network , we build a novel loss function to measure the difference between ground truth position map and the network output .
Mean square error ( MSE ) is a commonly used loss for such learning task , such as in .
However , MSE treats all points equally , so it is not entirely appropriate for learning the position map .
Since central region of face has more discriminative features than other regions , we employ a weight mask to form our loss function .
As shown in , the weight mask is a gray image recording the weight of each point on position map .
It has the same size and pixel - to - pixel correspondence to position map .
According to our objective , we separate points into four categories , each has its own weights in the loss function .
The position of 68 facial keypoints has the highest weight , so that to ensure the network to learn accurate locations of these points .
The neck region usually attracts less attention , and is often occluded by hairs or clothes in unconstrained images .
Since learning the 3D shape of neck or clothes is beyond our interests , we assign 0 weight to points in neck region to reduce disturbance to the training process .
Thus , we denote the predicted position map as P ( x , y) for x , y representing each pixel coordinate .
Given the ground truth position mapP ( x , y) and weight
Training Details
Because our training process needs datasets containing both 2 D face images and its corresponding 3 D point clouds with semantic meaning , we choose e.g. , 300W - LP to form our training sets , since it contains face images across different angles with the annotation of estimated 3 DMM coefficients , from which the 3D point cloud could be easily recovered .
Specifically , we crop the images according the ground truth bounding box and rescale them to size 256 256 .
Then utilize their annotated 3 DMM parameters to generate the corresponding 3D position , and render them into UV space to obtain the ground truth position map , the map size in our training is also 256 256 , which means a precision of more than 45 K point cloud to regress .
Notice that , although our training data is generated from 3 DMM , our network 's output , the position map is not restricted to any face template or linear space of 3 DMM .
We perturb the training set by randomly rotating and translating the target face in 2D image plane .
To be specific , the rotation is from - 45 to 45 degree angles , translation changes is random from 10 percent of input size , and scale is from 0.9 to 1.2 .
Like , we also augment our training data by scaling color channels with scale range from 0.6 to 1.4 .
In order to handle images with occlusions , we synthesize occlusions by adding noise texture into raw images , which is similar to the work of .
With all above augmentation operations , our training data covers all the difficult cases .
We use the network described 3 to train our transfer model .
For optimization , we use Adam optimizer with a learning rate begins at 0.0001 and decays half after each 5 epochs .
The batch size is set as 16 .
All of our training codes are implemented with TensorFlow .
Experimental Results
In this part , we evaluate the performance of our proposed method on the tasks of 3D face alignment and 3D face reconstruction .
We first introduce the test datasets used in our experiments in section 4.1 .
Then in section 4.2 and 4.3 we compare our results with other methods in both quantitative and qualitative way .
We then compare our method 's runtime with other methods in section 4.4 .
In the end , the ablation study is conducted in section 4.5 to evaluate the effect of weight mask in our method .
Test Dataset
To evaluate our performance on the task of dense alignment and 3D face reconstruction , multiple test datasets listed below are used in our experiments :
AFLW2000 - 3D is constructed by to evaluate 3 D face alignment on challenging unconstrained images .
This database contains the first 2000 images from AFLW and expands its annotations with fitted 3 DMM parameters and 68 3D landmarks .
We use this database to evaluate the performance of our method on both face reconstruction and face alignment tasks .
AFLW - LFPA is another extension of AFLW dataset constructed by .
By picking images from AFLW according to the poses , the authors construct this dataset which contains 1299 test images with a balanced distribution of yaw angle .
Besides , each image is annotated with 13 additional landmarks as a expansion to only 21 visible landmarks in AFLW .
This database is evaluated on the task of 3D face alignment .
We use 34 visible landmarks as the ground truth to measure the accuracy of our results .
Florence is a 3 D face dataset that contains 53 subjects with its ground truth 3 D mesh acquired from a structured - light scanning system .
On experiments , each subject generates renderings with different poses as the same with : a pitch of - 15 , 20 and 25 degrees and spaced rotations between - 80 and 80 .
We compare the performance of our method on face reconstruction against other very recent state - of - the - art methods VRN - Guided and 3DDFA on this dataset .
3D Face Alignment
To evaluate the face alignment performance .
We employ the Normalized Mean Error ( NME ) to be the evaluation metric , bounding box size is used as the normalization factor .
Firstly , we evaluate our method on a sparse set of 68 facial landmarks , and compare our result with 3 DDFA , DeFA and 3D - FAN on dataset AFLW2000 - 3D .
As shown in figure 5 , our result slightly outperforms the state - of - the - art method 3D - FAN when calculating per distance with 2D coordinates .
When considering the depth value , the performance discrepancy between our method and 3D - FAN increases .
Notice that , the 3D - FAN needs another network to predict the z coordinate of landmarks , while the depth value can be obtained directly in our method .
To further investigate the performance of our method across poses and datasets , we also report the NME of faces with small , medium and large yaw angles on AFLW2000 - 3D dataset and the mean NME on both AFLW2000 - 3 D and AFLW - LPFA datasets .
shows the comparison with other methods , note that the numerical values are recorded from their published papers except the ones of 3D - FAN .
Follow the work , we also randomly select 696 faces from AFLW2000 to balance the distribution .
The result shows that our method is robust to the change of pose and datasets .
Although all the state - of - the - art methods of face alignment conduct evaluation on AFLW2000 - 3D dataset , the ground truth of this dataset is still controversial due to its annotation pipeline which is based on Landmarks Marching method in .
Thus , we visualize some results in that have NME larger than 6.5 % and we find our results are more accurate than the ground truth in some cases . :
Performance comparison on two large - pose face alignment datasets AFLW2000 - 3D ( 68 landmarks ) and AFLW - LFPA ( 34 visible landmarks ) .
The NME ( % ) for faces with different yaw angles are reported .
The first best result in each category is highlighted in bold , the lower is the better . :
Examples from AFLW2000 - 3 D dataset show that our predictions are more accurate than ground truth in some cases .
Green : predicted landmarks by our method .
Red : ground truth from .
We also compare our dense alignment result against other state - of - the - art methods including 3DDFA and DeFA on the only test dataset AFLW2000 - 3D .
In order to compare different methods with the same set of points , we select the points from the largest common face region provided by all methods , and finally around 45 K points were used for the evaluation .
As shown in figure 7 , our method outperforms the best methods with a large margin of more than 27 % on both 2 D and 3D coordinates . : CED curves on AFLW2000 - 3D .
Evaluation is performed on all points with both the 2D ( left ) and 3D ( right ) coordinates .
Overall 2000 images from AFLW2000 - 3D dataset are used here .
The mean NME % of each method is also showed in the legend .
Runtime
Surpassing the performance of all other state - of - the - art methods on 3D face alignment and reconstruction , our method is surprisingly more light - weighted and faster .
Since our network uses basic encoder - decoder structure , our model size is only 160 MB compared to 1.5 GB in VRN .
We also compare the runtime between our method and other state - of - the - art methods .
shows the result .
The results of 3DDFA and 3 DSTN are directly recorded from their published papers and others are recorded by running their publicly available source codes .
Notice that ,
We measure the run time of the process which is defined from inputing the cropped face image until recovering the 3D geometry ( point cloud , mesh or voxel data ) for 3D reconstruction methods or obtaining the 3D landmarks for alignment methods .
The harware used for evaluation is an NVIDIA GeForce GTX 1080 GPU and an Intel ( R ) Xeon ( R ) CPU E5-2640 v 4 @ 2.40 GHz .
Specifically , DeFA needs 11.8 ms ( GPU ) to predict 3 DMM parameters and another 23.6 ms ( CPU ) to generate mesh data from predicted parameters , 3DFAN needs 29.1 ms ( GPU ) to estimate 2D coordinates first and 25.6 ms ( GPU ) to obtain depth value , VRN - Guided detects 68 2D landmarks with 28.4 m s ( GPU ) , then regress the voxel data with 40.6 ms ( GPU ) , our method provides both 3D reconstruction and dense alignment result from cropped image in one pass in 9.8 ms ( GPU ) .
Ablation Study
In this section , we conduct several experiments to evaluate the influence of our weight mask on training and provide both sparse and dense alignment CED on AFLW2000 to evaluate different settings .
shows three options for our weight mask , we could see that weight ratio 1 corresponds to the situation when no weight mask is used , weight ratio 2 and 3 are slightly different on the emphasis in loss function .
The results are shown in .
Network trained without using weight mask has worst performance compared with other two settings .
By adding weights to specific regions such as 68 facial landmarks or central face region , weight ratio 3 shows considerable improvement on 68 points datasets over weight ratio 2 .
Conclusion
In this paper , we propose an end - to - end method , which well solves the problems of 3 D face alignment and 3D face reconstruction simultaneously .
By learning the position map , we directly regress the complete 3D structure along with semantic meaning from a single image .
Quantitative and qualitative results demonstrate our method is robust to poses , illuminations and occlusions .
Experiments on three test datasets show that our method achieves significant improvements over others .
We further show that our method runs faster than other methods and is suitable for real time usage .
