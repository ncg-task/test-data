title
Unsupervised Training for 3D Morphable Model Regression
abstract
We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs .
The training loss is based on features from a facial recognition network , computed onthe-fly by rendering the predicted faces with a differentiable renderer .
To make training from features feasible and avoid network fooling effects , we introduce three objectives : a batch distribution loss that encourages the output distribution to match the distribution of the morphable model , a loopback loss that ensures the network can correctly reinterpret its own output , and a multi-view identity loss that compares the features of the predicted 3 D face and the input photograph from multiple viewing angles .
We train a regression network using these objectives , a set of unlabeled photographs , and the morphable model itself , and demonstrate state - of - the - art results .
Introduction
A 3D morphable face model ( 3 DMM ) provides a smooth , low - dimensional " face space " spanning the range of human appearance .
Finding the coordinates of a person in this space from a single image of that person is a common task for applications such as 3D avatar creation , facial animation transfer , and video editing ( e.g. ) .
The conventional approach is to search the space through inverse rendering , which generates a face that matches the photograph by optimizing shape , texture , pose , and lighting parameters .
This approach requires a complex , nonlinear optimization that can be difficult to solve in practice .
Recent work has demonstrated fast , robust fitting by regressing from image pixels to morphable model coordinates using a neural network .
The major issue with the regression approach is the lack of ground - truth 3 D face data for training .
Scans of face geometry and texture are difficult to acquire , both because of expense and privacy considerations .
Previous approaches have explored synthesizing training pairs of image and morphable model coordinates in a preprocess , or training an image -.
Neutral 3D faces computed from input photographs using our regression network .
We map features from a facial recognition network into identity parameters for the Basel 2017 Morphable Face Model .
to - image autoencoder with a fixed , morphable - model - based decoder and an image - based loss .
This paper presents a method for training a regression network that removes both the need for supervised training data and the reliance on inverse rendering to reproduce image pixels .
Instead , the network learns to minimize a loss based on the facial identity features produced by a face recognition network such as VGG - Face or Google 's FaceNet .
These features are robust to pose , expression , lighting , and even non-photorealistic inputs .
We exploit this invariance to apply a loss that matches the identity features between the input photograph and a synthetic rendering of the predicted face .
The synthetic rendering need not have the same pose , expression , or lighting of the photograph , allowing our network to predict only shape and texture .
Simply optimizing for similarity between identity features , however , can teach the regression network to fool the recognition network by producing faces that match closely in feature space but look unnatural .
We alleviate the fooling problem by applying three novel losses : a batch distribution loss to match the statistics of each training batch to the statistics of the morphable model , a loopback loss to ensure the regression network can correctly reinterpret its own output , and a multi-view identity loss that combines features from multiple , independent views of the predicted shape .
Using this scheme , we train a 3D shape and texture regression network using only a face recognition network , a morphable face model , and a dataset of unlabeled face images .
We show that despite learning from unlabeled photographs , the 3D face results improve on the accuracy of previous work and are often recognizable as the original subjects .
Related Work
Morphable 3D
Face Models
Blanz and Vetter introduced the 3D morphable face model as an extension of the 2D active appearance model .
They demonstrated face reconstruction from a single image by iteratively fitting a linear combination of registered scans and pose , camera , and lighting parameters .
They decomposed the geometry and texture of the face scans using PCA to produce separate , reduced - dimension geometry and texture spaces .
Later work added more face scans and extended the model to include expressions as another separate space .
We build directly off of this work by using the PCA weights as the output of our network .
Convergence of iterative fitting is sensitive to the initial conditions and the complexity of the scene ( i.e. , lighting , expression , and pose ) .
Subsequent work ( and others ) has applied a range of techniques to improve the accuracy and stability of the fitting , producing very accurate results under good conditions .
However , iterative approaches are still unreliable under general , in - the - wild , conditions , leading to the interest in regression - based approaches .
Learning to Generate 3D Face Models
Deep neural networks provide the ability to learn a regression from image pixels to 3D model parameters .
The chief difficulty becomes how to collect enough training data to feed the network .
One solution is to generate synthetic training data by drawing random samples from the morphable model and rendering the resulting faces .
However , a network trained on purely synthetic data may perform poorly when faced with occlusions , unusual lighting , or ethnicities that are not well - represented by the morphable model .
We include randomly generated , synthetic faces in each training batch to provide ground truth 3D coordinates , but train the network on real photographs at the same time .
Tran et al .
address the lack of training data by using an iterative optimization to fit an expressionless model to a large number of photographs , and treat results where the optimization converged as ground truth .
To generalize to faces with expression , identity labels and at least one neutral image are required , so the potential size of the training dataset is restricted .
We also directly predict a neutral expression , but our unsupervised approach removes the need for an initial iterative fitting step .
An approach closely related to ours was recently proposed by Tewari , et al. , who train an autoencoder network on unlabeled photographs to predict shape , expression , texture , pose , and lighting simultaneously .
The encoder is a regression network from images to morphablemodel coordinates , and the decoder is a fixed , differentiable rendering layer that attempts to reproduce the input photograph .
Like ours , this approach does not require supervised training pairs .
However , since the training loss is based on individual image pixels , the network is vulnerable to confounding variation between related variables .
For example , it can not readily distinguish between dark skin tone and a dim lighting environment .
Our approach exploits a pretrained face recognition network , which distinguishes such related variables by extracting and comparing features across the entire image .
Other recent deep learning approaches predict depth maps or voxel grids , trading off a compact and interpretable output mesh for more faithful reproductions of the input image .
As for , identity and expression are confounded in the output mesh .
The result maybe suitable for image processing tasks , such as relighting , at the expense of animation tasks such as rigging .
Facial Identity Features
Current face recognition networks achieve high accuracy over millions of identities .
The networks operate by embedding images in a high - dimensional space , where images of the same person map to nearby points .
Recent work has shown that this mapping is somewhat reversible , meaning the features can be used to produce a likeness of the original person .
We build on this work and use FaceNet to both produce input features for our regression network , and to verify that the output of the regression resembles the input photograph ..
End - to - end computation graph for unsupervised training of the 3 DMM regression network .
Training batches consist of combinations of real ( blue ) and synthetic ( red ) face images .
Identity , loopback and batch distribution losses are applied to real images , while the 3 DMM parameter loss is applied to synthetic images .
The regression network ( yellow ) is shown in two places , but both correspond to the same instance during training .
The identity encoder network is fixed during training .
Model
We employ an encoder - decoder architecture that permits end - to - end unsupervised learning of 3D geometry and texture morphable model parameters ) .
Our training framework utilizes a realistic , parameterized illumination model and differentiable renderer to form neutralexpression face images under varying pose and lighting conditions .
We train our model on hybrid batches of real face images from VGG - Face and synthetic faces constructed from the Basel Face 3DMM .
The main strength and novelty of our approach lies in isolating our loss function to identity .
By training the model to preserve identity through conditions of varying expression , pose , and illumination , we are able to avoid network fooling and achieve robust state - of - the - art recognizability in our predictions .
Encoder
We use FaceNet for the network encoder , since its features have been shown to be effective for generating face images .
Other facial recognition networks such as VGG - Face , or even networks not focused on recognition , may work equally well .
The output of the encoder is the penultimate , 1024 - D avgpool layer of the " NN2 " FaceNet architecture .
We found the avgpool layer more effective than the final , 128 - D normalizing layer as input to the decoder , but use the normalizing layer for our identity loss ( Sec. 3.3.2 ) .
Decoder
Given encoder outputs generated from a face image , our decoder generates parameters for the Basel Face Model 2017 3 DMM .
The Basel 2017 model generates shape
Here , s , t ? R 199 and e ?
R 100 are shape , texture , and expression parameterization vectors with standard normal distributions ; S , T ?
R 3N are the average face shape and texture ; P SS , PT ? R 3N 199 and P SE ?
R 3N 100 are linear PCA bases ; and W SS , W T ? R 199199 and W SE ?
R 100100 are diagonal matrices containing the square roots of the corresponding PCA eigenvalues .
The decoder is trained to predict the 398 parameters constituting the shape and texture vectors , sand t , fora face .
The expression vector e is not currently predicted and is set to zero .
The decoder network consists of two 1024 - unit fully connected + ReLU layers followed by a 398 - unit regression layer .
The weights were regularized towards zero .
Deeper networks were considered , but they did not significantly improve performance and were prone to overfitting .
Differentiable Renderer
In contrast to previous approaches that backpropagate loss through an image , we employ a general - purpose , differentiable rasterizer based on a deferred shading model .
The rasterizer produces screen - space buffers containing triangle IDs and barycentric coordinates at each pixel .
After rasterization , per-vertex attributes such as colors and normals are interpolated at the pixels using the barycentric coordinates and IDs .
This approach allows rendering with full perspective and any lighting model that can be computed in screen - space , which prevents image quality from being a bottleneck to accurate training .
The source code for the renderer is publicly available 1 .
The rasterization derivatives are computed for the barycentric coordinates , but not the triangle IDs .
We extend the definition of the derivative of barycentric coordinates with respect to vertex positions to include negative barycentric coordinates , which lie outside the border of a triangle .
Including negative barycentric coordinates and omitting triangle IDs effectively treats the shape as locally planar , which is an acceptable approximation away from occlusion boundaries .
Faces are largely smooth shapes with few occlusion boundaries , so this approximation is effective in our case , but it could pose problems if the primary source of loss is related to translation or occlusion .
Illumination Model
Because our differentiable renderer uses deferred shading , illumination is computed independently per-pixel with a set of interpolated vertex attribute buffers computed for each image .
We use the Phong reflection model for shading .
Because human faces exhibit specular highlights , Phong reflection allows for improved realism over purely diffuse approximations , such as those used in MoFA .
It is both efficient to evaluate and differentiable .
To create appropriately even lighting , we randomly position two point light sources of varying intensity several meters from the face to be illuminated .
We select a random color temperature for each training image from approximations of common indoor and outdoor light sources , and perturb the color to avoid overfitting .
Finally , since the Basel Face model does not contain specular color information , we use a heuristic to define specular colors K s from the diffuse colors K d of the predicted model :
Losses
We propose a novel loss function that focuses on facial identity , and ignores variations in facial expression , illumination , pose , occlusion , and resolution .
This loss function is conceptually straightforward and enables unsupervised endto - end training of our network .
It combines four terms :
Here , L param imposes 3D shape and texture similarity for the synthetic images , L id imposes identity preservation on 1 http://github.com/google/tf_mesh_renderer
the real images in a batch , L batchdistr regularizes the predicted parameter distributions within a batch to the distribution of the morphable model , and L loopback ensures the network can correctly interpret its own output .
The effects of removing the batch distribution , loopback , and limiting the identity loss to a single view are shown in .
We use ? batch = 10.0 and ?
loop = 0.07 for our results .
Training proceeds in two stages .
First , the model is trained solely on batches of synthetic faces generated by randomly sampling for shape , texture , pose , and illumination parameters .
This stage performs only a partial training of the model : since shape and texture parameters are sampled independently in this stage , the model is restricted from learning correlations between them .
Second , the partiallytrained model is trained to convergence on batches consisting of a combination of real face images from the VGG - Face dataset and synthetic faces .
Synthetic faces are subject to only the L param loss , while real face images are subject to all losses except L param .
Parameter Loss
For synthetic faces , the true shape and texture parameters are known , so we use independent Euclidean losses between the randomly generated true synthetic parameter vectors , s b andt b , and the predicted ones , s band tb , in a batch .
where ?
sand ?
t control the relative contribution of the shape and texture losses .
Due to different units , we set ? s = 0.4 and ? t = 0.002 .
Identity Loss
Robust prediction of recognizable meshes can be facilitated with a loss that derives from a facial recognition network .
We used FaceNet , though the identity - preserving loss generalizes to other networks such as VGG - Face .
The final FaceNet normalizing layer is a 128 - D unit vector such that , regardless of expression , pose , or illumination , same - identity inputs map closer to each other on the hypersphere than different - identity ones .
For our identity loss L id , we define similarity of two faces as the cosine score of their respective output unit vectors , ? 1 and ?
2 :
To use this loss in an unsupervised manner on real faces , we calculate the cosine score between a face image and the image resulting from passing the decoder outputs into the differentiable renderer with random pose and illumination .
Identity prediction can be further enhanced by using multiple poses for each face .
Multiple poses decrease the presence of occluded regions of the mesh .
Additionally , since each pose provides a backpropagation path to the mesh vertices , the model trains in a more robust manner than if only a single pose is used .
We use three randomly determined poses for each real face .
Batch Distribution
Loss
Applying the identity loss alone allows training to introduce biases into the decoder outputs that change their distribution from the zero- mean standard normal distribution assumption made by the Basel Face Model .
These changes are likely due to domain transfer effects between the real images and those rendered from the decoder outputs .
Initially , we attempted to compensate for these effects by adding a shallow network to transform the model - rendered encoder outputs prior to calculating the identity loss .
While this approach did increase overall recognizability in the model , it also introduced unrealistic artifacts into the model outputs .
Instead , we opted to regularize each batch to directly constrain the lowest two moments of the shape and texture parameter distributions to match those of a zeromean standard normal distribution .
This loss , which is applied at a batch level , combines four terms :
Here , L Sand L T regularize the batch shape and texture parameters to have zero mean , and L ?
Sand L ?
T regularize them to have unit variance .
Loopback Loss
A limitation of using real face images for unsupervised training is that the true shape and texture parameters for the faces are unknown .
If they were known , then the more direct lower - level parameter loss in Sec. 3.3.1 could be directly imposed instead of the identity loss in Sec. 3 .
3.2 .
A close approximation to this lower - level loss for real images can be achieved using a " loopback " loss ( .
The nature of this loss lies in generalizing the model near the regions for which real face image data exists .
Similar techniques have proven to be successful in generalizing model learning for image applications .
To compute the loopback loss at any training step , the current - state decoder outputs fora batch of real face images are extracted and used to generate synthetic faces rendered in random poses and illuminations .
The synthetic faces are then passed back through the encoder and decoder again , and the parameter loss in Sec. 3.3.1 is imposed between the resulting parameters and those first output by the decoder .
As shown in , two loopback loss backpropagation paths to the decoder exist .
The effects of each are complementary : the synthetic face parameter path generalizes the decoder in the region near that of real face parameters , and the real image channel regularizes the decoder away from generating unrealistic faces .
Additionally , the two paths encourage the regression network to match its responses for real and synthetic versions of the same face .
Experiments
We first show and discuss the qualitative improvements of our method compared with other morphable model regression approaches ( Sec. 4.1 ) .
We then evaluate our method quantitatively by comparing reconstruction error against scanned 3 D face geometry ( Sec. 4.2 ) and features produced by VGG - Face , which was not used for training ( Sec. 4.3 and 4.4 ) .
We also show qualitative results on corrupted and non-photorealistic inputs ( Sec. 4.5 ) .
Tran MoFA MoFA + Exp Sela .
Results on the MoFA - Test dataset .
Our method shows improved likeness and color fidelity over competing methods , especially in the shape of the eyes , eyebrows , and nose .
Note that MoFA solves for pose , expression , lighting , and identity , so is shown both with ( row 5 ) and without ( row 4 ) expression .
The unstructured method of Sela , et al. produces only geometry , so is shown without color .
Qualitative Comparison
as part of MoFA .
An extended evaluation is available in the supplemental material .
Our method improves on the likenesses of previous approaches , especially in features relevant to facial recognition such as the eyebrow texture and nose shape .
Our method also predicts coloration and skin tone more faithfully .
This improvement is likely a consequence of our batch distribution loss , which allows individual faces to vary from the mean of the Basel model ( light skin tone ) , so long as the faces match the mean in aggregate .
Previous methods , by contrast , regularize each face towards the mean of the model 's distribution , tending to produce light skin tone overall .
The MoFA approach also sometimes confounds identity and expression ( , and skin tone and lighting .
Our method and Tran et al. are more resistant to confounding variables .
The unstructured method of Sela et al. does not sepa-rate identity and expression , predicts only shape , and is less robust than the model - based methods .
Neutral Pose Reconstruction on MICC
We quantitatively evaluate the ground - truth accuracy of our models on the MICC Florence 3D Faces dataset ( MICC ) in .
This dataset contains the ground truth scans of 53 Caucasian subjects in a neutral expression .
Accompanying the scans are three observation videos for each subject , in conditions of increasing difficulty : ' cooperative ' , ' indoor ' , and ' outdoor . '
We run the methods on each frame of the videos , and average the results over each video to get a single reconstruction .
The results of Tran et al. are averaged over the mesh , as in .
We instead average our encoder embeddings before making a single reconstruction .
To evaluate our predictions , we crop the ground truth scan to 95 mm around the tip of the nose as in , and run ICP with isotropic scale to find an alignment .
We solve for isotropic scale because we do not assume the methods predict absolute scale , and a small misalignment in scale can have a large effect on error .
shows the symmetric point - to - plane distance in millimeters within the ICPdetermined region of intersection , rather than point - to - point distances , as the methods and ground truth have different vertex densities .
Our results indicate that we have improved absolute error to the ground truth by 20 - 25 % , and our results are more consistent from person to person , with less than half the standard deviation when compared to .
We are also more stable across changing environments , with similar results for all three test sets .
Face Recognition Results
In order to quantitatively evaluate the likeness of our reconstructions , we use the VGG - Face recognition network 's activations as a measure of similarity .
VGG - Face was chosen because FaceNet appears in our training loss , making it unsuitable as an evaluation metric .
For each face in our evaluation datasets , we compute the cosine similarity of the ?( t ) layers of VGG - Face between the input image and a rendering of our output geometry , as described in .
The similarity distributions for Labeled Faces in the Wild ( LFW ) , MICC , and MoFA - Test are shown in .
The similarity between all pairs of photographs in the LFW dataset , separated into same - person and differentperson distributions , is shown for comparison in , top .
Our method achieves an average similarity between rendering and photo of 0.403 on MoFA test ( the dataset for which results for all methods are available ) .
By comparison , 22.7 % of pairs of photos of the same person in LFW have a score below 0.403 , and only 0.04 % of pairs of photos of different people have a score above 0.403 .
For additional validation , shows the Earth Mover 's distance between the all - pairs LFW distributions and the results of each method .
Our method 's results are closer to the same - person distribution than the differentperson distribution in all cases , while the other methods results ' are closer to the different - person distribution .
We conclude that ours is the first method that generates neutralpose , 3 D faces with recognizability approaching a photo .
The scores of the ground - truth 3 D face scans from MICC .
Earth mover 's distance between distributions of VGG - Face ?( t ) similarity and distributions of same and different identities on LFW .
A low distance for " Same " means the similarity scores between a photo and its associated 3 D rendering are close to the scores of same identity photos in LFW , while a low distance for " Diff . " means the scores are close to the scores of different identity photos .
and their input photographs provide a ceiling for similarity scores .
Notably , the distance between the GT distribution and the same - person LFW distribution is very low , with almost the same mean ( 0.51 vs 0.50 ) , indicating the VGG - Face network has little trouble bridging the domain gap between photograph and rendering , and that our method does not yet reach the ground - truth baseline .
MoFA - Test
LFW
Method
Top - 1 Top - 5 Top - 1 Top - 5 random 0.01 0.06 0.0002 0.001 MoFA 0.19 0.54 ? ?
Tran et al . 0.25 0.62 0.001 0.002 ours 0.87 0.96 0.16 0.51 . Identity Clustering Recall using VGG - Face distances on MoFA - Test and LFW .
Given a rendered mesh , the task is to recover the unknown source identity by looking up the nearest neighbor photographs according to VGG - Face ?( t ) cosine similarity .
Top - 1 and Top - 5 show the fractions for which a photograph of the correct identity was recalled as the nearest neighbor , or in the nearest 5 , respectively .
Performance is higher for MoFA - Test because it contains 84 images and 78 identities , while the LFW set contains 12,993 images and 5,749 identities ..
FERET dataset stress test .
The regression network is robust to changes in pose , lighting , expression , occlusion , and blur .
See supplemental material for additional results .
Face Clustering
To establish that our reconstructions are recognizable , we perform a clustering task to recover the identities of our generated meshes .
For each of LFW and MoFA - Test , we run our method on all faces in the dataset , and render the output geometry as shown in the figures in this paper .
For each rendering , we find the nearest neighbors according to the VGG - Face ?( t ) distance .
shows the fraction of meshes that recall a photo of the source identity as the nearest neighbor , and within the top 5 nearest neighbors .
On MoFA - Test , which has 84 images and 78 identities , we achieve a Top - 1 recall of 87 % , compared to 25 % for Tran et al. and 19 % for MoFA .
On the larger LFW dataset , which contains over 5,000 identities in 13,000 photographs , we still achieve a Top - 5 recall of 51 % .
We conclude our approach generates recognizable 3D morphable models , even in test sets with thousands of candidate identities .
Reconstruction from Challenging Images
Our regression network uses a facial identity feature vector as input , yielding results robust to changes in pose , expression , lighting , occlusion , and resolution , while remaining sensitive to changes in identity .
qualitatively demonstrates this robustness by varying conditions fora single subject and displaying consistent output ..
Art from the BAM dataset .
Because the inputs to our regression network are high - level identity features , the results are robust to stylized details at the pixel level .
Additionally , shows that our network can reconstruct plausible likenesses from non-photorealistic artwork , in cases where a fitting approach based on inverse rendering would have difficulty .
This result is possible because of the invariance of the identity features to unrealistic pixel - level information , and because our unsupervised loss focuses on aspects of reconstruction that are important for recognition .
Discussion and Future Work
We have shown it is possible to train a regression network from images to neutral , expressionless 3D morphable model coordinates using only unlabeled photographs and improve on the accuracy of supervised methods .
Our results approach the face recognition similarity scores of real photographs and exceed the scores of other regression approaches by a large margin .
Because of the accuracy of the approach , the predicted face can be directly used for facetracking based on landmarks .
This paper focuses on learning an expressionless face , which is suitable for creating VR avatars or landmark - based tracking .
In future work , we hope to extend the approach to predict pose , expression , and lighting , similar to Tewari , et al . .
Predicting these factors while avoiding their confounding effects should be possible by adding an inverse rendering stage to our decoder while maintaining the neutral - pose losses we currently apply .
The method produces generally superior results for young adults and Caucasian ethnicities .
The differences could be due to limited representation in the scans used to produce the morphable model , bias in the features extracted from the face recognition network , or limited representation in the VGG - Face dataset we use for training .
In future work , we hope to improve the performance of the method on a diverse range of ages and ethnicities ..
Occlusion Stress
Test on subjects from the MICC and FERET dataset .
We increase occlusion in the input image until our algorithm no longer predicts accurate features .
Facial features smoothly degrade as the necessary information is no longer present in the input image .
( bottom ) datasets .
Beginning with a frontal image of the subject , we apply a progressively larger gaussian blur kernel to examine the effect of lost detail in the input .
For the female subject , global shape begins to change subtly as the blur becomes extreme .
For both subjects , fine detail in the eyebrow shape and thickness is lost as the input is increasingly blurred .
Input
Ours
Tran MoFA
Input Ours Tran MoFA
Input Ours
Tran MoFA
Input Ours Tran MoFA
A.1 . Fitting Pose and Expression
Our system reconstructs shape and texture of faces , and ignores aspects such as pose , expression , and lighting .
Those components are needed to exactly match the reconstruction to the source image , and our neutral face output is an excellent starting point to find them .
shows results of gradient descent that starts with our output and fits the pose and expression by minimizing the distances of landmarks on our mesh and the image ( we used the 68 landmark configuration from the Multi - PIE database ) .
