249	3	18	compare against
249	43	68	logistic regression model
249	69	73	with
249	74	87	bag - ofwords
249	92	139	bidirectional gated recurrent unit ( Bi - GRU )
249	146	186	single - layer 1 D convolutional network
250	42	45	for
250	46	89	multi-label classification of clinical text
250	8	20	compare with
250	102	141	Condensed Memory Networks ( C - MemNN )
250	144	158	Attentive LSTM
250	163	195	Convolutional Attention ( CAML )
205	53	87	https://github.com/guoyinwang/LEAM
199	9	12	use
199	13	53	300 - dimensional Glo Ve word embeddings
199	72	75	for
199	76	112	word embeddings and label embeddings
200	0	35	Out - Of - Vocabulary ( OOV ) words
200	40	56	initialized from
200	59	79	uniform distribution
200	80	84	with
200	85	108	range [ ? 0.01 , 0.01 ]
201	4	20	final classifier
201	24	38	implemented as
201	42	51	MLP layer
201	52	63	followed by
201	66	93	sigmoid or softmax function
203	0	22	Dropout regularization
203	26	37	employed on
203	42	57	final MLP layer
203	60	64	with
203	65	77	dropout rate
203	78	81	0.5
204	4	9	model
204	13	30	implemented using
204	31	41	Tensorflow
204	49	59	trained on
204	60	71	GPU Titan X
202	3	8	train
202	9	32	our model 's parameters
202	33	37	with
202	42	81	Adam Optimizer ( Kingma and Ba , 2014 )
202	84	88	with
202	92	113	initial learning rate
202	114	116	of
202	117	122	0.001
202	131	145	minibatch size
202	146	148	of
202	149	152	100
35	68	81	making use of
35	86	111	label embedding framework
35	41	48	propose
35	130	172	Label - Embedding Attentive Model ( LEAM )
35	173	183	to improve
35	184	203	text classification
37	4	17	proposed LEAM
37	21	35	implemented by
37	36	53	jointly embedding
37	58	72	word and label
37	73	75	in
37	80	97	same latent space
37	108	128	text representations
37	133	159	constructed directly using
37	164	190	text - label compatibility
38	4	29	label embedding framework
38	76	113	Label - attentive text representation
38	117	132	informative for
38	137	167	downstream classification task
39	11	34	LEAM learning procedure
39	40	48	involves
39	51	87	series of basic algebraic operations
2	40	59	Text Classification
256	0	4	LEAM
256	5	13	provides
256	18	32	best AUC score
256	39	63	better F1 and P@5 values
256	64	68	than
256	69	80	all methods
256	81	87	except
256	88	91	CNN
257	0	3	CNN
257	4	28	consistently outperforms
257	33	60	basic Bi - GRU architecture
257	71	99	logistic regression baseline
257	100	108	performs
257	109	114	worse
257	115	119	than
257	120	151	all deep learning architectures
