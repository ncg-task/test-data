title
Joint Embedding of Words and Labels for Text Classification
abstract
Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences. We propose to view text classification as a label-word joint embedding problem: each label is embedded in the same space with the word vectors. We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels. The attention is learned on a training set of labeled samples to ensure that, given a text sequence, the relevant words are weighted higher than the irrelevant ones. Our method maintains the interpretability of word embeddings, and enjoys a built-in ability to leverage alternative sources of information, in addition to input text sequences. Extensive results on the several large text datasets show that the proposed framework outperforms the state-of-the-art methods by a large margin, in terms of both accuracy and speed. U B Q r rd W a j 2 Y 9 N 2 Mb D h P x o TD / 4 R L x 6 0 u 3 IQ c J K m L + + 9 a W em D W L O j PX 9 D 6 + w s r q 2
J A W sq J M Z 2 6 H 9 t e Sr R ll I M 7 M z E Q E / p MB t B x U B I B pp f m M 5 n i Y 8 e E O FL a L W l x z v 7 NS I kw W X 3 O 6 Z o em k Ut I / / T O om N Ln op k 3 F i Q d L f i 6 K E Y 6 t w N m Ac Mg 3 U 8 r E D h Gr ma s V 0 SD S h 1 j 1 Dy Q 2 h v t j y M mi d 1 i 5 r / u 1 Z t X E 1 m 0 Y R Ha I j d IL q 6 B w 1 0 A 1 q oh a i 6 B V 9 om 8 P e e / e U B Q r rd W a j 2 Y 9 N 2 Mb D h P x o TD / 4 R L x 6 0 u 3 IQ c J K m L + + 9 a W em D W L O j PX 9 D 6 + w s r q 2 v l H c LG 2 Vt 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G OJ P Q s s x ya M ca i Ag 4 PA b P 1 5 n + O A J t m J L 3 d h x D T 5 CB Z B G j x D q q X 3 n q Sn i h S g g i w 7 R 7 J 4 id pm k 3 i PD dd F q a 0 w K w Z J S Li o d m L N y G c 3 L R O F l 0 T T L H Z NS v VP 2 an w d e B v U Z q K J Z NP u Vt 2 6 o a C J A W sq J M Z 2 6 H 9 t e Sr R ll I M 7 M z E Q E / p MB t B x U B I B pp f m M 5 n i Y 8 e E O FL a L W l x z v 7 NS I kw W X 3 O 6 Z o em k Ut I / / T O om N Ln op k 3 F i Q d L f i 6 K E Y 6 t w N m Ac Mg 3 U 8 r E D h Gr ma s V 0 SD S h 1 j 1 Dy Q 2 h v t j y M mi d 1 i 5 r / u 1 Z t X E 1 m 0 Y R Ha I j d IL q 6 B w 1 0 A 1 q oh a i 6 B V 9 om 8 P e e / e U B Q r rd W a j 2 Y 9 N 2 Mb D h P x o TD / 4 R L x 6 0 u 3 IQ c J K m L + + 9 a W em D W L O j PX 9 D 6 + w s r q 2 v l H c LG 2 Vt 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G OJ P Q s s x ya M ca i Ag 4 PA b P 1 5 n + O A J t m J L 3 d h x D T 5 CB Z B G j x D q q X 3 n q Sn i h S g g i w 7 R 7 J 4 id pm k 3 i PD dd F q a 0 w K w Z J S Li o d m L N y G c 3 L R O F l 0 T T L H Z NS v VP 2 an w d e B v U Z q K J Z NP u Vt 2 6 o a C J A W sq J M Z 2 6 H 9 t e Sr R ll I M 7 M z E Q E / p MB t B x U B I B pp f m M 5 n i Y 8 e E O FL a L W l x z v 7 NS I kw W X 3 O 6 Z o em k Ut I / / T O om N Ln op k 3 F i Q d L f i 6 K E Y 6 t w N m Ac Mg 3 U 8 r E D h Gr ma s V 0 SD S h 1 j 1 Dy Q 2 h v t j y M mi d 1 i 5 r / u 1 Z t X E 1 m 0 Y R Ha I j d IL q 6 B w 1 0 A 1 q oh a i 6 B V 9 om 8 P e e / e
Introduction
Text classification is a fundamental problem in natural language processing (NLP). The task is to annotate a given text sequence with one (or multiple) class label(s) describing its textual content. A key intermediate step is the text representation. Traditional methods represent text with hand-crafted features, such as sparse lexical features (e.g., n-grams). Recently, neural models have been employed to learn text representations, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs) based on long short-term memory (LSTM).
To further increase the representation flexibility of such models, attention mechanisms have been introduced as an integral part of models employed for text classification. The attention module is trained to capture the dependencies that make significant contributions to the task, regardless of the distance between the elements in the sequence. It can thus provide complementary information to the distance-aware dependencies modeled by RNN/CNN. The increasing representation power of the attention mechanism comes with increased model complexity.
Alternatively, several recent studies show that the success of deep learning on text classification largely depends on the effectiveness of the word embeddings. Particularly, quantitatively show that the word-embeddings-based text classification tasks can have the similar level of difficulty regardless of the employed models, using the concept of intrinsic dimension. Thus, simple models are preferred. As the basic building blocks in neural-based NLP, word embeddings capture the similarities/regularities between words. This idea has been extended to compute embeddings that capture the semantics of word sequences (e.g., phrases, sentences, paragraphs and documents). These representations are built upon various types of compositions of word vectors, ranging from simple averaging to sophisticated architectures. Further, they suggest that simple models are efficient and interpretable, and have the poten-arXiv:1805.04174v1 [cs.CL] 10 May 2018 tial to outperform sophisticated deep neural models.
It is therefore desirable to leverage the best of both lines of works: learning text representations to capture the dependencies that make significant contributions to the task, while maintaining low computational cost. For the task of text classification, labels play a central role of the final performance. A natural question to ask is how we can directly use label information in constructing the text-sequence representations.

Our Contribution
Our primary contribution is therefore to propose such a solution by making use of the label embedding framework, and propose the Label-Embedding Attentive Model (LEAM) to improve text classification. While there is an abundant literature in the NLP community on word embeddings (how to describe a word) for text representations, much less work has been devoted in comparison to label embeddings (how to describe a class). The proposed LEAM is implemented by jointly embedding the word and label in the same latent space, and the text representations are constructed directly using the text-label compatibility.
Our label embedding framework has the following salutary properties: (i) Label-attentive text representation is informative for the downstream classification task, as it directly learns from a shared joint space, whereas traditional methods proceed in multiple steps by solving intermediate problems.
(ii) The LEAM learning procedure only involves a series of basic algebraic operations, and hence it retains the interpretability of simple models, especially when the label description is available. (iii) Our attention mechanism (derived from the text-label compatibility) has fewer parameters and less computation than related methods, and thus is much cheaper in both training and testing, compared with sophisticated deep attention models. (iv) We perform extensive experiments on several text-classification tasks, demonstrating the effectiveness of our label-embedding attentive model, providing state-of-the-art results on benchmark datasets. (v) We further apply LEAM to predict the medical codes from clinical text. As an interesting by-product, our attentive model can highlight the informative key words for prediction, which in practice can reduce a doctor's burden on reading clinical notes.

Related Work
Label embedding has been shown to be effective in various domains and tasks. In computer vision, there has been avast amount of research on leveraging label embeddings for image classification, multimodal learning between images and text, and text recognition in images. It is particularly successful on the task of zero-shot learning, where the label correlation captured in the embedding space can improve the prediction when some classes are unseen. In NLP, labels embedding for text classification has been studied in the context of heterogeneous networks in and multitask learning in, respectively. To the authors' knowledge, there is little research on investigating the effectiveness of label embeddings to design efficient attention models, and how to joint embedding of words and labels to make full use of label information for text classification has not been studied previously, representing a contribution of this paper.
For text representation, the currently bestperforming models usually consist of an encoder and a decoder connected through an attention mechanism, with successful applications to sentiment classification , sentence pair modeling and sentence summarization. Based on this success, more advanced attention models have been developed, including hierarchical attention networks, attention over attention, and multi-step attention. The idea of attention is motivated by the observation that different words in the same context are differentially informative, and the same word maybe differentially important in a different context. The realization of "context" varies in different applications and model architectures. Typically, the context is chosen as the target task, and the attention is computed over the hidden layers of a CNN/RNN. Our attention model is directly builtin the joint embedding space of words and labels, and the context is specified by the label embedding.
Several recent works have demonstrated that sim-ple attention architectures can alone achieve stateof-the-art performance with less computational time, dispensing with recurrence and convolutions entirely. Our work is in the same direction, sharing the similar spirit of retaining model simplicity and interpretability. The major difference is that the aforementioned work focused on self attention, which applies attention to each pair of word tokens from the text sequences. In this paper, we investigate the attention between words and labels, which is more directly related to the target task. Furthermore, the proposed LEAM has much less model parameters.

Preliminaries
Throughout this paper, we denote vectors as bold, lower-case letters, and matrices as bold, uppercase letters. We use for element-wise division when applied to vectors or matrices. We use ? for function composition, and ? p for the set of one hot vectors in dimension p.
Given a training set S = {(X n , y n )} N n=1 of pair-wise data, where X ? X is the text sequence, and y ? Y is its corresponding label. Specifically, y is a one hot vector in single-label problem and a binary vector in multi-label problem, as defined later in Section 4.1. Our goal for text classification is to learn a function f : X ? Y by minimizing an empirical risk of the form:
where ? : Y ? Y ? R measures the loss incurred from predicting f (X) when the true label is y, where f belongs to the functional space F. In the evaluation stage, we shall use the 0/1 loss as a target loss: ?(y, z) = 0 if y = z, and 1 otherwise.
In the training stage, we consider surrogate losses commonly used for structured prediction in different problem setups (see Section 4.1 for details on the surrogate losses used in this paper). More specifically, an input sequence X of length L is composed of word tokens: X = {x 1 , ? ? ? , x L }. Each token x l is a one hot vector in the space ? D , where Dis the dictionary size. Performing learning in ? Dis computationally expensive and difficult. An elegant framework in NLP, initially proposed in, allows to concisely perform learning by mapping the words into an embedding space. The framework relies on so called word embedding: ? D ? RP , where P is the dimensionality of the embedding space. Therefore, the text sequence X is represented via the respective word embedding for each token: V = {v 1 , ? ? ? , v L }, where v l ? RP . A typical text classification method proceeds in three steps, endto-end, by considering a function decomposition f = f 0 ? f 1 ? f 2 as shown in(a):
? f 0 : X ? V, the text sequence is represented as its word-embedding form V, which is a matrix of P ? L.
? f 1 : V ? z, a compositional function f 1 aggregates word embeddings into a fixed-length vector representation z.
? f 2 : z ? y, a classifier f 2 annotates the text representation z with a label.
A vast amount of work has been devoted to devising the proper functions f 0 and f 1 , i.e., how to represent a word or a word sequence, respectively. The success of NLP largely depends on the effectiveness of word embeddings inf 0. They are often pre-trained offline on large corpus, then refined jointly via f 1 and f 2 for task-specific representations. Furthermore, the design off 1 can be broadly cast into two categories. The popular deep learning models consider the mapping as a "black box," and have employed sophisticated CNN/RNN architectures to achieve state-of-theart performance. On the contrary, recent studies show that simple manipulation of the word embeddings, e.g., mean or max-pooling, can also provide surprisingly excellent performance. Nevertheless, these methods only leverage the information from the input text sequence.

Label-Embedding Attentive Model

Model
Yahoo DBPedia AGNews Yelp P. Yelp F. Bag-of-words 68.90 96.60 88.80 92.20 58.00 Small word CNN 69.98 98.15 89.13 94.46 58.59 Large word CNN 70.94 98.28 91.45 95.11 59.48 LSTM 70 73.53 98.42 92.24 93.76 61.11 fastText 72.30 98.60 92.50 95.70 63.90 HAN 75.80 ----Bi-BloSAN 76. The results are shown in.
Testing accuracy Simple compositional methods indeed achieve comparable performance as the sophisticated deep CNN/RNN models. On the other hand, deep hierarchical attention model can improve the pure CNN/RNN models. The recently proposed self-attention network generally yield higher accuracy than previous methods. All approaches are better than traditional bag-of-words method. Our proposed LEAM outperforms the state-of-the-art methods on two largest datasets, i.e., Yahoo and DBPedia. On other datasets, LEAM ranks the 2nd or 3rd best, which are similar to top 1 method in term of the accuracy. This is probably due to two reasons: (i) the number of classes on these datasets is smaller, and (ii) there is no explicit corresponding word embedding available for the label embedding initialization during learning. The potential of label embedding may not be fully exploited. As the ablation study, we replace the nonlinear compatibility to the linear one in (2) . The degraded performance demonstrates the necessity of spatial dependency and nonlinearity in constructing the attentions. Nevertheless, we argue LEAM is favorable for text classification, by comparing the model size and time cost, as well as convergence speed in. The time cost is reported as the wall-clock time for 1000 iterations. LEAM maintains the simplicity and low cost of SWEM, compared with other models. LEAM uses much less model parameters, and converges significantly  faster than Bi-BloSAN. We also compare the performance when only a partial dataset is labeled, the results are shown in(b). LEAM consistently outperforms other methods with different proportion of labeled data.
Hyper-parameter Our method has an additional hyperparameter, the window sizer to define the length of "phase" to construct the attention. Larger r captures long term dependency, while smaller r enforces the local dependency. We study its impact in(c). The topic classification tasks generally requires a larger r, while sentiment classification tasks allows relatively smaller r. One may safely chooser around 50 if not finetuning. We report the optimal results in.

Joint Embeddings of Words and Labels
We propose to embed both the words and the labels into a joint space i.e., ? D ? RP and Y ? RP . The label embeddings are C = [c 1 , ? ? ? , c K ], where K is the number of classes.
A simple way to measure the compatibility of label-word pairs is via the cosine similarity
where? is the normalization matrix of size K?L, with each element obtained as the multiplication of 2 norms of the c-th label embedding and l-th word embedding:? kl = ck v l .
To further capture the relative spatial information among consecutive words (i.e., phrases 1 ) and introduce non-linearity in the compatibility measure, we consider a generalization of (2). Specifically, for a text phase of length 2r + 1 centered at l, the local matrix block G l?r:l+r in G measures the label-to-token compatibility for the "label-phrase" pairs. To learn a higher-level compatibility stigmatization u l between the l-th phrase and all labels, we have:
where W 1 ? R 2r+1 and b 1 ? R K are parameters to be learned, and u l ? R K . The largest compatibility value of the l-th phrase wrt the labels is collected:
Together, m is a vector of length L. The compatibility/attention score for the entire text sequence is:
where the l-th element of SoftMax is
.
The text sequence representation can be simply obtained via averaging the word embeddings, weighted by label-based attention score:
Relation to Predictive Text Embeddings Predictive Text Embeddings (PTE) is the first method to leverage label embeddings to improve the learned word embeddings. We discuss three major differences between PTE and our LEAM: (i) The general settings are different. PTE casts the text representation through heterogeneous networks, while we consider text representation through an attention model. (ii) In PTE, the text representation z is the averaging of word embeddings. In LEAM, z is weighted averaging of word embeddings through the proposed labelattentive score in (6). (iii) PTE only considers the linear interaction between individual words and labels. LEAM greatly improves the performance by considering nonlinear interaction between phrase and labels. Specifically, we note that the text embedding in PTE is similar with a very special case of LEAM, when our window sizer = 1 and attention score ? is uniform. As shown later in of the experimental results, LEAM can be significantly better than the PTE variant.
Training Objective The proposed joint embedding framework is applicable to various text classification tasks. We consider two setups in this paper. For a learned text sequence representation z = f 1 ?f 0 (X), we jointly optimize f = f 0 ?f 1 ?f 2 over F, where f 2 is defined according to the specific tasks:
? Single-label problem: categorizes each text instance to precisely one of K classes, y ?
where CE(?, ?) is the cross entropy between two probability vectors, and f 2 (z n ) = SoftMax (z n ), with z n = W 2 z n + b 2 and W 2 ? R K?P , b 2 ? R K are trainable parameters.
? Multi-label problem: categorizes each text instance to a set of K target labels {y k ? ? 2 |k = 1, ? ? ? , K}; there is no constraint on how many of the classes the instance can be assigned to, and
where f 2 (z nk ) = 1 1+exp(z nk ) , and z nk is the kth column of z n .
To summarize, the model parameters ? = {V, C, W 1 , b 1 , W 2 , b 2 }. They are trained endto-end during learning. {W 1 , b 1 } and {W 2 , b 2 } are weights inf 1 and f 2 , respectively, which are treated as standard neural networks. For the joint embeddings {V, C} inf 0 , the pre-trained word embeddings are used as initialization if available.

Learning & Testing with LEAM
Learning and Regularization The quality of the jointly learned embeddings are key to the model performance and interpretability. Ideally, we hope that each label embedding acts as the "anchor" points for each classes: closer to the word/sequence representations thatare in the same classes, while farther from those in different classes. To best achieve this property, we consider to regularize the learned label embeddings ck to be on its corresponding manifold. This is imposed by the fact ck should be easily classified as the correct label y k :
where f 2 is specficied according to the problem in either or. This regularization is used as a penalty in the main training objective in or, and the default weighting hyperparameter is set as 1. It will lead to meaningful interpretability of learned label embeddings as shown in the experiments.
Interestingly in text classification, the class itself is often described as a set of E words {e i , i = 1, ? ? ? , E}. These words are considered as the most representative description of each class, and highly distinguishing between different classes. For example, the Yahoo! Answers Topic dataset contains ten classes, most of which have two words to precisely describe its class-specific features, such as "Computers & Internet", "Business & Finance" as well as "Politics & Government" etc. We consider to use each label's corresponding pre-trained word embeddings as the initialization of the label embeddings. For the datasets without representative class descriptions, one may initialize the label embeddings as random samples drawn from a standard Gaussian distribution.
Testing Both the learned word and label embeddings are available in the testing stage. We clarify that the label embeddings C of all class candidates Y are considered as the input in the testing stage; one should distinguish this from the use of groundtruth label yin prediction. For a text sequence X, one may feed it through the proposed pipeline for prediction: (i) f 1 : harvesting the word embeddings V, (ii) f 2 : V interacts with C to obtain G, pooled as ?, which further attends V to derive z, and (iii) f 3 : assigning labels based on the tasks. To speedup testing, one may store G offline, and avoid its online computational cost.

Parameters
Complexity We compare CNN, LSTM, Simple Word Embeddings-based Models (SWEM) and our LEAM wrt the parameters and computational speed. For the CNN, we assume the same size m for all filters. Specifically, h represents the dimension of the hidden units in the LSTM or the number of filters in the CNN; R denotes the number of blocks in the Bi-BloSAN; P denotes the final sequence representation dimension. Similar to, we examine the number of compositional parameters, computational complexity and sequential steps of the four methods. As shown in, both the CNN and LSTM have a large number of compositional parameters. Since K m, h, the number of parameters in our models is much smaller than for the CNN and LSTM models. For the computational complexity, our model is almost same order as the most simple SWEM model, and is smaller than the CNN or LSTM by a factor of mh/K or h/K.

Experimental Results
Setup We use 300-dimensional GloVe word embeddings as initialization for word embeddings and label embeddings in our model. Out-Of-Vocabulary (OOV) words are initialized from a uniform distribution with range [?0.01, 0.01]. The final classifier is implemented as an MLP layer followed by a sigmoid or softmax function depending on specific task. We train our model's parameters with the Adam Optimizer (Kingma and Ba, 2014), with an initial learning rate of 0.001, and a minibatch size of 100. Dropout regularization is employed on the final MLP layer, with dropout rate 0.5. The model is implemented using Tensorflow and is trained on GPU Titan X.
The code to reproduce the experimental results is at https://github.com/guoyinwang/LEAM: Summary statistics of five datasets, including the number of classes, number of training samples and number of testing samples.

Classification on Benchmark Datasets
We test our model on the same five standard benchmark datasets as in. The summary statistics of the data are shown in, with content specified below:
? AGNews: Topic classification over four categories of Internet news articles composed of titles plus description classified into: World, Entertainment, Sports and Business.
? Yelp Review Full: The dataset is obtained from the Yelp Dataset Challenge in 2015, the task is sentiment classification of polarity star labels ranging from 1 to 5.
? Yelp Review Polarity: The same set of text reviews from Yelp Dataset Challenge in 2015, except that a coarser sentiment definition is considered: 1 and 2 are negative, and 4 and 5 as positive.
? DBPedia: Ontology classification over fourteen non-overlapping classes picked from DBpedia 2014 (Wikipedia).
? Yahoo! Answers Topic: Topic classification over ten largest main categories from Yahoo! Answers Comprehensive Questions and Answers version 1.0, including question title, question content and best answer.
We compare with a variety of methods, including (i) the bag-of-words in; (ii) sophisticated deep CNN/RNN models: large/small word CNN, LSTM reported in and deep CNN (29 layer); (iii) simple compositional methods: fastText and simple word embedding models (SWEM); (iv) deep attention models: hierarchical attention network (HAN) (Yang et al.,

Representational Ability
Label embeddings are highly meaningful To provide insight into the meaningfulness of the learned representations, in we visualize the correlation between label embeddings and document embeddings based on the Yahoo dateset. First, we compute the averaged document embeddings per class:z k = 1 text manifold for class k. Ideally, the perfect label embedding ck should be the representative anchor point for class k. We compute the cosine similarity betweenz k and ck across all the classes, shown in(a). The rows are averaged per-class document embeddings, while columns are label embeddings. Therefore, the on-diagonal elements measure how representative the learned label embeddings are to describe its own classes, while off-diagonal elements reflect how distinctive the label embeddings are to be separated from other classes. The high on-diagonal elements and low off-diagonal elements in indicate the superb ability of the label representations learned from LEAM. Further, since both the document and label embeddings live in the same high-dimensional space, we use t-SNE (Maaten and Hinton, 2008) to visualize them on a 2D map in. Each color represents a different class, the point clouds are document embeddings, and the label embeddings are the large dots with black circles. As can be seen, each label embedding falls into the inter-nal region of the respective manifold, which again demonstrate the strong representative power of label embeddings.
Interpretability of attention Our attention score ? can be used to highlight the most informative words wrt the downstream prediction task. We visualize two examples in for the Yahoo dataset. The darker yellow means more important words. The 1st text sequence is on the topic of "Sports", and the 2nd text sequence is "Entertainment". The attention score can correctly detect the key words with proper scores.

Applications to Clinical Text
To demonstrate the practical value of label embeddings, we apply LEAM for a real healthcare scenario: medical code prediction on the Electronic Health Records dataset. A given patient may have multiple diagnoses, and thus multi-label learning is required.
Specifically 0.876 0.907 0.576 0.625 0.620 C-MemNN 0.833 ---0.42 Attentive LSTM -0.900 -0.532 -CAML 0.875 0.909 0.532 0.614 0.609 LEAM 0.881 0.912 0.540 0.619 0.612: Quantitative results for doctor-notes multi-label classification task.
contains text and structured records from a hospital intensive care unit. Each record includes a variety of narrative notes describing a patients stay, including diagnoses and procedures. They are accompanied by a set of metadata codes from the International Classification of Diseases (ICD), which present a standardized way of indicating diagnoses/procedures. To compare with previous work, we follow, and preprocess a dataset consisting of the most common 50 labels. It results in 8,067 documents for training, 1,574 for validation, and 1,730 for testing.

Results
We compare against the three baselines: a logistic regression model with bag-ofwords, a bidirectional gated recurrent unit (Bi-GRU) and a single-layer 1D convolutional network. We also compare with three recent methods for multi-label classification of clinical text, including Condensed Memory Networks (C-MemNN), Attentive LSTM and Convolutional Attention (CAML). To quantify the prediction performance, we follow to consider the micro-averaged and macro-averaged F1 and area under the ROC curve (AUC), as well as the precision at n (P@n). Micro-averaged values are calculated by treating each (text, code) pair as a separate prediction. Macro-averaged values are calculated by averaging metrics computed per-label. P@n is the fraction of then highestscored labels thatare present in the ground truth.
The results are shown in. LEAM provides the best AUC score, and better F1 and P@5 values than all methods except CNN. CNN consistently outperforms the basic Bi-GRU architecture, and the logistic regression baseline performs worse than all deep learning architectures. We emphasize that the learned attention can be very useful to reduce a doctor's reading burden. As shown in, the health related words are highlighted.

Conclusions
In this work, we first investigate label embeddings for text representations, and propose the label-embedding attentive models. It embeds the words and labels in the same joint space, and measures the compatibility of word-label pairs to attend the document representations. The learning framework is tested on several large standard datasets and a real clinical text application. Compared with the previous methods, our LEAM algorithm requires much lower computational cost, and achieves better if not comparable performance relative to the state-of-the-art. The learned attention is highly interpretable: highlighting the most informative words in the text sequence for the downstream classification task.