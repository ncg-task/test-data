206	13	18	found
206	43	69	single convolutional layer
206	70	74	with
206	75	90	filter length 3
206	91	109	always outperforms
206	114	125	other cases
210	6	11	shown
210	17	43	single convolutional layer
210	44	48	with
210	49	64	filter length 3
210	65	73	performs
210	74	78	best
210	79	84	among
210	85	110	all filter configurations
211	0	3	For
211	16	45	multiple convolutional layers
211	46	48	in
211	49	57	parallel
211	66	76	shown that
211	77	98	filter configurations
211	99	103	with
211	104	119	filter length 3
211	120	135	performs better
211	136	140	that
211	147	171	without tri-gram filters
142	3	12	implement
142	13	22	our model
142	23	31	based on
142	32	38	Theano
143	75	80	train
143	85	90	model
143	91	93	on
143	96	99	GPU
147	32	35	use
147	36	59	one convolutional layer
147	64	78	one LSTM layer
167	0	3	For
167	4	8	TREC
167	15	32	number of filters
167	36	45	set to be
167	46	49	300
167	58	74	memory dimension
167	78	87	set to be
167	88	91	300
168	4	40	word vector layer and the LSTM layer
168	45	52	dropped
168	53	60	outwith
168	63	74	probability
168	75	77	of
168	78	81	0.5
169	8	11	add
169	12	29	L2 regularization
169	30	34	with
169	37	43	factor
169	44	46	of
169	47	52	0.001
169	53	55	to
169	60	67	weights
169	68	70	in
169	75	88	softmax layer
169	89	92	for
169	93	103	both tasks
29	19	28	introduce
29	31	47	new architecture
29	48	57	short for
29	58	66	C - LSTM
29	67	79	by combining
29	80	92	CNN and LSTM
29	93	101	to model
29	102	111	sentences
30	56	62	design
30	65	109	simple end - to - end , unified architecture
30	110	120	by feeding
30	125	131	output
30	31	33	of
30	137	152	one - layer CNN
30	153	157	into
30	158	162	LSTM
31	4	7	CNN
31	11	32	constructed on top of
31	37	61	pre-trained word vectors
31	62	66	from
31	67	94	massive unlabeled text data
31	95	103	to learn
31	104	132	higher - level representions
31	133	135	of
31	136	143	n-grams
32	5	13	to learn
32	14	37	sequential correlations
32	38	42	from
32	43	81	higher - level suqence representations
32	88	100	feature maps
32	101	103	of
32	104	107	CNN
32	112	124	organized as
32	125	151	sequential window features
32	152	160	to serve
32	168	173	input
32	174	176	of
32	177	181	LSTM
34	3	9	choose
34	10	32	sequence - based input
34	81	87	before
34	88	95	feeding
34	96	98	in
34	103	117	neural network
173	0	24	Sentiment Classification
184	34	41	achieve
184	46	74	fourth best published result
184	75	78	for
184	83	112	5 - class classification task
185	0	3	For
185	8	34	binary classification task
185	40	47	achieve
185	48	66	comparable results
185	67	82	with respect to
185	87	114	state - of - the - art ones
191	0	28	Question Type Classification
198	6	16	Our result
198	17	41	consistently outperforms
198	42	78	all published neural baseline models
199	20	25	close
199	26	28	to
199	41	67	state - of - the - art SVM
2	30	49	Text Classification
170	0	7	Results
