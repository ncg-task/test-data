14	34	39	scale
14	46	55	baselines
14	31	33	to
14	59	76	very large corpus
14	77	81	with
14	84	102	large output space
15	75	84	show that
15	85	98	linear models
15	99	103	with
15	106	121	rank constraint
15	128	151	fast loss approximation
15	156	164	train on
15	167	180	billion words
15	181	187	within
15	188	199	ten minutes
53	0	18	Sentiment analysis
60	3	6	use
60	7	22	10 hidden units
60	27	30	run
60	31	39	fastText
60	40	43	for
60	44	52	5 epochs
60	53	57	with
60	60	73	learning rate
60	74	85	selected on
60	88	102	validation set
60	103	107	from
60	110	133	0.05 , 0.1 , 0.25 , 0.5
61	15	21	adding
61	22	40	bigram information
61	41	49	improves
61	54	65	performance
61	66	68	by
61	69	76	1 - 4 %
62	8	20	our accuracy
62	24	39	slightly better
62	40	44	than
62	45	55	char - CNN
62	60	71	char - CRNN
62	80	89	bit worse
62	90	94	than
62	95	100	VDCNN
63	84	88	with
63	89	97	trigrams
63	104	115	performance
63	116	118	on
63	119	124	Sogou
63	125	129	goes
63	130	142	up to 97.1 %
65	67	72	using
65	73	80	n-grams
65	81	86	up to
65	87	88	5
65	89	97	leads to
65	102	118	best performance
69	0	14	Tag prediction
79	3	11	consider
79	14	40	frequency - based baseline
79	47	55	predicts
79	60	77	most frequent tag
80	8	20	compare with
80	21	54	Tagspace ( Weston et al. , 2014 )
80	68	88	tag prediction model
83	74	80	adding
83	81	88	bigrams
83	89	94	gives
83	100	117	significant boost
83	118	120	in
83	121	129	accuracy
2	28	47	Text Classification
82	0	7	Results
