title
Explicit Interaction Model towards Text Classification
abstract
Text classification is one of the fundamental tasks in natural language processing .
Recently , deep neural networks have achieved promising performance in the text classification task compared to shallow models .
Despite of the significance of deep models , they ignore the fine - grained ( matching signals between words and classes ) classification clues since their classifications mainly rely on the text - level representations .
To address this problem , we introduce the interaction mechanism to incorporate word - level matching signals into the text classification task .
In particular , we design a novel framework , EXplicit interAction Model ( dubbed as EXAM ) , equipped with the interaction mechanism .
We justified the proposed approach on several benchmark datasets including both multilabel and multi-class text classification tasks .
Extensive experimental results demonstrate the superiority of the proposed method .
As a byproduct , we have released the codes and parameter settings to facilitate other researches .
Introduction
Text classification is one of the fundamental tasks in natural language processing , targeting at classifying apiece of text content into one or multiple categories .
According to the number of desired categories , text classification can be divided into two groups , namely , multi-label ( multiple categories ) and multi-class ( unique category ) .
For instance , classifying an article into different topics ( e.g. , machine learning or data mining ) falls into the former one since an article could be under several topics simultaneously .
By contrast , classifying a comment of a movie into its corresponding rating level lies into the multi -class group .
Both multi-label and multi-class text classifications have been widely applied in many fields like sentimental analysis , topic tagging , and document classification .
Feature engineering dominates the performance of traditional shallow text classification methods for a very longtime .
Various rule - based and statistical features like bag - of - words and N - grams ) are designed to describe the text , and fed into the shallow machine learning models such as Linear Regression and Support Vector Machine to make the judgment .
Traditional solutions suffer from two defects :
1 ) High labor intensity for the manually crafted features , and 2 ) data sparsity ( a N -grams could occur only several times in a given dataset ) .
Recently , owing to the ability of tackling the aforementioned problems , deep neural networks ; Liu , Qiu , and Huang 2016 ; Grave et al. ) have become the promising solutions for the text classification .
Deep neural networks typically learn a word - level representation for the input text , which is usually a matrix with each row / column as an embedding of a word in the text .
They then compress the word - level representation into a text - level representation ( vector ) with aggregation operations ( e.g. , pooling ) .
Thereafter , a fullyconnected ( FC ) layer at the topmost of the network is appended to make the final decision .
Note that these solutions are also called encoding - based methods , since they encode the textual content into a latent vector representation .
Although great success has been achieved , these deep neural network based solutions naturally ignore the finegrained classification clues ( i.e. , matching signals between words and classes ) , since their classifications are based on text - level representations .
As shown in , the classification ( i.e. , FC ) layer of these solutions matches the text - level representation with class representations via a dotproduct operation .
Mathematically , it interprets the parameter matrix of the FC layer as a set of class representations ( each column is associated with a class ) .
As such , the probability of the text belonging to a class is largely determined by their over all matching score regardless of word - level matching signals , which would provide explicit signals for classification ( e.g. , missile strongly indicates the topic of military ) .
To address the aforementioned problems , we introduce the interaction mechanism ( Wang and Jiang 2016 b ) , which is capable of incorporating the word - level matching signals for text classification .
The key idea behind the interaction mechanism is to explicitly calculate the matching scores between the words and classes .
From the word - level representation , it computes an interaction matrix , in which each entry is the matching score between a word and a class ( dot -product between their representations ) , illustrating the word - level matching signals .
By taking the interaction matrix as a text representation , the later classification layer could incorporate fine - grained word level signals for the finer classification rather than simply making the text - level matching .
Based upon the interaction mechanism , we devise an EXplicit interAction Model ( dubbed as EXAM ) .
Specifically , the proposed framework consists of three main components : word - level encoder , interaction layer , and aggregation layer .
The word - level encoder projects the textual contents into the word - level representations .
Hereafter , the interaction layer calculates the matching scores between the words and classes ( i.e. , constructs the interaction matrix ) .
Then , the last layer aggregates those matching scores into predictions over each class , respectively .
We justify our proposed EXAM model over both the multi-label and multi-class text classifications .
Extensive experiments on several benchmarks demonstrate the effectiveness of the proposed method , surpassing the corresponding state - of - the - art methods remarkably .
In summary , the contributions of this work are threefold : We present a novel framework , EXAM , which leverages the interaction mechanism to explicitly compute the wordlevel interaction signals for the text classification .
We justify the proposed EXAM model over both multilabel and multi-class text classifications .
Extensive experimental results demonstrate the effectiveness of the proposed method .
We release the implementation of our method ( including some baselines ) and the involved parameter settings to facilitate later researchers 1 .
Preliminaries
In this section , we introduce two widely - used word - level encoders :
Gated Recurrent Units notations in this paper , we use bold capital letters ( e.g. , X ) and bold lowercase letters ( e.g. , x ) to denote matrices and vectors , respectively .
We employ non-bold letters ( e.g. , x ) to represent scalars , and Greek letters ( e.g. , ? ) as parameters .
X i, : is used to refer the i - th row of the matrix X , X :, j to represent the j - th column vector and X i , j to denote the element in the i - th row and j- th column .
Gated Recurrent Units
Owing to the ability of capturing the sequential dependencies and being easily optimized ( i.e. , avoid the gradient vanishing and explosion problems ) , Gated Recurrent Units ( GRU ) becomes a widely used word - level encoder .
Typically , a GRU generates word - level representations in two phases :
1 ) mapping each word in the text into an embedding ( a real - valued vector ) , and 2 ) projecting the sequence of word embeddings into a sequence of hidden representations , which encodes the sequential dependencies .
Word embedding .
Word embedding is a general method to map a word from one hot vector to a low dimensional and real - valued vector .
With enough data , word embedding can capture high - level representations of words .
Hidden representation .
Given an embedding feature sequence E = [ E 1 , : , E 2 , : , , E n, : ] , GRU will compute a vector H i , : at the i - th time - step for each E i , : , and H i , : is defined as :
where Mr and M z are trainable parameters in the GRU , and ?
and tanh are sigmoid and tanh activation functions , respectively .
The sequence of hidden representations H = [ H 1 , : , , H n, : ] is denoted as the word - level representation of the input text .
Region Embedding
Although word embedding is a good representation for the word , it can only compute the feature vector for the single word .
Qiao et al. proposed region embedding to learn and utilize task - specific distributed representations of Ngrams .
In the region embedding layer , the representation of a word has two parts , the embedding of the word itself and a weighting matrix to interact with the local context .
For the word w i , the first part e w i is learned by an embedding matrix E ?
R kv and the second part
where v is the size of the vocabulary , 2 s + 1 the region size and k the embedding size .
And then , each column in K w i is used to interact with the context word in the corresponding relative position of w i to get the contextaware pt w i +t for each word w i +t in the region .
Formally it is computed by the following function : where denotes element - wise multiply .
And the final representation r i , s of the middle word w i is computed as follows :
Model Problem Formulation
Multi - Class Classification .
In this task , we should categorize each text instance to precisely one of c classes .
Suppose that we have a data set D = {d i , l i } N , where d i denotes the text and the one - hot vector l i ?
R c represents the label ford i , our goal is to learn a neural network N to classify the text .
Multi- Label Classification .
In this task , each text instance belongs to a set of c target labels .
Formally , suppose that we have a dataset D = {d i , l i } N i = 1 , where d i denotes the text and the multi -hot vector l i represents the label for the text d i .
Our goal is to learn a neural network N to classify the text .
Model Overview
Motivated by the limitation of encoding - based models for text classification , which is lacking the fine - grained classification clue , we propose a novel framework , named EXplicit interAction Model ( EXAM ) , leveraging the interaction mechanism to incorporate word - level matching signals .
As can be seen from , EXAM mainly contains three components :
A word - level encoder to project the input text d i into a word - level representation H. An interaction layer to compute the interaction signals between the words and classes .
An aggregation layer to aggregate the interaction signals for each class and make the final predictions .
Considering that word - level encoders are well investigated in previous studies ( as mentioned in the Section 2 ) , and the target of this work is to learn the fine - grained classification signals , we only elaborate the interaction layer and aggregation layer in the following subsections .
Interaction Layer
Interaction mechanism is widely used in tasks of matching source and target textual contents , such as natural language inference ( Wang and Jiang 2016 b ) and retrieve - based chatbot .
The key idea of interaction mechanism is to use the interaction features between the small units ( e.g. , words in the textual contents ) to infer fine - grained clues whether two contents are matching .
Inspired by the success of methods equipped with interaction mechanism over encodebased methods in matching the textual contents , we introduce the interaction mechanism into the task of matching textual contents with their classes ( i.e. , text classification ) .
Specifically , we devise an interaction layer which aims to compute the matching score between the word and class .
Different from conventional interaction layer , where the wordlevel representations of both source and target are extracted with encoders like GRU , here we first project classes into real - valued latent representations .
In other words , we employ a trainable representation matrix T ?
R ck to encode classes ( each row represents a class ) , where c denotes the amount of classes and k is the embedding size equals to that of words .
We then adopt dot product as the interaction function to estimate the matching score between the target word t and class s , of which the formulation is ,
where H ?
R nk denotes word - level representation of the text , extracted by the encoder with n denoting the length of the text .
In this way , we can compute the interaction matrix I ?
R cn by following :
Note that we reject more complex interaction functions like element - wise multiply ( Gong , Luo , and and cosine similarity ( Wang , Hamza , and Florian 2017 ) for the consideration of efficiency .
Aggregation Layer
This layer is devised to aggregate the interaction features for each class s into a logits o s i , which denotes the matching score between class sand the input text d i .
The aggregation layer can be implemented in different ways such as CNN ( Gong , Luo , and and LSTM ( Wang , Hamza , and Florian 2017 ) .
However , to keep the simplicity and efficiency of EXAM , here we only use a MLP with two FC layers , where ReLU is employed as the activation function of the first layer .
Formally , the MLP aggregates the interaction features I s , : for class s , and compute it s associated logits as following :
where W 1 and W 2 are trainable parameters and b is the bias in the first layer .
We then normalize the logits oi = [ o 1 i , , o c i ] into probabilities pi .
Note that we follow previous work and employ softmax and sigmoid for multi-class and multi-label classifications , respectively .
Loss Function
Similar to previous studies , in the multi-class text classification , we use cross entorpy loss as our loss function :
Following previous researchers ( Grave et al. ) , we choose binary classification loss as our loss function for the multi-label one :
Generalized Encoding - Based Model
In this section , we elaborate how the encoding - based model can be interpreted as a special case of our EXAM framework .
As FastText is the most popular model for text classification and has been investigated extensively in the literature , being able to recover it allows EXAM to mimic a large family of text classification models .
FastText contains three layers :
1 ) an embedding layer to get the word - level representation H t , : for the word t , 2 ) an average pooling layer to get the text - level representation f ?
R 1 k , and 3 ) a FC layer to get the final logits p ?
R 1 c , where k denotes the embedding size and c means the number of classes .
Note that we omit the subscript of the document ID for conciseness .
Formally , it computes the logits p s of s-th class as follows :
where W ?
R kc and b ?
R 1c are the trainable parameters in the last FC layer , and n denotes the length of the text .
The Eqn. ( 9 ) has an equivalent form as following :
It is worth noting that H t , : W :,s is exactly the interaction feature between word t and class s .
Therefore , the FastText is a special case of EXAM with an average pooling as the aggregation layer .
In EXAM , we use a non-linear MLP to be the aggregation layer , and it will generalize FastText to a non-linear setting which might be more expressive than the original one .
Experiments
Multi - Class Classification
Datasets
We used publicly available benchmark datasets from ( Zhang , Zhao , and LeCun 2015 ) to evaluate EXAM .
There are in total 6 text classification datasets , corresponding to sentiment analysis , news classification , question - answer and ontology extraction tasks , respectively .
shows the descriptive statistics of datasets used in our experiments .
Stanford tokenizer is used to tokenize the text and all words are converted to lowercase .
We used padding to handle the various lengths of the text , and different maximum lengths are set for each dataset , respectively .
If the length of the text is less than the corresponding predefined value , we padded it with zero ; otherwise we truncated the original text .
To guarantee a fair comparison , the same evaluation protocol of ( Zhang , Zhao , and LeCun 2015 ) is employed .
We split 10 % samples from the training set as the validation set to perform early stop for our models .
Hyperparameters
For the multi -class task , we chose region embedding as the Encoder in EXAM .
The region size is 7 and embedding size is 128 .
We used adam ( Kingma and Ba 2014 ) as the optimizer with the initial learning rate 0.0001 and the batch size is set to 16 .
As for the aggregation MLP , we set the size of the hidden layer as 2 times interaction feature length .
Our models are implemented and trained by MXNet ( Chen et al. ) with a single NVIDIA TITAN Xp .
Baselines
To demonstrate the effectiveness of our proposed EXAM , we compared it with several state - of - the - art baselines .
The baselines are mainly in three variants :
1 ) models based on feature engineering ;
2 ) Char - based deep models , and 3 ) Word - based deep models .
The first category uses the feature from the text to conduct the classification , and we reported the results from BoW Overall Performance We compared our EXAM to several state - of - the - art baselines with respect to accuracy .
All results are summarized in .
Four points are observed as following :
Models based on feature engineering get the worst results on all the five datasets compared to the other methods .
The main reason is that the feature engineering can not take full advantage of the supervision from the training set and it also suffers from the data sparsity .
Char - based models get the highest over all scores on the two Amazon datasets .
There are possibly two reasons , 1 ) compared to the word - based models , char - based models enrich the supervision from characters and the characters are combined to form N-grams , stems , words and phrase which are helpful in the sentimental classification .
2 ) The two Amazon datasets contain millions of training samples , perfectly fitting the deep residual architecture for the VDCNN .
For the three char - based baselines , VDCNN gets the best performance on almost all the datasets because it has 29 convolutional layers allowing the model to learn more combinations of characters .
Word - based baselines exceed the other variants on three datasets and lose on the two Amazon datasets .
The main reason is that the three tasks like news classification conduct categorization mainly via key words , and the wordbased models are able to directly use the word embedding without combining the characters .
For the five baselines , W.C Region Emb performs the best , because it learns the region embedding to utilize the N- grams feature from the text .
It is clear to see that EXAM achieves the best performance over the three datasets : AG , Yah. A. and DBP .
For the Yah.A. , EXAM improves the best performance by 1.1 % .
Additionally , as a word - based model , EXAM beats all the word - based baselines on the other two Amazon datasets with a performance gain of 1.0 % on the Amazon Full , because our EXAM considers more fine - grained interaction features between classes and words , which is quite helpful in this task .
Component - wise Evaluation
We studied the variant of our model to further investigate the effectiveness of the interaction layer and aggregation layer .
We built a model called EXAM Encoder to preserve only the Encoder component with a max pooling layer and FC layer to derive the final probabilities .
EXAM
Encoder does not consider the interaction features between the classes and words , so it will automatically be degenerated into the Encoding - Based model .
We reported the results of the two models on all the datasets at , and it is clear to see that EXAM Encoder is not a patch on the original EXAM , verifying the effectiveness of interaction mechanism .
We also drew the convergence lines for EXAM and the EXAM Encoder for the datasets .
From the , where the red lines represent EXAM and the blue is EXAM Encoder , we observed that EXAM converges faster than EXAM Encoder with respect to all the datasets .
Therefore , the interaction brings not only performance improvement but also faster convergence .
The possible reason is that a non-linear aggregation layer introduces more parameters to fit the interaction features compared to the average pooling layer as mentioned in Section 4 .
Multi - Label Classification
Datasets
We conducted experiments on two different multi-label text classification datasets , named KanShan - Cup dataset 2 ( a benchmark ) and Zhihu dataset 3 , respectively .
KanShan - Cup dataset .
This dataset is released by a competition of tagging topics for questions ( multi- label classification ) posted in the largest Chinese community question answering platform , Zhihu .
The dataset contains 3,000,000 questions and 1,999 topics ( classes ) , where one question may belong to one to five topics .
For questions with more than 30 words , we kept the last 30 words , otherwise , we padded zeros .
We separated the dataset into training , validation , and testing with 2,800,000 , 20,000 , and 180,000 questions , respectively .
Zhihu dataset .
Considering the user privacy and data security , KanShan - Cup does not provide the original texts of the questions and topics , but uses numbered codes and numbered segmented words to represent text messages .
Therefore , it is inconvenient for researchers to perform analyses like visualization and case study .
To solve this problem , we constructed a dataset named Zhihu dataset .
We chose the top 1,999 frequent topics from Zhihu and crawled all the questions relevant to these topics .
Finally , we acquired 3,300,000 questions , with less than 5 topics for each question .
We adopted 3,000,000 samples as the training set , 30,000 samples as validation and 300,000 samples as testing .
Baselines
We applied the following models as baselines to evaluate the effectiveness of EXAM .
Hyperparameters
We implemented the baseline models and EXAM by MXNet .
We used the matrix trained by word2vec to initialize the embedding layer , and the embedding size is 256 .
We adopted GRU as the Encoder , and each GRU Cell has 1,024 hidden states .
The accumulated MLP has 60 hidden units .
We applied Adam to optimize models on one NVIDIA TITAN Xp with the batch size of 1000 and the initial learning rate is 0.001 .
The validation set is applied for early - stopping to avoid overfitting .
All hyperparameters are chosen empirically .
Metrics
We used the following metrics to evaluate the performance of our model and baseline models .
Precision : Different from the traditional precision metric ( Precision@ 5 ) which is set as the fraction of the relevant topic tags among the five returned tags , we utilized weighted precision to encourage the relevant topic tags to be ranked higher in the returned list .
Formally , the Precision is computed as following , P recision = pos ? { 1 , 2 , 3 , 4 , 5 }
P recision@pos log ( pos + 1 ) .:
The visualization of interaction features of EXAM .
Recall@5 : Recall is the fraction of relevant topic tags that have been retrieved over the total amount of five relevant topic tags , high recall means that the model returns most of the relevant topic tags . F 1 : F 1 is the harmonic average of the precision and recall , we computed it as following ,
Performance Comparison gives the performance of our model and baselines over two different datasets with respect to Precision , Recall@5 and F 1 .
We observed the following from the :
Word - based models are better than char - based models in Kanshan - Cup dataset .
That maybe because in Chinese the words can offer more supervisions than characters and the question tagging task needs more word supervision .
For word - based baseline models , all the baselines have similar performance which corroborates the conclusion in FastText ) that simple network is on par with deep learning classifiers in text classification .
Our models achieve the state - of - the - art performance over two different datasets though we only slightly modified Text RNN to build EXAM .
Different from the traditional models which encode the whole text into a vector , in EXAM , the representations of classes firstly interact with words to get more fine - grained features as shown in .
The results suggest that word - level interaction features are relatively more important than global text - level representations in this task .
Interaction Visualization
To illustrate the effectiveness of explicit interaction , we visualized an interaction feature I of the question " Second - hand TIDDA 1.6 T Mannual gear has gotten some problems , please everybody help me to solve it ? " .
This question has 5 topics :
Car ,
Second - hand Car , Motor Dom , Autocar Conversation and Autocar Service .
EXAM only misclassified the last topic .
In , we observed that when classifying different topics , the interaction features are different .
The topics " Car " and " Second - hand Car " pay much attention to the words like " Second - hand TIIDA " and the other topic like " Autocar Conversation " focuses more on " got some problems " .
The results clearly signify that the interaction feature between the word and class is well - learned and highly meaningful .
Related Work
Text Classification
Existing researches on text classification can be categorized into two groups : feature - based and deep neural models .
The former focuses on handcraft features and uses machine learning algorithms as the classifier .
Bag - of - words ( Wallach 2006 ) is a very efficient way to conduct the feature engineering .
SVM and Naive Bayes are constantly the classifier .
The latter , deep neural models , taking advantage of neural networks to accomplish the model learning from data , have become the promising solution for the text classification .
For instance , Iyyer et al.
proposed Deep Averaging Networks ( DAN ) and Grave et al.
proposed the FastText , and both are simple but efficient .
To get the temporal features between the words in the text , some models like TextCNN and Char - CNN ( Zhang , Zhao , and LeCun 2015 ) exploit the convolutional neural network , and there are also some models based on Recurrent Neural Network ( RNN ) .
Recently , Johnson et al. investigated the residual architecture and built a model called VD - CNN and Qiao et al. proposed a new method of region embedding for the text classification .
However , as mentioned in the Introduction , all these methods are text - level models while EXAM conducts the matching at the word level .
Interaction Mechanism Interaction Mechanism is widely used in Natural Language Sentence Matching ( NLSM ) .
The key idea of interaction mechanism is to use the interaction features between the small units ( like words in sentence ) to make the matching .
Wang et al. proposed a " matching - aggregation " framework to perform the interaction in Natural Language Inference .
Following this work , Parikh et al. integrated the attention mechanism into this framework , called Decomposable Attention Model .
Then Wang et al.
discussed different interaction functions in Text Matching .
Yu et al. adopted tree - LSTM to get different level units to perform the interaction .
Gong et al. proposed a densely interactive inference network to use DenseNet to aggregate dense interaction features .
Our work is different from them since they mainly apply this mechanism in text matching instead of the classification .
Conclusion
In this work , we present a novel framework named EXAM which employs the interaction mechanism to explicitly compute the word - level interaction signals for the text classification .
We apply the proposed EXAM on multi-class and multi-label text classifications .
Experiments over several benchmark datasets verify the effectiveness of our proposed mechanism .
In the future , we plan to investigate the effect of different interaction functions in the interaction mechanism .
Besides , we are interested in extend EXAM by introducing more complex aggregation layers like ResNet or DenseNet .
