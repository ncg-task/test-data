21	93	104	focusing on
21	120	143	document classification
21	33	41	question
21	55	90	overly complex neural architectures
22	0	13	Starting with
22	16	51	large - scale reproducibility study
22	52	54	of
22	55	83	several recent neural models
22	89	98	find that
22	101	151	simple bi-directional LSTM ( BiLSTM ) architecture
22	152	156	with
22	157	183	appropriate regularization
22	184	190	yields
22	191	207	accuracy and F 1
22	224	245	competitive or exceed
22	250	266	state of the art
22	267	269	on
22	270	302	four standard benchmark datasets
60	3	10	conduct
60	13	48	large - scale reproducibility study
60	49	58	involving
60	59	93	HAN , XML - CNN , KimCNN , and SGM
64	17	24	compare
64	29	46	neural approaches
64	47	49	to
64	50	76	logistic regression ( LR )
64	81	113	support vector machines ( SVMs )
65	4	12	LR model
65	16	29	trained using
65	32	69	one - vs - rest multi-label objective
65	82	85	SVM
65	89	101	trained with
65	104	117	linear kernel
67	27	39	performed on
67	40	76	Nvidia GTX 1080 and RTX 2080 Ti GPUs
67	79	83	with
67	84	97	PyTorch 0.4.1
67	98	100	as
67	105	122	backend framework
68	3	6	use
68	7	25	Scikitlearn 0.19.2
68	26	39	for computing
68	44	60	tf - idf vectors
68	65	77	implementing
68	78	89	LR and SVMs
2	52	75	Document Classification
111	3	11	see that
111	12	37	our simple LSTM reg model
111	38	46	achieves
111	47	63	state of the art
111	64	66	on
111	67	83	Reuters and IMDB
111	110	122	establishing
111	123	134	mean scores
111	135	137	of
111	138	151	87.0 and 52.8
111	152	155	for
111	156	178	F 1 score and accuracy
111	179	181	on
111	186	195	test sets
111	196	198	of
111	199	215	Reuters and IMDB
113	3	10	observe
113	16	24	LSTM reg
113	25	46	consistently improves
113	47	51	upon
113	56	67	performance
113	68	70	of
113	71	80	LSTM base
113	81	87	across
113	88	104	all of the tasks
114	2	26	few of our LSTM reg runs
114	27	33	attain
114	34	69	state - of - theart test F 1 scores
114	70	72	on
114	73	77	AAPD
119	20	51	non-neural LR and SVM baselines
119	52	59	perform
119	60	75	remarkably well
120	0	2	On
120	3	10	Reuters
120	31	34	SVM
120	35	40	beats
120	41	62	many neural baselines
120	65	74	including
120	75	104	our non-regularized LSTM base
121	3	7	AAPD
121	14	17	SVM
121	25	38	ties or beats
121	43	55	other models
121	58	72	losing only to
121	73	76	SGM
122	0	11	Compared to
122	16	19	SVM
122	26	37	LR baseline
122	38	45	appears
122	46	59	better suited
122	60	63	for
122	68	110	single - label datasets IMDB and Yelp 2014
122	122	130	achieves
122	131	146	better accuracy
122	147	151	than
122	156	159	SVM
