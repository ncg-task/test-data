title
Rethinking Complex Neural Network Architectures for Document Classification
abstract
Neural network models for many NLP tasks have grown increasingly complex in recent years , making training and deployment more difficult .
A number of recent papers have questioned the necessity of such architectures and found that well - executed , simpler models are quite effective .
We show that this is also the case for document classification : in a large - scale reproducibility study of several recent neural models , we find that a simple BiLSTM architecture with appropriate regularization yields accuracy and F 1 that are either competitive or exceed the state of the art on four standard benchmark datasets .
Surprisingly , our simple model is able to achieve these results without attention mechanisms .
While these regularization techniques , borrowed from language modeling , are not novel , to our knowledge we are the first to apply them in this context .
Our work provides an opensource platform and the foundation for future work in document classification .
Introduction
Recent developments in neural architectures for a wide range of NLP tasks can be characterized as a drive towards increasingly complex network components and modeling techniques .
Worryingly , these new models are accompanied by smaller and smaller improvements in effectiveness on standard benchmark datasets , which leads us to wonder if observed improvements are " real " .
There is , however , ample evidence to the contrary .
To provide a few examples : report that standard LSTM architectures outperform more recent models when properly tuned .
show that sequence transduction using encoder - decoder networks with attention mechanisms work just as well with the attention module only , making most of the complex * Equal contribution .
neural machinery unnecessary .
show that simple RNN - and CNN - based models yield accuracies rivaling far more complex architectures in simple question answering over knowledge graphs .
Perhaps most damning are the indictments of , who lament the lack of empirical rigor in our field and cite even more examples where improvements can be attributed to far more mundane reasons ( e.g. , hyperparameter tuning ) or are simply noise .
concur with these sentiments , adding that authors often use fancy mathematics to obfuscate or to impress ( reviewers ) rather than to clarify .
Complex architectures are more difficult to train , more sensitive to hyperparameters , and brittle with respect to domains with different data characteristics - thus both exacerbating the " crisis of reproducibility " and making it difficult for practitioners to deploy networks that tackle real - world problems in production environments .
Like the papers cited above , we question the need for overly complex neural architectures , focusing on the problem of document classification .
Starting with a large - scale reproducibility study of several recent neural models , we find that a simple bi-directional LSTM ( BiLSTM ) architecture with appropriate regularization yields accuracy and F 1 that are either competitive or exceed the state of the art on four standard benchmark datasets .
As the closest comparison point , we find no benefit to the hierarchical modeling proposed by and we are able to achieve good classification results without attention mechanisms .
While these regularization techniques , borrowed from language modeling , are not novel , we are to our knowledge the first to apply them in this context .
Our work provides an opensource platform and the foundation for future work in document classification .
Background and Related Work
Document Classification
Over the last few years , deep neural networks have achieved the state of the art in document classification .
One popular model , hierarchical attention network ( HAN ) , uses word - and sentence - level attention in classifying documents .
Although this model nicely captures the intuition that modeling word sequences in sentences should be handled separately from sentence - level discourse modeling , one wonders if such complex architectures are really necessary , especially given the size of training data available today .
An important variant of document classification is the multi-label , multi-class case .
develop XML - CNNs for multi-label text classification , basing the architecture on with increased filter sizes and an additional fully - connected layer .
They also incorporate dynamic adaptive max - pooling instead of the vanilla max - pooling overtime in KimCNN .
The paper compares with CNN - based approaches for the multi-label task , but only reports precision and disregards recall .
instead adopts encoder - decoder sequence generation models ( SGMs ) for generating multiple labels for each document .
Similar to our critique of HAN , we opine against the high complexity of these multi-label approaches .
Regularizing RNNs
There have been attempts to extend dropout from feedforward neural networks to recurrent ones .
Unfortunately , direct application of dropout on the hidden units of an RNN empirically harms its ability to retain longterm information .
Recently , however , successfully apply dropout - like techniques to regularize RNNs for language modeling , achieving competitive word - level perplexity on multiple datasets .
Inspired by this development , we adopt two of their regularization techniques , embedding dropout and weight - dropped LSTMs , to our task of document classification .
Weight - dropped LSTM .
LSTMs comprise eight total input-hidden and hidden - hidden weight matrices ; in weight dropping , regularize the four hidden - hidden matrices with DropConnect .
The operation is applied only once per sequence , using the same dropout mask across multiple timesteps .
Conveniently , this allows practitioners to use fast , out - of the - box LSTM implementations without affecting the RNN formulation or training performance .
Embedding Dropout .
Introduced in and successfully employed for neural language modeling , embedding dropout performs dropout on entire word embeddings , effectively removing some of the words at each training iteration .
As a result , the technique conditions the model to be robust against missing input ; for document classification , this discourages the model from relying on a small set of input words for prediction .
BiLSTM
Model
We design our model to be minimalistic :
First , we feed the word embeddings w 1:n of a document to a single - layer BiLSTM , extracting concatenated forward and backward word - level context vectors h 1:n = hf 1:n ? h b 1:n .
Subsequently , we max - pool h 1:n across time to yield document vector d-see , labels a-f .
Finally , we feed d to a sigmoid or a softmax layer over the labels , depending on if the task type is multi-label or single - label classification ( label g ) .
Contrary to prior art , our approach refrains from attention , hierarchical structure , and sequence generation , each of which increases model complexity .
For one , hierarchical structure requires sentence - level tokenization and multiple RNNs .
For another , sequence generation uses an encoderdecoder architecture , reducing computational parallelism .
All three methods add depth to the model ; our approach instead uses a single - layer BiLSTM with trivial max - pooling and concatena - tion operations , which makes for both simple implementation and resource - efficient inference .
Experimental Setup
We conduct a large - scale reproducibility study involving HAN , XML - CNN , KimCNN , and SGM .
These are compared to our proposed model , referred to as LSTM reg , as well as an ablated variant without regularization , denoted LSTM base .
The implementation of our model as well as fromscratch reimplementations of all the comparison models ( except for SGM ) are provided in our toolkit called Hedwig , which we make publicly available to serve as the foundation for future work .
1
In addition , we compare the neural approaches to logistic regression ( LR ) and support vector machines ( SVMs ) .
The LR model is trained using a one - vs - rest multi-label objective , while the SVM is trained with a linear kernel .
Both of these methods use word - level tf - idf vectors of the documents as features .
All of our experiments are performed on Nvidia GTX 1080 and RTX 2080 Ti GPUs , with PyTorch 0.4.1 as the backend framework .
We use Scikitlearn 0.19.2 for computing the tf - idf vectors and implementing LR and SVMs .
Datasets
We evaluate our models on the following four datasets : Reuters - 21578 , arXiv Abstract Paper dataset ( AAPD ) , IMDB , and Yelp 2014 .
Reuters and AAPD are multi-label datasets , whereas IMDB and Yelp are single - label ones .
For IMDB and Yelp , we use random sampling to split the dataset such that 80 % is used for training , 10 % for validation , and 10 % for test .
We use the standard ModApte splits for the Reuters dataset , and author - defined splits for AAPD .
We summarize the statistics of these datasets in .
Unfortunately , there is little consensus within the natural language processing community for choosing the splits of IMDB and Yelp 2014 .
Furthermore , they are often unreported in modeling papers , hence preventing direct comparison with past results .
We are notable to find the exact splits use ; for consistency , we use the same proportion the authors report , but of course this yields different samples in each split .
For the multi-label datasets , we report the wellknown micro-averaged F 1 score , which is the class - weighted harmonic mean between recall and precision .
For the single - label datasets , we compare the models using accuracy .
Training and Hyperparameters
To ensure a fair comparison , we tune the hyperparameters for all baseline models .
For HAN , we use a batch size of 32 across all the datasets , with a learning rate of 0.01 for Reuters and 0.001 for the rest .
To train XML - CNN , we select a dynamic pooling window length of eight , a learning rate of 0.001 , and 128 output channels , with batch sizes of 32 and 64 for single - label and multilabel datasets , respectively .
For KimCNN , we use a batch size of 64 with a learning rate of 0.01 .
For training SGM on Reuters , we use the source code provided by the authors 2 and follow the same hyperparameters in their paper .
For the LR and SVM models , we use the default set of hyperparameters in Scikit - learn .
For LSTM reg and LSTM base , we use the Adam optimizer with a learning rate of 0.01 on Reuters and 0.001 on the rest of the datasets , using batch sizes of 32 and 64 for multi-label and single - label tasks , respectively .
For LSTM reg , we also apply temporal averaging ( TA ) : as shown in , TA reduces both generalization error and stochastic noise in recent parameter estimates from stochastic approximation .
We set the default TA exponential smoothing coefficient of ? EMA to 0.99 .
We choose 512 hidden units for the Bi - LSTM models , whose max - pooled output is regularized using a dropout rate of 0.5 .
We also regularize the input-hidden and hidden - hidden Bi - LSTM connections using embedding dropout and weight dropping , respectively , with dropout rates of 0.1 and 0.2 .
For our optimization objective , we use crossentropy and binary cross - entropy loss for singlelabel and multi-label tasks , respectively .
On all datasets and models , we use 300 - dimensional word vectors pre-trained on Google News .
We train all neural models for 30 epochs with five random seeds , reporting the mean validation set scores and their corresponding test set results .
Toward Robust Baselines .
Recently , reproducibility is becoming a growing concern for the NLP community .
Indeed , very few of the papers that we consider in this study report validation set results , let alone run on multiple seeds .
In order to address these issues , we report scores on both validation and test sets for our reimplementations ; doing so is good practice , since it reinforces the validity of the experimental results and claims .
We also provide the standard deviation of the scores across different seeds to demonstrate the stability of our results .
This is inline with previous papers that emphasize reporting variance for robustness against potentially spurious conclusions .
Results and Discussion
We report the mean and standard deviation ( SD ) of the F 1 scores and accuracy for all five runs in Table
2 .
For HAN and KimCNN , we include results from the original papers to validate our reimplementation .
We fail to replicate the reported results of SGM on AAPD using the authors ' codebase and data splits .
3
As a result , we simply copy the value reported in in , row 8 , which represents their maximum F 1 score .
To verify the correctness of our HAN and KimCNN reimplementations , we compare the differences in F 1 and accuracy on the appropriate datasets .
We attribute the small differences to using different dataset splits ( see Section 4.1 ) and reporting mean values .
Baseline Comparison .
We see that our simple LSTM reg model achieves state of the art on Reuters and IMDB ( see , rows 9 and 10 ) , establishing mean scores of 87.0 and 52.8 for F 1 score and accuracy on the test sets of Reuters and IMDB , respectively .
This highlights the efficacy of proper regularization and optimization techniques for the task .
We observe that LSTM reg consistently improves upon the performance of LSTM base across all of the tasks - see rows 9 and 10 , where , on average , regularization yields increases of 1.5 and 0.5 points for F 1 score and accuracy , respectively .
A few of our LSTM reg runs attain state - of - theart test F 1 scores on AAPD .
However , in the interest of robustness , we report the mean value , as mentioned in Section 4.2 .
We also find the accuracy of LSTM reg and our reimplemented version of HAN on Yelp 2014 to be almost two points lower than the copied result of HAN ( rows 6 , 7 , and 10 ) from .
On the other hand , both of the models surpass the original result by nearly two points for the IMDB dataset .
We can not rule out that these disparities are caused by the absence of any widely - accepted splits for evaluation on Yelp 2014 and IMDB ( as opposed to model or implementation differences ) .
Interestingly , the non-neural LR and SVM baselines perform remarkably well .
On Reuters , for example , the SVM beats many neural baselines , including our non-regularized LSTM base ( rows 2 - 9 ) .
On AAPD , the SVM either ties or beats the other models , losing only to SGM ( rows 2 - 8 ) .
Compared to the SVM , the LR baseline appears better suited for the single - label datasets IMDB and Yelp 2014 , where it achieves better accuracy than the SVM does .
Conclusions and Future Work
In this paper , we question the complexity of existing neural network architectures for document classification .
To demonstrate the effectiveness of proper regularization and optimization , we apply embedding dropout , weight dropping , and temporal averaging when training a simple BiLSTM model , establishing either competitive or state - of the - art results on multiple datasets .
One potential extension of this work is to conduct a comprehensive ablation study to determine the relative contribution of each of the regularization and optimization techniques .
Furthermore , it would be interesting to compare these techniques to the recent line of research in deep language representation models , such as Embeddings from Language Models ( ELMo ; and pre-trained transformers .
Finally , the examined regularization and optimization methods deserve exploration in other NLP tasks as well .
