214	0	10	Leveraging
214	15	29	unlabeled data
214	30	34	from
214	35	48	other domains
214	54	86	not offer consistent improvement
152	0	16	Fine-tune ( Ft )
152	19	32	Fine - tuning
152	37	54	pre-trained model
152	55	59	with
152	64	92	source - domain training set
154	0	28	Fine - tune with UDA ( UDA )
155	12	20	utilizes
155	25	39	unlabeled data
155	40	44	from
155	49	62	target domain
155	63	76	by optimizing
155	81	98	UDA loss function
156	0	53	Self - training based on the UDA model ( UDA + Self )
157	3	14	first train
157	19	41	Ft model and UDA model
157	48	54	choose
157	77	90	teacher model
158	21	34	used to train
158	37	52	new XLM student
158	53	58	using
158	59	84	only unlabeled data U tgt
158	85	87	in
158	92	105	target domain
38	19	27	focus on
38	28	42	two approaches
38	43	46	for
38	47	64	domain adaptation
39	4	16	first method
39	20	28	based on
39	29	71	masked language model ( MLM ) pre-training
39	82	87	using
39	88	121	unlabeled target language corpora
41	4	17	second method
41	21	59	unsupervised data augmentation ( UDA )
41	64	69	where
41	70	91	synthetic paraphrases
41	96	110	generated from
41	115	131	unlabeled corpus
41	142	147	model
41	151	161	trained on
41	164	186	label consistency loss
2	27	66	CROSS - LINGUAL DOCUMENT CLASSIFICATION
5	23	60	cross - lingual understanding ( XLU )
8	257	260	XLU
171	0	10	Looking at
171	11	29	Ft ( XLM ) results
171	130	145	substantial gap
171	146	153	between
171	158	175	model performance
171	66	68	of
171	179	236	the cross -lingual settings and the monolingual baselines
172	9	43	UDA algorithm and MLM pre-training
172	48	53	offer
172	54	78	significant improvements
172	79	91	by utilizing
172	96	110	unlabeled data
184	14	27	our framework
184	28	35	reaches
184	36	70	new state - of - the - art results
184	73	87	improving over
184	88	109	vanilla XLM baselines
184	110	112	by
184	113	128	44 % on average
173	0	2	In
173	7	36	sentiment classification task
173	81	100	Ft ( XLM ft ) model
173	101	106	usnig
173	107	123	MLM pre-training
173	137	145	provides
173	146	165	larger improvements
173	166	179	compared with
173	184	194	UDA method
174	24	34	MLM method
174	35	37	is
174	38	72	relatively more resource intensive
174	77	82	takes
174	83	89	longer
174	90	92	to
174	93	101	converge
178	42	49	observe
178	54	79	self - training technique
178	80	101	consistently improves
178	102	106	over
178	111	124	teacher model
179	3	9	offers
179	10	22	best results
179	23	25	in
179	26	63	both XLM and XLM ft based classifiers
181	7	20	MLdoc dataset
181	23	38	self - training
181	44	52	achieves
181	57	78	best results over all
175	14	16	in
175	21	34	MLdoc dataset
175	46	50	size
175	51	53	of
175	58	75	unlabeled samples
175	76	78	is
175	79	86	limited
175	93	103	UDA method
175	107	119	more helpful
183	149	161	by utilizing
183	162	176	unlabeled data
183	177	179	in
183	184	199	target language
183	112	128	completely close
183	133	148	performance gap
183	10	24	comparing with
183	29	94	best cross - lingual results and monolingual fine - tune baseline
