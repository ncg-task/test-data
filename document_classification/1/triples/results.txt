(Contribution||has||Results)
(Results||Looking at||Ft ( XLM ) results)
(Ft ( XLM ) results||has||substantial gap)
(substantial gap||between||model performance)
(model performance||of||the cross -lingual settings and the monolingual baselines)
(Results||In||MLdoc dataset)
(MLdoc dataset||has||self - training)
(self - training||achieves||best results over all)
(Results||In||sentiment classification task)
(sentiment classification task||has||Ft ( XLM ft ) model)
(Ft ( XLM ft ) model||provides||larger improvements)
(larger improvements||compared with||UDA method)
(Ft ( XLM ft ) model||usnig||MLM pre-training)
(sentiment classification task||has||MLM method)
(MLM method||is||relatively more resource intensive)
(MLM method||takes||longer)
(longer||to||converge)
(sentiment classification task||observe||self - training technique)
(self - training technique||offers||best results)
(best results||in||both XLM and XLM ft based classifiers)
(self - training technique||has||consistently improves)
(consistently improves||over||teacher model)
(Results||in||MLdoc dataset)
(MLdoc dataset||has||size)
(size||of||unlabeled samples)
(unlabeled samples||is||limited)
(MLdoc dataset||has||UDA method)
(UDA method||has||more helpful)
(Results||by utilizing||unlabeled data)
(unlabeled data||in||target language)
(unlabeled data||completely close||performance gap)
(performance gap||comparing with||best cross - lingual results and monolingual fine - tune baseline)
(Results||has||UDA algorithm and MLM pre-training)
(UDA algorithm and MLM pre-training||offer||significant improvements)
(UDA algorithm and MLM pre-training||by utilizing||unlabeled data)
(Results||has||our framework)
(our framework||reaches||new state - of - the - art results)
(new state - of - the - art results||improving over||vanilla XLM baselines)
(vanilla XLM baselines||by||44 % on average)
