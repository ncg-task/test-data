170	4	13	dimension
170	14	16	of
170	17	32	word embeddings
170	33	35	is
170	36	39	300
170	46	58	hidden units
170	59	61	of
170	62	66	LSTM
170	67	69	is
170	70	73	300
171	3	6	use
171	7	37	100 convolutional filters each
171	38	41	for
171	42	54	window sizes
171	55	57	of
171	58	67	( 3 , 3 )
171	70	85	2D pooling size
171	86	88	of
171	89	98	( 2 , 2 )
172	3	6	set
172	11	26	mini-batch size
172	27	29	as
172	30	32	10
172	41	54	learning rate
172	55	57	of
172	58	66	AdaDelta
172	67	69	as
172	74	91	default value 1.0
173	0	3	For
173	4	18	regularization
173	24	30	employ
173	31	48	Dropout operation
173	49	53	with
173	54	66	dropout rate
173	67	69	of
173	70	73	0.5
173	74	77	for
173	82	97	word embeddings
173	100	103	0.2
173	104	107	for
173	112	123	BLSTM layer
173	128	131	0.4
173	132	135	for
173	140	157	penultimate layer
173	168	171	use
173	172	183	l 2 penalty
173	184	188	with
173	189	207	coefficient 10 ? 5
173	208	212	over
173	217	227	parameters
31	23	31	proposes
31	32	79	Bidirectional Long Short - Term Memory Networks
31	80	84	with
31	85	114	Two - Dimensional Max Pooling
31	117	134	BLSTM - 2DPooling
31	137	147	to capture
31	148	156	features
31	157	159	on
31	169	223	time - step dimension and the feature vector dimension
32	3	17	first utilizes
32	18	75	Bidirectional Long Short - Term Memory Networks ( BLSTM )
32	76	88	to transform
32	93	97	text
32	98	102	into
32	103	110	vectors
33	9	33	2D max pooling operation
33	49	55	obtain
33	58	79	fixed - length vector
34	16	23	applies
34	24	56	2D convolution ( BLSTM - 2DCNN )
34	57	67	to capture
34	68	92	more meaningful features
34	93	105	to represent
34	110	120	input text
179	4	23	BLSTM - 2DCNN model
179	24	32	achieves
179	33	54	excellent performance
179	55	57	on
179	58	74	4 out of 6 tasks
180	25	42	52.4 % and 89.5 %
180	43	58	test accuracies
180	59	61	on
180	62	69	SST - 1
180	74	81	SST - 2
181	0	17	BLSTM - 2DPooling
181	18	26	performs
181	27	32	worse
181	33	37	than
181	42	71	state - of - the - art models
183	0	11	BLSTM - CNN
183	12	17	beats
183	18	31	all baselines
183	32	34	on
183	35	42	SST - 1
183	45	52	SST - 2
183	59	72	TREC datasets
184	3	6	for
184	7	27	Subj and MR datasets
184	30	43	BLSTM - 2DCNN
184	44	48	gets
184	51	75	second higher accuracies
188	0	13	Compared with
188	14	18	RCNN
188	21	34	BLSTM - 2DCNN
188	35	43	achieves
188	46	63	comparable result
190	14	18	ReNN
190	25	44	proposed two models
190	45	58	do not depend
190	59	61	on
190	62	99	external language - specific features
190	100	107	such as
190	108	130	dependency parse trees
194	14	19	DSCNN
194	22	35	BLSTM - 2DCNN
194	36	47	outperforms
194	51	53	on
194	54	67	five datasets
2	0	19	Text Classification
176	0	7	Results
