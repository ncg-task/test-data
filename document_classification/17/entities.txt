34	38	48	to develop
34	49	67	deep architectures
34	78	91	able to learn
34	92	120	hierarchical representations
34	121	123	of
34	124	139	whole sentences
34	142	149	jointly
34	150	154	with
34	159	163	task
35	30	33	use
35	34	52	deep architectures
35	53	55	of
35	56	81	many convolutional layers
35	106	111	using
35	112	127	up to 29 layers
196	4	14	dictionary
196	15	26	consists of
196	54	129	abcdefghijklmnopqrstuvwxyz0123456 789-,;.!?:'" / | # $ % & *' +=<>( ) [ ]{}
196	139	154	special padding
196	157	162	space
196	167	180	unknown token
197	4	14	input text
197	18	27	padded to
197	30	40	fixed size
197	41	43	of
197	44	48	1014
198	4	23	character embedding
198	27	29	of
198	30	37	size 16
199	0	8	Training
199	12	26	performed with
199	27	30	SGD
199	33	38	using
199	41	51	mini-batch
199	52	54	of
199	55	59	size
199	60	63	128
199	69	90	initial learning rate
199	91	93	of
199	94	98	0.01
199	103	111	momentum
199	112	114	of
199	115	118	0.9
204	4	18	implementation
204	22	32	done using
204	33	40	Torch 7
205	20	32	performed on
205	35	56	single NVidia K40 GPU
206	32	35	use
206	77	96	temporal batch norm
206	97	104	without
206	105	112	dropout
210	0	21	Our deep architecture
210	22	27	works
210	28	32	well
210	33	35	on
210	36	49	big data sets
214	4	27	most important decrease
214	28	30	in
214	31	51	classification error
214	59	70	observed on
214	75	103	largest data set Amazon Full
214	114	150	more than 3 Million training samples
212	0	3	For
212	8	22	smallest depth
212	26	29	use
212	32	54	9 convolutional layers
212	62	70	see that
212	71	80	our model
212	89	97	performs
212	98	104	better
212	105	109	than
212	110	142	Zhang 's convolutional baselines
212	151	159	includes
212	160	182	6 convolutional layers
212	193	215	different architecture
217	21	24	for
217	27	38	small depth
217	41	63	temporal max - pooling
217	64	69	works
217	70	74	best
217	75	77	on
217	78	91	all data sets
2	37	56	Text Classification
