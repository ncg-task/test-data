title
Practical Text Classification With Large Pre-Trained Language Models
abstract
Multi-emotion sentiment classification is a natural language processing ( NLP ) problem with valuable use cases on realworld data .
We demonstrate that large - scale unsupervised language modeling combined with finetuning offers a practical solution to this task on difficult datasets , including those with label class imbalance and domain - specific context .
By training an attention - based Transformer network ( Vaswani et al. 2017 ) on 40 GB of text ( Amazon reviews ) ( McAuley et al. 2015 ) and fine - tuning on the training set , our model achieves a 0.69 F1 score on the SemEval Task 1:E - c multidimensional emotion classification problem ( Mohammad et al. 2018 ) , based on the Plutchik wheel of emotions ( Plutchik 1979 ) .
These results are competitive with state of the art models , including strong F 1 scores on difficult ( emotion ) categories such as Fear ( 0.73 ) , Disgust ( 0.77 ) and Anger ( 0.78 ) , as well as competitive results on rare categories such as Anticipation ( 0.42 ) and Surprise ( 0.37 ) .
Furthermore , we demonstrate our application on a real world text classification task .
We create a narrowly collected text dataset of real tweets on several topics , and show that our finetuned model outperforms general purpose commercially available APIs for sentiment and multidimensional emotion classification on this dataset by a significant margin .
We also perform a variety of additional studies , investigating properties of deep learning architectures , datasets and algorithms for achieving practical multidimensional sentiment classification .
Overall , we find that unsupervised language modeling and finetuning is a simple framework for achieving high quality results on realworld sentiment classification .
Introduction
Recent work has shown that language models - both RNN variants like the multiplicative LSTM ( m LSTM ) , as well as the attention - based Transformer network ) - can be trained efficiently over very large datasets , and that the resulting models can be transferred to downstream language understanding problems , often matching or exceeding the previous state of the art approaches on academic datasets .
However , how well do these models perform on practical text classification problems , with real world data ?
Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .
All rights reserved .
In this work , we train both mLSTM and Transformer language models on a large 40 GB text dataset , then transfer those models to two text classification problems : binary sentiment ( including Neutral labels ) , and multidimensional emotion classification based on the Plutchik wheel of emotions .
We examine our performance on these tasks , both against large academic datasets , and on an original text dataset that we compiled from social media messages about several specific topics , such as video games .
We demonstrate that our approach matches the state of the art on the academic datasets without domain - specific training and without excessive hyper - parameter tuning .
Meanwhile on the social media dataset , our approach outperforms commercially available APIs by significant margins , even when those models are re-calibrated to the test set .
Furthermore , we notice that 1 ) the Transformer model generally out - performs the m LSTM model , especially when fine - tuning on multidimensional emotion classification , and 2 ) fine - tuning the model significantly improves performance on the emotion tasks , both for the m LSTM and the Transformer model .
We suggest that our approach creates models with good generalization to increasingly difficult text classification problems , and we offer ablation studies to demonstrate that effect .
It is difficult to fit a single model for text classification across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .
For example , words such as war and sick are not necessarily negative in the context of video games , which are significantly represented in our dataset .
By training a language model across a large text dataset , we expose our model to many contexts .
Perhaps a small amount of downstream transfer is enough to choose the right context features for emotion classification in the appropriate setting .
Our work shows that unsupervised language modeling combined with finetuning offers a practical solution to specialized text classification problems , including those with large category class imbalance , and significant human label dis agreement .
Background
Supervised learning is difficult to apply to NLP problems because labels are expensive .
Following , and , we train unsupervised text models on large amounts of unlabelled text data , and transfer the model features to small supervised text problems .
The supervised text classification problem used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .
Some of these binary text examples are subtle .
Prior works show that unsupervised language models can learn nuanced features of text , such as word ordering and double negation , just from the underlying task of next - word prediction .
However , while this includes difficult examples , it does not necessarily represent sentiment on practical text problems .
The source material ( professionally written movie reviews ) does not include colloquial language .
The dataset excludes Neutral sentiment texts and those with weak directional sentiment .
The dataset does not include dimensions of sentiment apart from Positive and Negative .
Plutchik 's Wheel of Emotions
We focus our multidimension emotion classification on Plutchik 's wheel of emotions .
This taxonomy , in use since 1979 , aims to classify human emotions as a combination of four dualities :
Joy - Sadness , Anger - Fear , Trust - Disgust , and Surprise - Anticipation .
According to the basic emotion model , while humans experience hundreds of emotions , some emotions are more fundamental than others .
The commercial general purpose emotion classification API that we compare against , IBM 's Watson 1 , offers classification scores for the Joy , Sadness , Fear , Disgust and Anger emotions - all present in Plutchik 's wheel (.
Sem Eval Multidimension Emotion Dataset
The Se-m Eval Task 1:E - c problem ) offers a training set of 6,857 tweets , with binary labels for the eight Plutchik categories , plus Optimism , Pessimism , and Love .
This dataset was created through a process of text selection and human labeling .
We show our results on this dataset and compare it to the current state of the art performance .
While it is not possible to report rater agreement on these categories for the compilation of the dataset , the authors note that 2 out of 7 raters had to agree for a positive label to be applied , as requiring larger agreement caused a scarcity of .
labels for some categories .
This indicates that some of the categories had significant rater dis agreement between the human raters .
The dataset also included a substantial degree of label class imbalance , with some categories like Anger ( 37 % ) , Disgust ( 38 % ) , Joy ( 36 % ) and Sadness ( 29 % ) represented often in the dataset , while others like Trust ( 5 % ) and Surprise ( 5 % ) present much less frequently ) .
This class imbalance and human rater dis agreement is not uncommon for real world text classification problems 2 .
Company Tweet Dataset
In addition to the SemEval tweet dataset , we wanted to see how our model would perform on a similar but domain - specific task : Plutchik emotion classification on tweets relevant to a particular company .
We collected tweets on a variety of topics , including :
Video game tweets Tweets about the company stock
We submitted the first batch of 4,000 tweets to human
2 We submitted the SemEval training set for re-labeling using our rater instructions .
See for an estimate of rater dis agreement over the SemEval training set . 13,326 11.7 12.9 6.8 2.9 20.6 4.2 5.0 7.6 8.9/47.0 raters on the FigureEight 3 platform , with rules similar to those used by SemEval , which also used the FigureEight platform for human labeling .
Specifically , we verified that raters passed our golden set ( answering 70 % of test questions correctly ) .
We applied positive labels for each category where 2 out of 5 raters agreed .
This is slightly less permissive than the 2 out of 7 raters used by SemEval , because we did not have a budget for 7 raters per tweet .
After the first pass , we noticed that random sampling led to some categories being severely under - sampled , below 5 % of tweets .
Thus we employed a bootstrapping technique to pre-classify tweets by category using our current model , and choose tweets with more likely emotion tweets for classification .
See Active Learning section for details .
We also sampled 5,000 tweets balanced by source category , since video game tweets have much more emotion , thus dominated the bootstrapped selections .
Henceforth , we refer to the combined company tweets dataset consisting of : 4,021 random tweets 5,024 tweets selected for higher emotion content 4,281 tweets selected for source category balance Finetuning Recent work has shown promising results using unsupervised language modeling , followed by transfer learning to natural language tasks ( Radford , Jzefowicz , and Sutskever 2017 ) , ) .
Furthermore , these models benefit when the entire model is fine - tuned on the transfer task , as demonstrated in .
Specifically , these methods have beaten the state of the art on binary sentiment classification .
These models have also attained the best over all score on the GLUE Benchmark 4 ) , comprised of a variety of text understanding tasks , including entailment and question answering .
Methodology
We use a larger batch size with shorter sequence length , specifically a global batch of 512 and sequence length 64 tokens ( tokenized with a 32,000 BPE vocabulary , as detailed in Characters and Subword Units .
The shorter sequence length works well because the transfer target are tweets , which are short pieces of text .
We trained our language model on the Amazon Reviews dataset rather than other large datasets like BooksCorpus , because reviews are rich in emotional context .
We also train an m LSTM network on the same dataset , based on the model from .
We chose to compare these particular models because they work in fundamentally different ways and because they collectively hold state of the art results on many significant academic NLP benchmarks .
We wanted to test these models on difficult classification problems with real - world data .
Unsupervised Pretraining .
The language modeling objective can be summarized as a maximum likelihood estimation problem for a sequence of tokens .
We treat our model as a function with two parts : an encoder f e and decoder f d .
The encoder forms the bulk of the model , including the token embedding dictionary as the first module .
The decoder is simply a softmax linear layer that projects the encoder output into the dimension equal to the vocabulary size .
The objective to maximize is as follows .
where h l t is a hidden layer activation in the final layer off e , indexed 1 . . . l for timestep t.
The model is tasked with predicting the next token given all of the ones prior by outputting a probability distribution over the vocabulary of tokens .
Doing this for each timestep t produces each term in the sum of the log - likelihood formulation , and so maximizing the correct probabilities is away to understand the joint probability distribution of sequences in this corpus of text .
Characters and Subword Units .
While , and ) have shown state of the art results for language modeling and task transfer with character - level m LSTM models , we found that our ) uses a bytepair encoding vocabulary with 40,000 word pieces for their state of the art results on language transfer tasks with a Transformer model .
Our work closely follows their model .
Supervised Finetuning .
After the pretraining , we initialize a new decoder f d to be exclusively trained on the supervised problem .
Depending on the task , this decoder maybe a single linear layer with activation or an MLP .
We also retain the original decoder f d and continue to train it by using language modeling as an auxiliary loss when finetuning on the new corpus .
Error signals from both decoders are backpropagated into the language model .
The differences between the hyperparameters for finetuning and language modeling are described in .
ELMo Baseline
We also compare our language models to ELMo ) , a contextualized word representation based on a deep bidirectional language model , trained on large text corpus .
We use a publicly available pretrained Multihead vs .
Single Head Finetuning Decoders
The tweet datasets are an example of a multilabel classification problem .
We can formulate the problem for the finetuning decoder , f d as either a collection of single binary problems or multiple problems put together .
The single binary problem formulation allows for a focus on one class and end - to - end optimization will only have one error signal .
However , because the label classes are imbalanced in all categories , this may lead to a sparse gradient signal for the positive label , which may impact recall and precision .
Increasing the size off d to more than one linear layer leads to rapid overfitting and lower validation performance .
The combined binary problems formulation ( henceforth described as multihead ) allows for a richer error signal that propagates more information through the encoder f e and sentiment representation inf d .
In this setup , constructing a Multilayer network is far more useful , and can bethought of as specifically creating sentiment features to be used at the final layer to predict the presence of the individual emotions .
We find that the inclusion of easier , more balanced label categories improves performance on harder ones in .
However , the easier categories have slightly lower performance because the network is not being optimized for only those categories .
Thresholding Supervised Results
For both the multihead MLP and the single linear layer instantiating off d , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as t * = 0.5 .
This makes sense since the label classes for most categories are very imbalanced .
For thresholding , we take a dataset of tweets and split it into training ( 70 % ) , thresholding ( 10 % ) and validation ( 20 % ) sets .
At each epoch of finetuning on the training set , we calculate validation accuracy and save predictions on the threshold set on the epoch for which this is maximized .
To threshold , we search the discretized version of [ 0 , 1 ] : the linear space T = { i 200 : 1 ? i ? 200 } for the positive label threshold for each category .
We denoted the threshold which gave the best score on the threshold set as t * .
IBM Watson and Google NLP 6 both offer commercial APIs for binary sentiment analysis , producing scalar values that correspond to a continuous [ - 1 , + 1 ] sentiment score .
We applied our thresholding procedure to these scores .
In the case of classification with neutrals we create two thresholds 0 < t * 1 < t * 2 < 1 which we individually optimized jointly over T as well .
With the finetuning procedure , we found success with a decoder f d = MLP ( 64 , 2 ) , whose two output units ? p ,?
n are probability estimates of the positive and negative labels y p , y n .
These units both have sigmoid activations , since we denote a neutral as y p = y n =
0 . To threshold these predictions , we searched the cartesian product T T to determine 0 < t * p , t * n < 1 .
Active Learning
We hypothesized that we could achieve greater precision and recall on our datasets if our class label were more equally balanced .
To this end , we employed an active learning procedure to select unlabeled tweets to be labeled .
The algorithm consisted of first finetuning a language model f = ( f e , f d , f d ) on labeled tweets for 5 epochs .
At peak validation accuracy , we obtain predictions P ?
R 8 nu , for Plutchik sentiment on the unlabeled tweets .
From the labeled dataset , we calculate the negative class percentage for each category v ? R 8 .
Then we obtain category a weighting parameter w = 10 ( v ? 0.5 ) so that w i ? [ ? 5 , 5 ] for i ?
1 . . .
8 . Then , we get scores for each unlabeled point as weighted features : s = e w P ?
R nu .
This way , positive predictions for sentiment categories are weighted by how much they would contribute towards balancing all of the class distributions .
The scores s are used as weights in a weighted uniform random sampler , and from this , we sampled 5,000 tweets to be labeled .
We found that over all , the method produced tweets with more emotion .
Not only was the positive class balance averaged across label categories higher ( 11.2 % compared to 8.2 % for random sampling ) , but the percentage of tweets which had no emotion was dramatically lower : 35.6 % compared to 52.1 % for random sampling .
We hence achieved better class balance than the dataset prior to the augmentation .
Results
Binary Sentiment Tweets
For binary sentiment , we compare our model on two tasks : the academic SST dataset , which consists of a balanced set of Positive and Negative labels , and the company tweets dataset , which consists of a balance between Positive , Neutral and Negative labels .
See.
While the Transformer gets close but does not exceed the state of the art on the SST dataset , it exceeds both the mL - STM and ELMo baseline as well as both Watson and Google Sentiment APIs on the company tweets .
This is despite optimally calibrating the API results on the test set .
Multi - Label Emotion Tweets
The IBM Watson API offers multi-label emotion predictions for five categories : Anger , Disgust , Fear , Joy and Sadness .
We compare our models to Watson on these categories for both the SemEval dataset and the company tweets in .
We find that our models outperform Watson on every emotion category .
Sem Eval Tweets
We submitted our finetuned Transformer model to the SemEval Task1:E - C challenge , as seen in Table 6 .
These results were computed by the organizers on a golden test set , for which we do not have access to the truth labels .
Our model achieved the top macro-averaged F1 score among all submission , with competitive but lower scores for the micro -average F1 an the Jaccard Index accuracy 8 .
This suggests that our model out - performs the other top submission on rare and difficult categories , since macroaverage weighs performance on all classes equally , and the most common categories of Joy , Anger , Disgust and Optimism get relatively higher F 1 scores across all models .
We also compare the deep learning architectures of the Transformer and m LSTM on this dataset in and find that the Transformer outperforms the m LSTM across Plutchik categories .
The winner of the Task 1:E - c challenge ) trained a bidirectional LSTM with an 800,000 word embedding vocabulary derived from training word vectors ) on a dataset of 550 million tweets .
Similarly , the second place winner of the SemEval leaderboard trained a word - level bidirectional LSTM with attention , as well as including non-deep learning features into their ensemble .
Both submissions used training data across SemEval tasks , as well as additional training data outside of the training set .
In comparison , we demonstrate that finetuning can be as effective on this task , despite training only on 7,000 tweets .
Furthermore , out language modeling took place on the Amazon Reviews dataset , which does not contain emoji , hashtags or usernames .
We would expect to see improvements if our unsupervised dataset contained emoji , for example .
Plutchik on Company Tweets
Our models gets lower F 1 scores on the company tweets dataset than on equivalent Se -m Eval categories .
As with the SemEval challenge tweets , the Transformer outperformed the mLSTM .
These results are shown in .
Both models performed significantly better than the Watson API on all categories for which Watson supplies predictions .
We could not conclusively determine whether the singlehead or the multihead Transformer will perform better on a given task .
Thus we recommend trying both methods on a new dataset .
Analysis
Classification Performance by Dataset Size
We would have liked to label more data for the company tweets dataset , and thus looked into how much extra labeling contributes to finetuned model performance accuracy .
First , let us explain the difference between micro and macro averaging of the F 1 scores .
We can summarize the F 1 scores of categories c ?
C ( or any other metric M ) through macro and micro averaging to obtain M .
The macro method weights each class equally by averaging the metric calculated on each individual class .
The micro method accounts for the class imbalances in each category by aggregating all of the true / false positives / negatives first , and then calculating an over all metric .
In one experiment , we decreased the size of the training dataset and observed the resulting macro and micro averaged F 1 scores across all categories on company tweets .
The results are shown in .
We observe that the macro average is more sensitive to dataset size and falls more quickly than the micro average .
The interpretation of this is that categories with worse class imbalance ( which consequently influence macro more than micro average ) benefit more from having a larger training dataset size .
This suggests that we may obtain substantially better results with more data in the harder categories .
We conducted a related experiment that focused on the difference in category performance when using a single head versus a multihead decoder f d .
We apply the two architectures at different training dataset sizes for three different label categories : Anger , Anticipation and Trust , which we categorize as low , medium and high difficulty , respectively .
As seen in it appears that the difference between the single and multihead becomes more pronounced for more difficult categories , as well as for smaller dataset sizes .
We do not have enough data to make a firm conclusion , but this study suggests that we could get more out of the labeled data that we have , by studying which categories benefit from single head and multihead decoders .
All categories benefit from more training data , but some categories benefit from from marginal labeled data than others .
This suggests further and more rigorous study of the boostrapping methods we used to select tweets for our human labeling budget , as described in the Active Learning section .
Following a similar process , we required 2 out of 5 raters for a positive label , and in the case of binary sentiment labels ( Positive , Neutral , Negative ) , we rounded toward polarized sentiment and away from Neutral labels in the case of a 2 / 3 split .
Applying the SemEval - trained Transformer directly to our company tweets dataset gets reasonably good results ( 0.338 macro average ) , also validating that our labeling technique is similar to that of SemEval .
Looking at rater agreement by dataset , we see that Plutchik category labels contain large rater dis agreement , even among vetted raters who passed the golden set test .
Furthermore , datasets with more emotions ( the SemEval dataset and our active learning sampled company tweets ) contain higher Plutchik dis agreement than random company tweets .
This is likely because raters tend to apply the " No Emotion " label when they are not sure about a category .
As shows , the SemEval and active company tweets datasets contain fewer no-emotion tweets than other datsets .
It would be interesting to analyze rater dis agreement by category , how much this effects classifier convergence , whether getting 7 + ratings per tweet helps classifier convergence , and also whether this work could benefit from estimating rater quality via agreement with the crowd , as proposed in .
However this analysis is not straightforward , as the truth data is itself collected through human labeling .
Alongside classifier convergence by dataset size , we think that this could bean interesting area a future research .
Difficult tweets and challenging contexts .
There is not sufficient space for a thorough analysis , but we wanted to suggest why general purpose APIs may notwork well on our company tweets dataset .
samples the largest binary sentiment dis agreements between human raters and the Wat-son API .
For simplicity , we restrict examples to video game tweets , which comprise 19.1 % of our test set .
As we can see , all of these examples appear to ascribe negative emotion to generally negative terms which , in a video game context , do not indicate negative sentiment .
Our purpose is not to castigate the Watson or the GCL APIs .
Rather , we propose that it may not be possible to provide context - independent emotion classification scores that work well across text contexts .
It may work better in practice , on some tasks , to train a large unsupervised model and to use a small amount of labeled data to finetune on the context present in the specific dataset .
We would like to quantify this further in future work .
Recent work shows that training an RNN with multiple softmax outputs leads to a much improved BPC on language modeling , especially for diverse datasets and models with large vocabularies .
This is because the multiple softmaxes are able to capture a larger number of distinct contexts in the text than a single output .
Perhaps our Transformer also captures the features relevant to a large number of distinct contexts , and the finetuning is able to select the most significant of these features , while ignoring those features that - while adding value in general - are not appropriate in a video game setting .
Conclusion
In this work we demonstrate that unsupervised pretraining and finetuning provides a flexible framework that is effective for difficult text classification tasks .
We noticed that the finetuning was especially effective with the Transformer network , when transferring to downstream tasks with noisy labels and specialized context .
We think that this framework makes it easy to customize a text classification model on niche tasks .
Unsupervised language modeling can be done on general text datasets , and requires no labels .
Meanwhile downstream task transfer works well enough , even on small amounts of domain - specific labelled data , to be accessible to most academics and small organization .
It would be great to see this approach applied to a variety of practical text classification problems , much as
