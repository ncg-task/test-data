105	3	12	developed
105	17	23	system
105	24	29	using
105	34	69	PyTorch framework 2 , release 0.4.1
108	3	11	minimize
108	16	46	negative log - likelihood loss
108	47	52	using
108	53	68	teacher forcing
108	73	77	Adam
23	74	81	present
23	84	132	character - level sequence - to - sequence model
23	133	137	with
23	138	157	attention mechanism
23	163	173	results in
23	176	221	completely neural end - to - end architecture
24	50	66	does not require
24	67	83	delexicalization
24	86	98	tokenization
24	103	114	lowercasing
25	86	94	produces
25	97	120	vocabulary - free model
25	129	152	inherently more general
25	161	179	does not depend on
25	182	213	specific domain 's set of terms
25	227	229	on
25	232	248	general alphabet
27	133	164	character - wise copy mechanism
27	167	180	consisting in
27	183	194	soft switch
27	195	202	between
27	203	227	generation and copy mode
27	322	349	peculiar training procedure
27	358	366	improves
27	371	407	internal representation capabilities
27	410	419	enhancing
27	420	426	recall
27	432	443	consists in
27	448	456	exchange
27	89	91	of
27	460	484	encoder and decoder RNNs
2	41	86	Character - based Data - to - text Generation
4	110	137	natural language generation
8	128	143	text generation
137	2	26	first interesting result
137	45	51	EDA_CS
137	52	66	always obtains
137	67	87	higher metric values
137	88	103	with respect to
137	104	108	TGen
137	109	111	on
137	116	145	Hotel and Restaurant datasets
138	25	30	E2E +
138	33	37	TGen
138	38	46	achieves
138	72	86	metrics values
138	47	52	three
138	53	59	out of
138	60	64	five
140	37	55	approach EDA_CS TL
140	66	72	obtain
140	73	91	better performance
140	92	107	with respect to
140	108	116	training
140	117	123	EDA_CS
140	124	126	in
140	131	143	standard way
140	144	146	on
140	151	180	Hotel and Restaurant datasets
141	11	20	EDA_CS TL
141	21	26	shows
141	29	43	bleu increment
141	44	46	of
141	47	60	at least 14 %
141	61	76	with respect to
141	77	90	TGen 's score
141	96	107	compared to
141	108	142	both Hotel and Restaurant datasets
142	14	28	baseline model
142	31	34	EDA
142	40	60	largely outperformed
142	61	63	by
142	64	90	all other examined methods
118	0	7	Results
