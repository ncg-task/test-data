158	3	7	used
158	8	36	one - layer pointer networks
158	37	43	during
158	44	60	content planning
158	67	84	two - layer LSTMs
158	85	91	during
158	92	107	text generation
159	0	13	Input feeding
159	18	30	employed for
159	35	47	text decoder
161	0	6	Models
161	12	23	trained for
161	24	33	25 epochs
161	34	38	with
161	43	60	Adagrad optimizer
161	67	88	initial learning rate
161	93	97	0.15
161	100	119	learning rate decay
161	124	137	selected from
161	138	152	{ 0.5 , 0.97 }
161	159	169	batch size
161	174	175	5
164	0	10	All models
164	15	29	implemented in
164	30	43	Open NMT - py
160	3	10	applied
160	11	18	dropout
160	21	23	at
160	26	30	rate
160	31	33	of
160	34	37	0.3
162	0	3	For
162	4	17	text decoding
162	23	34	made use of
162	35	39	BPTT
162	46	49	set
162	54	69	truncation size
162	70	72	to
162	73	76	100
163	3	6	set
163	11	20	beam size
163	21	23	to
163	24	25	5
20	10	16	learns
20	19	31	content plan
20	32	36	from
20	41	46	input
20	51	64	conditions on
20	69	81	content plan
20	91	102	to generate
20	107	122	output document
22	3	8	train
22	19	33	end - to - end
22	34	39	using
22	40	55	neural networks
179	17	20	NCP
179	21	29	improves
179	30	34	upon
179	35	86	vanilla encoderdecoder models ( ED + JC , ED + CC )
179	89	104	irrespective of
179	109	123	copy mechanism
180	14	22	achieves
180	23	40	comparable scores
180	41	45	with
180	46	88	either joint or conditional copy mechanism
180	95	104	indicates
180	120	135	content planner
180	142	148	brings
180	149	173	performance improvements
181	10	18	NCP + CC
181	19	27	achieves
181	28	78	best content selection and content ordering scores
181	79	90	in terms of
181	91	95	BLEU
184	14	37	template - based system
184	72	79	obtains
184	80	83	low
184	84	105	BLEU and CS precision
184	110	116	scores
184	117	121	high
184	122	124	on
184	125	149	CS recall and RG metrics
182	0	11	Compared to
182	16	36	best reported system
182	60	67	achieve
182	71	91	absolute improvement
182	92	94	of
182	95	113	approximately 12 %
182	114	125	in terms of
182	126	145	relation generation
182	148	165	content selection
182	166	175	precision
182	181	189	improves
182	190	192	by
182	193	196	5 %
182	201	207	recall
182	208	210	by
182	211	215	15 %
182	218	234	content ordering
182	235	244	increases
182	245	247	by
182	248	251	3 %
182	258	262	BLEU
182	263	265	by
182	266	276	1.5 points
2	0	27	Data - to - Text Generation
165	0	7	Results
