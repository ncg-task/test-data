title
From POS tagging to dependency parsing for biomedical event extraction
abstract
Background :
Given the importance of relation or event extraction from biomedical research publications to support knowledge capture and synthesis , and the strong dependency of approaches to this information extraction task on syntactic information , it is valuable to understand which approaches to syntactic processing of biomedical text have the highest performance .
We perform an empirical study comparing state - of - the - art traditional feature - based and neural network - based models for two core natural language processing tasks of part - of - speech ( POS ) tagging and dependency parsing on two benchmark biomedical corpora , GENIA and CRAFT .
To the best of our knowledge , there is no recent work making such comparisons in the biomedical context ; specifically no detailed analysis of neural models on this data is available .
Experimental results show that in general , the neural models outperform the feature - based models on two benchmark biomedical corpora GENIA and CRAFT .
We also perform a task - oriented evaluation to investigate the influences of these models in a downstream application on biomedical event extraction , and show that better intrinsic parsing performance does not always imply better extrinsic event extraction performance .
We have presented a detailed empirical study comparing traditional feature - based and neural network - based models for POS tagging and dependency parsing in the biomedical context , and also investigated the influence of parser selection for a biomedical event extraction downstream task .
Availability of data and material :
We make the retrained models available at https://github.com/datquocnguyen/BioPosDep.
Background
The biomedical literature , as captured in the parallel repositories of PubMed ( abstracts ) and PubMed Central ( full text articles ) , is growing at a remarkable rate of over one million publications per year .
Effort to catalog the key research results in these publications demands automation .
Hence extraction of relations and events from the published literature has become a key focus of the biomedical natural language processing community .
Methods for information extraction typically make use of linguistic information , with a specific emphasis on the value of dependency parses .
A number of linguistically - annotated resources , notably including the GENIA and CRAFT corpora , have been * Correspondence : dqnguyen@unimelb.edu.au ,
The University of Melbourne Karin.Verspoor@unimelb.edu.au ,
The University of Melbourne https://www.ncbi.nlm.nih.gov/pubmed
https://www.ncbi.nlm.nih.gov/pmc produced to support development and evaluation of natural language processing ( NLP ) tools over biomedical publications , based on the observation of the substantive differences between these domain texts and general English texts , as captured in resources such as the Penn Treebank thatare standardly used for development and evaluation of syntactic processing tools .
Recent work on biomedical relation extraction has highlighted the particular importance of syntactic information .
Despite this , that work , and most other related work , has simply adopted a tool to analyze the syntactic characteristics of the biomedical texts without consideration of the appropriateness of the tool for these texts .
A commonly used tool is the Stanford CoreNLP dependency parser , although domain - adapted parsers ( e.g. ) are sometimes used .
Prior work on the CRAFT treebank demonstrated substantial variation in the performance of syntactic processing tools for that data .
Given the significant improvements in parsing performance in the last arXiv : 1808.03731v2 [ cs. CL ] 2 Jan 2019
Pre-trained POS tagger Retrained POS tagger
Pre-trained parser
Retrained parser
Event extraction few years , thanks to renewed attention to the problem and exploration of neural methods , it is important to revisit whether the commonly used tools remain the best choices for syntactic analysis of biomedical texts .
In this paper , we therefore investigate current stateof - the - art ( SOTA ) approaches to dependency parsing as applied to biomedical texts .
We also present detailed results on the precursor task of POS tagging , since parsing depends heavily on POS tags .
Finally , we study the impact of parser choice on biomedical event extraction , following the structure of the extrinsic parser evaluation shared task ( EPE 2017 ) for biomedical event extraction .
We find that differences in over all intrinsic parser performance do not consistently explain differences in information extraction performance .
Experimental methodology
In this section , we present our empirical approach to evaluate different POS tagging and dependency parsing models on benchmark biomedical corpora .
illustrates our experimental flow .
In particular , we compare pre-trained and retrained POS taggers , and investigate the effect of these pre-trained and retrained taggers in pre-trained parsing models ( in the first five rows of ) .
We then compare the performance of retrained parsing models to the pre-trained ones ( in the last ten rows of ) .
Finally , we investigate the influence of pre-trained and retrained parsing models in the biomedical event extraction task ( in ) .
Datasets
We use two biomedical corpora : GENIA and CRAFT .
GENIA includes abstracts from PubMed , while CRAFT includes full text publications .
It has been observed that there are substantial linguistic differences between the abstracts and the corresponding full text publications ; hence it is important to consider both contexts when assessing NLP tools in biomedical domain .
The GENIA corpus contains 18K sentences ( ? 486K words ) from 1,999 Medline abstracts , which are manually annotated following the Penn Treebank ( PTB ) bracketing guidelines .
On this treebank , we use the training , development and test split from .
We then use the Stanford constituent - to - dependency conversion toolkit ( v3.5.1 ) to generate dependency trees with basic Stanford dependencies .
The CRAFT corpus includes 21 K sentences ( ? 561K words ) from 67 full - text biomedical journal articles .
These sentences are syntactically annotated using an extended PTB tag set .
Given this extended set , the Stanford conversion toolkit is not suitable for generating dependency trees .
Hence , a dependency treebank using the CoNLL 2008 dependencies was produced from the CRAFT treebank using ClearNLP ; we directly use this dependency treebank in our experiments .
We use sentences from the first 6 files ( PubMed IDs : 11532192-12585968 ) for development and sentences from the next 6 files ( PubMed IDs : 12925238-15005800 ) for testing , while the the remaining 55 files are used for training .
gives an overview of the experimental datasets , while details corpus statistics .
We also include out - of - vocabulary ( OOV ) rate in .
OOV rate is relevant because if a word has not been observed in the training data at all , the tagger / parser is limited to using contextual clues to resolve the label ( i.e. it has observed no prior usage of the word during training and hence has no experience with the word to draw on ) .
POS tagging models
We compare SOTA feature - based and neural networkbased models for POS tagging over both GENIA and CRAFT .
We consider the following :
MarMoT is a well - known generic CRF framework as well as a leading POS and morphological tagger .
NLP4 J's POS tagging model is a dynamic feature induction model that automatically optimizes feature combinations .
NLP4J is the successor of ClearNLP .
https://nlp.stanford.edu/~mcclosky/biomedical.html
http://bionlp-corpora.sourceforge.net/CRAFT
http://cistern.cis.lmu.de/marmot
https://emorynlp.github.io/nlp4j/components/
part-of-speech-tagging.html
Statistics by the most frequent dependency and overlapped POS labels , sentence length ( i.e. number of words in the sentence ) and relative dependency distances i ?
j from a dependent w i to it s head w j .
In addition , % G and % C denote the occurrence proportions in GENIA and CRAFT , respectively .
Dependency labels POS tags
is a sequence labeling model which extends a standard BiLSTM neural network with a CRF layer . BiLSTM - CRF + CNN - char extends the model BiLSTM - CRF with character - level word embeddings .
For each word token , its character - level word embedding is derived by applying a CNN to the word 's character sequence .
BiLSTM - CRF + LSTM - char also extends the BiLSTM - CRF model with character - level word embeddings , which are derived by applying a BiL - STM to each word 's character sequence .
For the three BiLSTM - CRF - based sequence labeling models , we use a performance - optimized implementation from .
As detailed later in the POS tagging results section , we use NLP4J - POS to predict POS tags on development and test sets and perform 20 - way jackknifing to generate POS tags on the training set for dependency parsing .
Dependency parsers
Our second study assesses the performance of SOTA dependency parsers , as well as commonly used parsers , on biomedical texts .
Prior work on the CRAFT treebank identified the domain - retrained ClearParser , now part of the NLP4J toolkit , as a top - performing system for dependency parsing over that data .
It remains the best performing non-neural model for dependency parsing .
In particular , we compare the following parsers : https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf
The Stanford neural network dependency parser ( Stanford - NNdep ) is a greedy transitionbased parsing model which concatenates word , POS tag and arc label embeddings into a single vector , and then feeds this vector into a multilayer perceptron with one hidden layer for transition classification .
NLP4J 's dependency parsing model ( NLP4Jdep ) is a transition - based parser with a selectional branching method that uses confidence estimates to decide when employing a beam . jPTDP v 1 is a joint model for POS tagging and dependency parsing , which uses BiLSTMs to learn feature representations shared between POS tagging and dependency parsing .
jPTDP can be viewed as an extension of the graph - based dependency parser bmstparser , replacing POS tag embeddings with LSTM - based character - level word embeddings .
For jPTDP , we train with gold standard POS tags . The Stanford " Biaffine " parser v 1 extends bmstparser with biaffine classifiers to predict dependency arcs and labels , obtaining the highest parsing result to date on the benchmark English PTB .
The Stanford Biaffine parser v2 , further extends v 1 with LSTM - based character - level word embeddings , obtaining the highest result ( i.e. , 1 st place ) at the CoNLL 2017 shared task on multilin - https://nlp.stanford.edu/software/nndep.shtml
https://emorynlp.github.io/nlp4j/components/
dependency-parsing.html
https://github.com/datquocnguyen/jPTDP
gual dependency parsing .
We use the Stanford Biaffine parser v 2 in our experiments .
Implementation details
We use the training set to learn model parameters while we tune the model hyper - parameters on the development set .
Then we report final evaluation results on the test set .
The metric for POS tagging is the accuracy .
The metrics for dependency parsing are the labeled attachment score ( LAS ) and unlabeled attachment score ( UAS ) :
LAS is the proportion of words which are correctly assigned both dependency arc and label while UAS is the proportion of words for which the dependency arc is assigned correctly .
For the three BiLSTM - CRF - based models , Stanford - NNdep , jPTDP and Stanford - Biaffine which utilizes pre-trained word embeddings , we employ 200 dimensional pre-trained word vectors from .
These pre-trained vectors were obtained by training the Word2Vec skip - gram model on a PubMed abstract corpus of 3 billion word tokens .
For the traditional feature - based models MarMoT , NLP4J - POS and NLP4J - dep , we use their original pure Java implementations with default hyperparameter settings .
For the BiLSTM - CRF - based models , we use default hyper - parameters provided in with the following exceptions : for training , we use Nadam and run for 50 epochs .
We perform a grid search of hyperparameters to select the number of BiLSTM layers from { 1 , 2 } and the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .
Early stopping is applied when no performance improvement on the development set is obtained after 10 contiguous epochs .
For Stanford - NNdep , we select the word CutOff from { 1 , 2 } and the size of the hidden layer from { 100 , 150 , 200 , 250 , 300 , 350 , 400 } and fix other hyperparameters with their default values .
For jPTDP , we use 50 - dimensional character embeddings and fix the initial learning rate at 0.0005 .
We also fix the number of BiLSTM layers at 2 and select the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .
Other hyper - parameters are set at their default values .
For Stanford - Biaffine , we use default hyper - parameter values .
These default values can be considered as optimal ones as they helped producing the highest scores for 57 test sets ( including English test sets ) and second highest scores for 14 test sets over total 81 test sets across 45 different languages at the CoNLL 2017 shared task .
https://github.com/tdozat/Parser-v2
POS tagging accuracies on the test set with gold tokenization .
[ ] denotes a result with a pre-trained POS tagger .
We do not provide accuracy results of the pre-trained POS taggers on CRAFT because CRAFT uses an extended PTB POS tag set ( i.e. there are POS tags in CRAFT thatare not defined in the original PTB POS tag set ) .
Corpus - level accuracy differences of at least 0.17 % in GENIA and 0.26 % in CRAFT between two POS tagging models are significant at p ? 0.05 .
Here , we compute sentence - level accuracies , then use paired t- test to measure the significance level .
Main results , when trained on 90 % of the GENIA corpus ( cf. our 85 % training set ) .
It does not support a ( re ) - training process .
Model
POS tagging results
In general , we find that the six retrained models produce competitive results .
BiLSTM - CRF and Mar - MoT obtain the lowest scores on GENIA and CRAFT , respectively .
jPTDP obtains a similar score to Mar - MoT on GENIA and similar score to BiLSTM - CRF on CRAFT .
In particular , MarMoT obtains accuracy results at 98.61 % and 97.07 % on GENIA and CRAFT , which are about 0.2 % and 0.4 % absolute lower than NLP4J - POS , respectively .
NLP4J - POS uses additional features based on Brown clusters and pre-trained word vectors learned from a large external corpus , providing useful extra information .
BiLSTM - CRF obtains accuracies of 98.44 % on GE - NIA and 97.25 % on CRAFT .
Using character - level word embeddings helps to produce about 0.5 % and Trained on the PTB sections 0 - 18 , the accuracies for the GENIA tagger , Stanford tagger , MarMoT , NLP4J - POS , BiLSTM- CRF and BiLSTM - CRF + CNN - char on the benchmark test set of PTB sections 22 - 24 were reported at 97.05 % , 97.23 % , 97.28 % , 97.64 % , 97.45 % and 97.55 % , respectively .
Parsing results on the test set with predicted POS tags and gold tokenization ( except [ G ] which denotes results when employing gold POS tags in both training and testing phases ) .
" Without punctuation " refers to results excluding punctuation and other symbols from evaluation .
" Exact match " denotes the percentage of sentences whose predicted trees are entirely correct .
0.3 % absolute improvements to BiLSTM - CRF on GE - NIA and CRAFT , respectively , resulting in the highest accuracies on both experimental corpora .
Note that for PTB , CNN - based character - level word embeddings only provided a 0.1 % improvement to BiLSTM - CRF .
The larger improvements on GENIA and CRAFT show that character - level word embeddings are specifically useful to capture rare or unseen words in biomedical text data .
Character - level word embeddings are useful for morphologically rich languages , and although English is not morphologically rich , the biomedical domain contains a wide variety of morphological variants of domain - specific terminology .
Words tagged incorrectly are largely associated with gold tags NN , JJ and NNS ; many are abbreviations which are also out - of - vocabulary .
It is typically difficult for character - level word embeddings to capture those unseen abbreviated words .
On both GENIA and CRAFT , BiLSTM - CRF with character - level word embeddings obtains the highest accuracy scores .
These are just 0.1 % absolute higher than the accuracies of NLP4J - POS .
Note that small variations in POS tagging performance are not a critical factor in parsing performance .
In addition , we find that NLP4J - POS obtains 30 - time faster training and testing speed .
Hence for the dependency parsing task , we use NLP4J - POS to perform 20 - way jackknifing to generate POS tags on training data and to predict POS tags on development and test sets .
Overall dependency parsing results
We present the LAS and UAS scores of different parsing models in .
The first five rows show parsing results on the GENIA test set of " pre-trained " parsers .
The first two rows present scores of the pre-trained Stanford NNdep and Biaffine v 1 models with POS tags predicted by the pre-trained Stanford tagger , while the next two rows 3 - 4 present scores of these pretrained models with POS tags predicted by NLP4J - POS .
Both pre-trained NNdep and Biaffine models were trained on a dependency treebank of 40K sentences , which was converted from the English PTB sections 2 - 21 .
The fifth row shows scores of BLLIP + Bio , the BLLIP reranking constituent parser with an improved self - trained biomedical parsing model .
We use the Stanford conversion toolkit ( v3.5.1 ) to generate dependency trees with the basic Stanford dependencies and use the data split on GENIA as used in , therefore parsing scores are comparable .
The remaining rows show results of our retrained dependency parsing models .
On GENIA , among pre-trained models , BLLIP obtains highest results .
This model , unlike the other pretrained models , was trained using GENIA , so this result is unsurprising .
The pre-trained Stanford - Biaffine ( v1 ) model produces lower scores than the pre-trained Stanford - NNdep model on GENIA .
It is also unsurprising because the pre-trained Stanford - Biaffine utilizes pre-trained word vectors which were learned from newswire corpora .
Note that the pre-trained NNdep and Biaffine models result in no significant performance differences irrespective of the source of POS tags ( i.e. the pre-trained Stanford tagger at 98.37 % vs. the retrained NLP4J - POS model at 98.80 % ) .
Regarding the retrained parsing models , on both GENIA and CRAFT , Stanford - Biaffine achieves the
Parsing result analysis
Here we present a detailed analysis of the parsing results obtained by the retrained models with predicted POS tags .
For simplicity , the following more detailed analyses report LAS scores , computed without punctuation .
Using UAS scores or computing with punctuation does not reveal any additional information .
presents LAS scores by sentence length in bins of length 10 .
As expected , all parsers produce better results for shorter sentences on both corpora ; longer sentences are likely to have longer dependencies which are typically harder to predict precisely .
Scores drop by about 10 % for sentences longer than 50 words , relative to short sentences <= 10 words .
Exceptionally , on GENIA we find lower scores for the shortest sentences than for the sentences from 11 to 20 words .
This is probably because abstracts tend not to contain short sentences : ( i ) as shown in , the proportion of sentences in the first bin is very low at 3.5 % on GE - NIA ( cf. 17.8 % on CRAFT ) , and ( ii ) sentences in the first bin on GENIA are relatively long , with an average length of 9 words ( cf. 5 words in CRAFT ) .
shows LAS ( F1 ) scores corresponding to the dependency distance i ?
j , between a dependent w i and it s head w j , where i and j are consecutive indices of words in a sentence .
Short dependencies are often modifiers of nouns such as determiners or adjectives or pronouns modifying their direct neighbors , while longer dependencies typically represent modifiers of the root or the main verb .
All parsers obtain higher scores for left dependencies than for right dependencies .
This is not completely unexpected as English is strongly head - initial .
In addition , the gaps between LSTM - based models ( i.e. Stanford - Biaffine and jPTDP ) and non -LSTM models ( i.e. NLP4J - dep and Stanford - NNdep ) are larger for the long dependencies than for the shorter ones , as LSTM architectures can preserve long range information .
On both corpora , higher scores are also associated with shorter distances .
There is one surprising exception : on GENIA , in distance bins of ? 4 , ? 5 and < ?5 , Stanford - Biaffine and jPTDP obtain higher scores for longer distances .
This may result from the structural characteristics of sentences in the GENIA corpus .
Table 5 details the scores of Stanford - Biaffine in terms of the most frequent dependency labels in these leftmost dependency bins .
We find amod and nn are the two most difficult to predict dependency relations ( the same finding applied to jPTDP ) .
They appear much more frequently in the bins ? 4 and ? 5 than in bin < ?5 , explaining the higher over all score for bin < ?5 . present LAS scores for the most frequent dependency relation types on GENIA and CRAFT , respectively .
In most cases , Stanford - Biaffine obtains the highest score for each relation type on both corpora with the following exceptions : on GENIA , jPTDP gets the highest results to aux , dep and nn ( as well as nsubjpass ) , while NLP4J - dep and NNdep obtain the highest scores for auxpass and num , respectively .
On GENIA the labels associated with the highest average LAS scores ( generally > 90 % ) are amod , aux , auxpass , det , dobj , mark , nsubj , nsubjpass , pobj and root whereas on CRAFT they are NMOD , OBJ , PMOD , PRD , ROOT , SBJ , SUB and VC .
These labels either correspond to short dependencies ( e.g. aux , auxpass and VC ) , have strong lexical indications ( e.g. det , pobj and PMOD ) , or occur very often ( e.g. amod , subj , NMOD and SBJ ) .
Those relation types with the lowest LAS scores ( generally < 70 % ) are dep on GENIA and DEP , LOC , PRN and TMP on CRAFT ; dep / DEP are very general labels while LOC , PRN and TMP are among the least frequent labels .
Those types also associate to the biggest variation of obtained accuracy across parsers ( > 8 % ) .
In addition , the coordination - related labels cc , conj / CONJ and COORD show large variation across parsers .
These 9 mentioned relation labels generally correspond to long dependencies .
Therefore , it is not surprising that BiLSTM - based models Stanford - Biaffine and jPTDP can produce much higher accuracies on these labels than non -LSTM models NLP4J - dep and NNdep .
Sentence length
Dependency distance
Dependency label
The remaining types are either relatively rare labels ( e.g. appos , num and AMOD ) or more frequent labels but with a varied distribution of dependency distances ( e.g. advmod , nn , and ADV ) .
POS tag of the dependent analyzes the LAS scores by the most frequent POS tags ( across two corpora ) of the dependent .
Stanford - Biaffine achieves the highest scores on all these tags except TO where the traditional featurebased model NLP4J - dep obtains the highest score ( TO is relatively rare tag in GENIA and is the least frequent tag in CRAFT among tags listed in ) .
Among listed tags VBG is the least and second least frequent one in GENIA and CRAFT , respectively , and generally associates to longer dependency distances .
So , it is reasonable that the lowest scores we obtain on both corpora are accounted for by VBG .
The coordinating conjunction tag CC also often corresponds to long dependencies , thus resulting in biggest ranges across parsers on both GENIA and CRAFT .
The results for CC are consistent with the results obtained for the dependency labels cc in and COORD in because they are coupled to each other .
On the remaining POS tags , we generally find similar patterns across parsers and corpora , except for IN and VB where parsers produce 8 + % higher scores for IN on GENIA than on CRAFT , and vice versa producing 9 + % lower scores for VB on GENIA .
This is because on GENIA , IN is mostly coupled with the dependency label prep at a rate of 90 % ( thus their corresponding LAS scores in tables 8 and 6 are consistent ) , while on CRAFT IN is coupled to a more varied distribution of dependency labels such as ADV with a rate at 20 % , LOC at 14 % , NMOD at 40 % and TMP at 5 % .
Regarding VB , on CRAFT it usually associates to a short dependency distance of 1 word ( i.e. head and dependent words are next to each other ) with a rate at 80 % , and to a distance of 2 words at 15 % , while on GENIA it associates with longer dependency distances with a rate at 17 % for the distance of 1 word , 31 % for the distance of 2 words and 34 % for a distance of > 5 words .
So , parsers obtain much higher scores for VB on CRAFT than on GENIA .
Error analysis
We analyze token - level parsing errors that occur consistently across all parsers ( i.e. the intersection set of errors ) , and find that there are few common error patterns .
The first one is related to incorrect POS tag prediction ( 8 % of the intersected parsing errors on GENIA and 12 % on CRAFT are coupled with incorrect predicted POS tags ) .
For example , the word token " domains " is the head of the phrase " both the POU ( S ) and POU ( H ) domains " in .
We also have two OOV word tokens " POU ( S ) " and " POU ( H ) " which abbreviate " POU-specific " and " POU homeodomain " , respectively .
NLP4J - POS ( as well as all other POS taggers ) produced an incorrect tag of NN rather than adjective ( JJ ) for " POU ( S ) " .
As " POU ( S ) " is predicted to be a noun , all parsers make an incorrect prediction that it is the phrasal head , thus also resulting in errors to remaining dependent words in the phrase .
The second error type occurs on noun phrases such as " the Oct -1 - responsive octamer sequence AT - GCAAAT " ( in ) and " the herpes simplex virus Oct - 1 coregulator VP16 " , commonly referred to as appositive structures , where the second to last noun ( i.e. " sequence " and " coregulator " ) is considered to be the phrasal head , rather than the last noun .
However , such phrases are relatively rare and all parsers predict the last noun as the head .
The third error type is related to the relation labels dep / DEP .
We manually re-annotate every case where all parsers agree on the dependency label for a dependency arc with the same dependency label , where this label dis agrees with the gold label dep / DEP ( these cases are about 3.5 % of the parsing errors intersected across all parsers on GENIA and 0.5 % on CRAFT ) .
Based on this manual review , we find that about 80 % of these cases appear to be labelled correctly , despite not agreeing with the gold standard .
In other words , the gold standard appears to be in error in these cases .
This result is not completely unexpected because when converting from constituent treebanks to dependency treebanks , the general dependency label dep / DEP is usually assigned due to limitations in the automatic conversion toolkit .
Parser comparison on event extraction
We present an extrinsic evaluation of the four dependency parsers for the downstream task of biomedical event extraction .
Evaluation setup
Previously , Miwa et al.
adopted the BioNLP 2009 shared task on biomedical event extraction to compare the task - oriented performance of six " pretrained " parsers with 3 different types of dependency representations .
However , their evaluation setup requires use of a currently unavailable event extraction system .
Fortunately , the extrinsic parser evaluation ( EPE 2017 ) shared task aimed to evaluate different dependency representations by comparing their performance on downstream tasks , including a biomedical event extraction task .
We thus follow the experimental setup used there ; employing the Turku Event Extraction System ( TEES , ) to assess the impact of parser differences on biomedical relation extraction .
EPE 2017 uses the BioNLP 2009 shared task dataset , which was derived from the GENIA treebank corpus ( 800 , 150 and 260 abstract files used for BioNLP 2009 training , development and test , respectively ) .
We only need to provide dependency parses of raw texts using the pre-processed tokenized and sentencesegmented data provided by the EPE 2017 shared task .
For the Stanford - Biaffine , NLP4 J - dep and Stanford - NNdep parsers that require predicted POS tags , we use the retrained NLP4J - POS model to generate POS https://github.com/jbjorne/TEES/wiki/EPE-2017 678 of 800 training , 132 of 150 development and 248 of 260 test files are included in the GENIA treebank training set .
tags .
We then produce parses using retrained dependency parsing models .
TEES is then trained for the BioNLP 2009 Task 1 using the training data , and is evaluated on the development data ( gold event annotations are only available to public for training and development sets ) .
To obtain test set performance , we use an online evaluation system .
The online evaluation system for the BioNLP 2009 shared task is currently not available .
Therefore , we employ the online evaluation system for the BioNLP 2011 shared task with the " abstracts only " option .
The score is reported using the approximate span & recursive evaluation strategy .
presents the intrinsic UAS and LAS ( F1 ) scores on the pre-processed segmented BioNLP 2009 development sentences ( i.e. scores with respect to predicted segmentation ) , for which these sentences contain event interactions .
These scores are higher than those presented in because most part of the BioNLP 2009 dataset is extracted from the GENIA treebank training set .
Although gold event annotations in the BioNLP 2009 test set are not available to public , it is likely that we would obtain the similar intrinsic UAS and LAS scores on the pre-processed segmented test sentences containing event interactions .
compares parsers with respect to the EPE 2017 biomedical event extraction task .
The first row presents the score of the Stanford&Paris team ; the highest official score obtained on the test set .
Their system used the Stanford - Biaffine parser ( v2 ) trained on a dataset combining PTB , Brown corpus , and GE - NIA treebank data .
The second row presents our score for the pre-trained BLLIP +
Bio model ; remaining rows show scores using re-trained parsing models .
Impact of parsing on event extraction
The results for parsers trained with the GENIA treebank ( Rows 1 - 6 , ) are generally higher than http://bionlp-st.dbcls.jp/GE/2011/eval-test/eval.cgi
The EPE 2017 shared task focused on evaluating different dependency representations in downstream tasks , not on comparing different parsers .
Therefore each participating team employed only one parser , either a dependency graph or tree parser .
Only the Stanford&Paris team employ GENIA data , obtaining the highest biomedical event extraction score . for parsers trained on CRAFT .
This is logical because the BioNLP 2009 shared task dataset was a subset of the GENIA corpus .
However , we find that the differences in intrinsic parsing results as presented in tables 4 and 10 do not consistently explain the differences in extrinsic biomedical event extraction performance , extending preliminary related observations in prior work .
Among the four dependency parsers trained on GENIA , Stanford - Biaffine , jPTDP and NLP4J - dep produce similar event extraction scores on the development set , while on the the test set jPTDP and NLP4 Jdep obtain the lowest and highest scores , respectively .
also summarizes the results with the dependency structures only ( i.e. results without dependency relation labels ; replacing all predicted dependency labels by " UNK " before training TEES ) .
In most cases , compared to using dependency labels , event extraction scores drop on the development set ( except NLP4J - dep trained on CRAFT ) , while they increase on the test set ( except NLP4J - dep trained on GENIA and Stanford - NNdep trained on CRAFT ) .
Without dependency labels , better event extraction scores on the development set corresponds to better scores on the test set .
In addition , the differences in these event extraction scores without dependency labels are more consistent with the parsing performance differences than the scores with dependency labels .
These findings show that variations in dependency representations strongly affect event extraction performance .
Some ( predicted ) dependency labels are likely to be particularly useful for extracting events , while others hurt performance .
Also , investigating ?
20 frequent dependency labels in each dataset as well as some possible combinations between them could lead to an enormous number of additional experiments .
We believe a detailed analysis of the interaction between those labels in a downstream application task deserves another research paper with a more careful analysis .
Here , one contribution of our paper could be seen to be that we highlight the need for further research in this direction .
Conclusion
We have presented a detailed empirical study comparing SOTA traditional feature - based and neural network - based models for POS tagging and dependency parsing in the biomedical context .
In general , the neural models outperform the feature - based models on two benchmark biomedical corpora GENIA and CRAFT .
In particular , BiLSTM - CRF - based models with character - level word embeddings produce highest POS tagging accuracies which are slightly better than NLP4J - POS , while the Stanford - Biaffine parsing model obtains significantly better result than other parsing models .
We also investigate the influence of parser selection for a biomedical event extraction downstream task , and show that better intrinsic parsing performance does not always imply better extrinsic event extraction performance .
Whether this pattern holds for other information extraction tasks is left as future work .
