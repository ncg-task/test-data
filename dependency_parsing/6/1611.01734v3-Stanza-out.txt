title
DEEP BIAFFINE ATTENTION FOR NEURAL DEPENDENCY PARSING
abstract
This paper builds off recent work from Kiperwasser & Goldberg ( 2016 ) using neural attention in a simple graph - based dependency parser .
We use a larger but more thoroughly regularized parser than other recent BiLSTM - based approaches , with biaffine classifiers to predict arcs and labels .
Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages , achieving 95.7 % UAS and 94.1 % LAS on the most popular English PTB dataset .
This makes it the highest - performing graph - based parser on this benchmarkoutperforming Kiperwasser & Goldberg ( 2016 ) by 1.8 % and 2.2 % - and comparable to the highest performing transition - based parser ( Kuncoro et al. , 2016 ) , which achieves 95.8 % UAS and 94.6 % LAS .
We also show which hyperparameter choices had a significant effect on parsing accuracy , allowing us to achieve large gains over other graph - based approaches .
INTRODUCTION
Dependency parsers - which annotate sentences in a way designed to be easy for humans and computers alike to understand - have been found to be extremely useful for a sizable number of NLP tasks , especially those involving natural language understanding in someway .
However , frequent incorrect parses can severely inhibit final performance , so improving the quality of dependency parsers is needed for the improvement and success of these downstream tasks .
The current state - of - the - art transition - based neural dependency parser substantially outperforms many much simpler neural graph - based parsers .
We modify the neural graphbased approach first proposed by in a few ways to achieve competitive performance : we build a network that 's larger but uses more regularization ; we replace the traditional MLP - based attention mechanism and affine label classifier with biaffine ones ; and rather than using the top recurrent states of the LSTM in the biaffine transformations , we first put them through MLP operations that reduce their dimensionality .
Furthermore , we compare models trained with different architectures and hyperparameters to motivate our approach empirically .
The resulting parser maintains most of the simplicity of neural graph - based approaches while approaching the performance of the SOTA transition - based one .
BACKGROUND AND RELATED WORK
Transition - based parsers - such as shift - reduce parsers - parse sentences from left to right , maintaining a " buffer " of words that have not yet been parsed and a " stack " of words whose head has not been seen or whose dependents have not all been fully parsed .
At each step , transition - based parsers can access and manipulate the stack and buffer and assign arcs from one word to another .
One can then train any multi-class machine learning classifier on features extracted from the stack , buffer , and previous arc actions in order to predict the next action .
Chen & Manning ( 2014 ) make the first successful attempt at incorporating deep learning into a transition - based dependency parser .
At each step , the ( feedforward ) network assigns a probability to each action the parser can take based on word , tag , and label embeddings from certain words root / ROOT Casey / NNP hugged / VBD
Kim / NNP root nsubj dobj :
A dependency tree parse for Casey hugged Kim , including part - of - speech tags and a special root token .
Directed edges ( or arcs ) with labels ( or relations ) connect the verb to the root and the arguments to the verb head .
on the stack and buffer .
A number of other researchers have attempted to address some limitations of Chen & Manning 's Chen & Manning parser by augmenting it with additional complexity : and augment it with a beam search and a conditional random field loss objective to allow the parser to " undo " previous actions once it finds evidence that they may have been incorrect ; and and instead use LSTMs to represent the stack and buffer , getting state - of - the - art performance by building in a way of composing parsed phrases together .
Transition - based parsing processes a sentence sequentially to buildup a parse tree one arc at a time .
Consequently , these parsers do n't use machine learning for directly predicting edges ; they use it for predicting the operations of the transition algorithm .
Graph - based parsers , by contrast , use machine learning to assign a weight or probability to each possible edge and then construct a maximum spaning tree ( MST ) from these weighted edges .
present a neural graph - based parser ( in addition to a transition - based one ) that uses the same kind of attention mechanism as for machine translation .
In Kiperwasser & Goldberg 's 2016 model , the ( bidirectional ) LSTM 's recurrent output vector for each word is concatenated with each possible head 's recurrent vector , and the result is used as input to an MLP that scores each resulting arc .
The predicted tree structure at training time is the one where each word depends on its highestscoring head .
Labels are generated analogously , with each word 's recurrent output vector and it s gold or predicted head word 's recurrent vector being used in a multi -class MLP .
Similarly , include a graph - based dependency parser in their multi -task neural model .
In addition to training the model with multiple distinct objectives , they replace the traditional MLP - based attention mechanism that use with a bilinear one ( but still using an MLP label classifier ) .
This makes it analogous to Luong et al. 's 2015 proposed attention mechanism for neural machine translation .
likewise propose a graph - based neural dependency parser , but in a way that attempts to circumvent the limitation of other neural graph - based parsers being unable to condition the scores of each possible arc on previous parsing decisions .
In addition to having one bidirectional recurrent network that computes a recurrent hidden vector for each word , they have additional , unidirectional recurrent networks ( leftto - right and right - to - left ) that keep track of the probabilities of each previous arc , and use these together to predict the scores for the next arc .
PROPOSED DEPENDENCY PARSER
DEEP BIAFFINE ATTENTION
We make a few modifications to the graph - based architectures of , , and , shown in : we use biaffine attention instead of bilinear or traditional MLP - based attention ; we use a biaffine dependency label classifier ; and we apply dimension - reducing MLPs to each recurrent output vector r i before applying the biaffine transformation .
1
The choice of biaffine rather than bilinear or MLP mechanisms makes the classifiers in our model analogous to traditional affine classifiers , which use an affine transformation over a single LSTM output state r i ( or other vector input ) to predict the vector of scores s i for all classes ( 1 ) .
We can think of the proposed biaffine attention mechanism as being a traditional affine
Figure 2 : BiLSTM with deep biaffine attention to score each possible head for each dependent , applied to the sentence " Casey hugged Kim " .
We reverse the order of the biaffine transformation here for clarity .
classifier , but using a ( d d ) linear transformation of the stacked LSTM output RU ( 1 ) in place of the weight matrix W and a ( d 1 ) transformation Ru ( 2 ) for the bias term b.
In addition to being arguably simpler than the MLP - based approach ( involving one bilinear layer rather than two linear layers and a nonlinearity ) , this has the conceptual advantage of directly modeling both the prior probability of a word j receiving any dependents in the term r ? j u ( 2 ) and the likelihood of j receiving a specific dependent i in the term r ? j U ( 1 ) r i .
Analogously , we also use a biaffine classifier to predict dependency labels given the gold or predicted heady i ( 3 ) .
Fixed - class biaffine classifier
( 3 ) This likewise directly models each of the prior probability of each class , the likelihood of a class given just word i ( how probable a word is to take a particular label ) , the likelihood of a class given just the headword y i ( how probable a word is to take dependents with a particular label ) , and the likelihood of a class given both word i and it s head ( how probable a word is to take a particular label given that word 's head ) .
Applying smaller MLPs to the recurrent output states before the biaffine classifier has the advantage of stripping away information not relevant to the current decision .
That is , every top recurrent state r i will need to carry enough information to identify word i 's head , find all its dependents , exclude all its non-dependents , assign itself the correct label , and assign all its dependents their correct labels , as well as transfer any relevant information to the recurrent states of words before and after it .
Thus r i necessarily contains significantly more information than is needed to compute any individual score , and training on this superfluous information needlessly reduces parsing speed and increases the risk of overfitting .
Reducing dimensionality and applying a nonlinearity ( 4 - 6 ) addresses both of these problems .
We call this a deep bilinear attention mechanism , as opposed to shallow bilinear attention , which uses the recurrent states directly .
We apply MLPs to the recurrent states before using them in the label classifier as well .
As with other graph - based models , the predicted tree at training time is the one where each word is a dependent of its highest scoring head ( although at test time we ensure that the parse is a well - formed tree via the MST algorithm ) .
Aside from architectural differences between ours and the other graph - based parsers , we make a number of hyperparameter choices that allow us to outperform theirs , laid out in .
We use 100 - dimensional uncased word vectors 2 and POS tag vectors ; three BiLSTM layers ( 400 dimensions in each direction ) ; and 500 - and 100 - dimensional ReLU MLP layers .
We also apply dropout at every stage of the model : we drop words and tags ( independently ) ; we drop nodes in the LSTM layers ( input and recurrent connections ) , applying the same dropout mask at every recurrent timestep ( cf. the Bayesian dropout of ) ; and we drop nodes in the MLP layers and classifiers , likewise applying the same dropout mask at every timestep .
We optimize the network with annealed Adam for about 50,000 steps , rounded up to the nearest epoch .
HYPERPARAMETER CONFIGURATION
EXPERIMENTS & RESULTS
DATASETS
We show test results for the proposed model on the English Penn Treebank , converted into Stanford Dependencies using both version 3.3.0 and version 3.5.0 of the Stanford Dependency converter ( PTB - SD 3.3.0 and PTB - SD 3.5.0 ) ; the Chinese Penn Treebank ; and the CoNLL 09 shared task dataset , 3 following standard practices for each dataset .
We omit punctuation from evaluation only for the PTB - SD and CTB .
For the English PTB - SD datasets , we use POS tags generated from the Stanford POS tagger ; for the Chinese PTB dataset we use gold tags ; and for the CoNLL 09 dataset we use the provided predicted tags .
Our hyperparameter search was done with the PTB - SD 3.5.0 validation dataset in order to minimize overfitting to the more popular PTB - SD 3.3.0 benchmark , and in our hyperparameter analysis in the following section we report performance on the PTB - SD 3.5.0 test set , shown in Tables 2 and 3 .
HYPERPARAMETER CHOICES
ATTENTION MECHANISM
We examined the effect of different classifier architectures on accuracy and performance .
What we see is that the deep bilinear model outperforms the others with respect to both speed and accuracy .
The model with shallow bilinear arc and label classifiers gets the same unlabeled performance as the deep model with the same settings , but because the label classifier is much larger ( ( 801 c 801 ) as opposed to ( 101 c 101 ) ) , it runs much slower and overfits .
One way to decrease this overfitting is by increasing the MLP dropout , but that of course does n't change parsing speed ; another way is to decrease the recurrent size to 300 , but this hinders unlabeled accuracy without increasing parsing speedup to the same levels as our deeper model .
We also implemented the MLP - based approach to attention and classification used in .
We found this version to : Test Accuracy on PTB - SD 3.5.0 .
Statistically significant differences are marked with an asterisk .
likewise be somewhat slower and significantly underperform the deep biaffine approach in both labeled and unlabeled accuracy .
NETWORK SIZE
We also examine more closely how network size influences speed and accuracy .
In Kiperwasser & Goldberg 's 2016 model , the network uses 2 layers of 125 - dimensional bidirectional LSTMs ; in has one layer of 100 - dimensional bidirectional LSTMs dedicated to parsing ( two lower layers are also trained on other objectives ) ; and Cheng et al. 's 2016 model has one layer of 368 - dimensional GRU cells .
We find that using three or four layers gets significantly better performance than two layers , and increasing the LSTM sizes from 200 to 300 or 400 dimensions likewise signficantly improves performance .
5
RECURRENT CELL
GRU cells have been promoted as a faster and simpler alternative to LSTM cells , and are used in the approach of ; however , in our model they drastically underperformed LSTM cells .
We also implemented the coupled input - forget gate LSTM cells ( Cif - LSTM ) suggested by , 6 finding that while the resulting model still slightly underperforms the more popular LSTM cells , the difference between the two is much smaller .
Additionally , because the gate and candidate cell activations can be computed simultaneously with one matrix multiplication , the Cif - LSTM model is faster than the GRU version even though they have the same number of parameters .
We hypothesize that the output gate in the Cif - LSTM model allows it to maintain a sparse recurrent output state , which helps it adapt to the high levels of dropout needed to prevent overfitting in a way that GRU cells are unable to do .
Because we increase the parser 's power , we also have to increase its regularization .
In addition to using relatively extreme dropout in the recurrent and MLP layers mentioned in , we also regularize the input layer .
We drop 33 % of words and 33 % of tags during training : when one is dropped the other is scaled by a factor of two to compensate , and when both are dropped together , the model simply gets an input of zeros .
Models trained with only word or tag dropout but not both windup signficantly overfitting , hindering label accuracy and - in the latter case - attachment accuracy .
Interestingly , not using any tags at all actually results in better performance than using tags without dropout .
OPTIMIZER
We choose to optimize with Adam , which ( among other things ) keeps a moving average of the L 2 norm of the gradient for each parameter throughout training and divides the gradient for each parameter by this moving average , ensuring that the magnitude of the gradients will on average be close to one .
However , we find that the value for ?
2 recommended by Kingma & Bawhich controls the decay rate for this moving average - is too high for this task ( and we suspect more generally ) .
When this value is very large , the magnitude of the current update is heavily influenced by the larger magnitude of gradients very far in the past , with the effect that the optimizer ca n't adapt quickly to recent changes in the model .
Thus we find that setting ? 2 to . 9 instead of . 999 makes a large positive impact on final performance .
RESULTS
Our model gets nearly the same UAS performance on PTB - SD 3.3.0 as the current SOTA model from in spite of its substantially simpler architecture , and gets SOTA UAS performance on CTB 5.1 7 as well as SOTA performance on all CoNLL 09 languages .
It is worth noting that the CoNLL 09 datasets contain many non-projective dependencies , which are difficult or impossible for transition - based - but not graph - based - parsers to predict .
This may account for some of the large , consistent difference between our model and Andor et al. 's 2016 transition - based model applied to these datasets .
Where our model appears to lag behind the SOTA model is in LAS , indicating one of a few possibilities .
Firstly , it maybe the result of inefficiencies or errors in the GloVe embeddings or POS tagger , in which case using alternative pretrained embeddings or a more accurate tagger might improve label classification .
Secondly , the SOTA model is specifically designed to capture phrasal compositionality ; so another possibility is that ours does n't capture this compositionality as effectively , and that this results in a worse label score .
Similarly , it maybe the result of a more general limitation of graph - based parsers , which have access to less explicit syntactic information than transition - based parsers when making decisions .
Addressing these latter two limitations would require a more innovative architecture than the relatively simple one used in current neural graph - based parsers .
CONCLUSION
In this paper we proposed using a modified version of bilinear attention in a neural dependency parser that increases parsing speed without hurting performance .
We showed that our larger but more regularized network outperforms other neural graph - based parsers and gets comparable performance to the current SOTA transition - based parser .
We also provided empirical motivation for the proposed architecture and configuration over similar ones in the existing literature .
Future work will involve exploring ways of bridging the gap between labeled and unlabeled accuracy and augment the parser with a smarter way of handling out - of - vocabulary tokens for morphologically richer languages .
